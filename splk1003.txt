50. Which of the following indexes come pre-configured with Splunk Enterprise? (Choose all that apply.)
A. _licence
B. _internal
C. _external
D. _thefishbucket

ChantreyC Highly Voted 3 years, 6 months ago
B & D - pg 95 SysAdmin pdf
upvoted 8 times
Sandy_1988 Highly Voted 3 years, 5 months ago
BD are the options
upvoted 6 times
samsam5136431 Most Recent 1 week, 1 day ago
Selected Answer: D
B and D
upvoted 1 times
allahsal 4 months, 2 weeks ago
Selected Answer: B
B and D
upvoted 2 times
HNaka 5 months, 3 weeks ago
Selected Answer: D
B and D

_internal
To index Splunks own logs and metrics
_audit
To store Splunk audit trails and other optional auditing information
_introspection
To track system performance, Splunk resource usage data, and provide Monitoring Console (MC) with performance data
_thefishbucket
To contain checkpoint information for file monitoring inputs
summary
Default index for summary indexing system
main
Default index for inputs; located in the defaultdb directory
upvoted 1 times
adamsca 1 year, 3 months ago
B & D are correct
upvoted 1 times
oswaldek 1 year, 7 months ago
Selected Answer: B
_thefishbucket looks decommitted
https://community.splunk.com/t5/Splunk-Search/How-do-I-activate-quot-thefishbucket-quot-index/m-p/410263
upvoted 2 times
Steve2610 1 year, 11 months ago
Selected Answer: B
B and D
System Admin Slide 105
upvoted 3 times
huu_nguyen 2 years, 5 months ago
B and D are my final answers
upvoted 5 times
Apis 2 years, 6 months ago
Selected Answer: B
B & D are correct
upvoted 4 times
lilsem 2 years, 10 months ago
B, D are the correct answer. After installing Splunk 8.2 on my local machine I checked the default indexes.conf, and there is the fishbucket index configured.
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed B and D. Quoting the Splunk Reference URL https://www.splunk.com/en_us/blog/tips-and-tricks/what-is-this-fishbucket-thing.html

"ts time for a little Indexing 101. If you look in the directory where your Splunk datastore resides (default location /opt/splunk/var/lib/splunk) you will find a directory called fishbucket. This index is not really intended for normal humans to investigate, more just Splunk engineers trying to decipher file input issues. It contains seek pointers and CRCs for the files you are indexing, so splunkd can tell if it has read them already. To see whats there, try searching for index=_thefishbucket. Events look something like this:"
upvoted 1 times
furiousjase 2 years, 10 months ago
I believe the only answer is B.

The other preconfigured indexes are:
main: The default Splunk Enterprise index. All processed external data is stored here unless otherwise specified.
_internal: This index includes Splunk Enterprise internal logs.
_metrics: This index contains Splunk Enterprise internal data, stored in the form of metric data points.
_audit: Events from the file system change monitor, auditing, and all user search history.
_introspection: This index provides data about the Splunk Enterprise instance and environment .

https://docs.splunk.com/Documentation/Splunk/8.2.2/Indexer/Aboutmanagingindexes
upvoted 2 times
SasnycoN 2 years, 7 months ago
_thefishbucket is also preconfigured. Just checked on my installation.
Can confirm B and D
upvoted 1 times
rodrigok 3 years, 3 months ago
B & D sounds better
upvoted 4 times
Shaq007 3 years, 6 months ago
Splunk Enterprise comes with a number of preconfigured indexes, including:
main: This is the default Splunk Enterprise index. All processed data is stored here unless otherwise specified.
_internal: Stores Splunk Enterprise internal logs and processing metrics.
_audit: Contains events related to the file system change monitor, auditing, and all user search history.

Since the only choice available is "_internal" the answer is B.
Ref: https://docs.splunk.com/Documentation/Splunk/7.3.1/Indexer/Howindexingworks
upvoted 3 times
jgab 3 years, 8 months ago
Answer B & C
upvoted 2 times
jgab 3 years, 8 months ago
Sorry
The correct are B & D
upvoted 10 times
------------------------

48. Which Splunk indexer operating system platform is supported when sending logs from a Windows universal forwarder?
A. Any OS platform.
B. Linux platform only.
C. Windows platform only.
D. None of the above.

giubal Highly Voted 4 years, 2 months ago
As per "data administrator" pdf about windows input
"Data can be forwarded to any Splunk indexer on any OS platform"
upvoted 16 times
Bianchi 2 years, 11 months ago
Yup, Pag: 185
upvoted 3 times
Josi12 Highly Voted 4 years, 1 month ago
The answer is A. Regardless of the OS host the forwarder/indexer; from the forwarder box configure IP address of the indexer(s) and replication port 9997.
upvoted 9 times
samsam5136431 Most Recent 1 week, 1 day ago
Selected Answer: A
Data can be forwarded to any Splunk indexer on any OS platform
upvoted 1 times
yybbb 5 months, 2 weeks ago
Selected Answer: A
Data can be forwarded to any Splunk indexer on any OS platform
upvoted 1 times
royjn1981 2 years, 5 months ago
Selected Answer: A
As per "data administrator" pdf about windows input
"Data can be forwarded to any Splunk indexer on any OS platform"
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 2 times
lollo1234 3 years, 4 months ago
A is correct. Never use a windows deployment-server to manage Linux hosts, it's unsupported
upvoted 3 times
Shaq007 3 years, 6 months ago
A. Any OS platform.
upvoted 3 times
Praf7 3 years, 8 months ago
Option A is correct
upvoted 5 times
ames 3 years, 10 months ago
I think the UF has to be Windows specific for windows events/inputs but indexer can run any OS platform
upvoted 1 times
AbuAli 4 years, 3 months ago
This question is tricky
It's not B & C 
A is confusing its support all OS (within the sphere of supported platforms)
So it's could be D but I will go with A

See below link for more details
https://answers.splunk.com/answers/153612/what-is-the-best-way-to-get-data-from-a-linux-forwarder-to-a-windows-indexer.html
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the provided Splunk Reference URL

"The forwarder/indexer relationship can be considered platform agnostic (within the sphere of supported platforms) because they exchange their data handshake (and the data, if you wish) over TCP.
upvoted 1 times
------------------------

35. How would you configure your distsearch.conf to allow you to run the search below? sourcetype=access_combined status=200 action=purchase splunk_server_group=HOUSTON
A. [distributedSearch:NYC] default = false servers = nyc1:8089, nyc2:8089 [distributedSearch:HOUSTON] default = false servers = houston1:8089, houston2:8089
B. [distributedSearch] servers =nyc1, nyc2, houston1, houston2 [distributedSearch:NYC] default = false servers = nyc1, nyc2 [distributedSearch:HOUSTON] default = false servers = houston1, houston2
C. [distributedSearch] servers =nyc1:8089, nyc2:8089, houston1:8089, houston2:8089 [distributedSearch:NYC] default = false servers = nyc1:8089, nyc2:8089 [distributedSearch:HOUSTON] default = false servers = houston1:8089, houston2:8089
D. [distributedSearch] servers =nyc1:8089; nyc2:80893; houston1:8089; houston2:8089 [distributedSearch:NYC] default = false servers = nyc1:8089; nyc2:8089 [distributedSearch:HOUSTON] default = false servers = houston1:80897706; houston2:80898350

nottyan Highly Voted 3 years, 8 months ago
I think C is Ans.
https://docs.splunk.com/Documentation/Splunk/8.1.0/DistSearch/Distributedsearchgroups
upvoted 12 times
newrose Highly Voted 3 years, 7 months ago
In my opinion it is C:
Example from https://docs.splunk.com/Documentation/Splunk/8.1.0/DistSearch/Distributedsearchgroups:

[distributedSearch]
# This stanza lists the full set of search peers.
servers = 192.168.1.1:8089, 192.168.1.2:8089, 175.143.1.1:8089, 175.143.1.2:8089, 175.143.1.3:8089

[distributedSearch:NYC]
# This stanza lists the set of search peers in New York.
default = false
servers = 192.168.1.1:8089, 192.168.1.2:8089

[distributedSearch:SF]
# This stanza lists the set of search peers in San Francisco.
default = false
servers = 175.143.1.1:8089, 175.143.1.2:8089, 175.143.1.3:8089

And specifications from distsearch.conf:

servers = <comma-separated list>
* An initial list of servers.
* Each member of this list must be a valid URI in the format of
 scheme://hostname:port
upvoted 9 times
HR1234 Most Recent 2 weeks, 3 days ago
Selected Answer: C
C is Ans
upvoted 1 times
tmmt 1 year, 5 months ago
Selected Answer: C
Is C, others have invalid parameter separator, port and invalid stanza for distsearch
upvoted 2 times
toney_mu 1 year, 5 months ago
I would choose C
https://docs.splunk.com/Documentation/Splunk/9.0.0/DistSearch/Distributedsearchgroups
upvoted 1 times
Steve2610 1 year, 11 months ago
Selected Answer: B
B I think
upvoted 1 times
Marco63 2 years, 2 months ago
Selected Answer: C
see https://docs.splunk.com/Documentation/Splunk/8.0.3/DistSearch/Distributedsearchgroups
The servers attribute lists groups of search peers by IP address and management port.
The servers list for each search group must be a subset of the list in the general [distributedSearch] stanza.
upvoted 2 times
rafiki31 2 years, 3 months ago
A is also correct to me: 
"the full set of search peers in the [distributedSearch] stanza will be queried when the search does not specify a search group."
https://docs.splunk.com/Documentation/Splunk/8.1.0/DistSearch/Distributedsearchgroups

Here the search specifies the search group
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
ArDeKu 3 years, 3 months ago
The answer is C..
Refer link - https://docs.splunk.com/Documentation/Splunk/8.0.3/DistSearch/Distributedsearchgroups
upvoted 3 times
boruilei 3 years, 8 months ago
i think d is ans
upvoted 1 times
Ashton_98 3 years, 7 months ago
100% not D. You can't have ports over 65,535.
upvoted 2 times
AngusBlack 3 years ago
Plus they are supposed to be comma separated, not colons
upvoted 1 times
------------------------

155. When working with an indexer cluster, what changes with the global precedence when comparing to a standalone deployment?
A. Nothing changes.
B. The peer-apps local directory becomes the highest priority.
C. The app local directories move to second in the priority list.
D. The system default directory becomes the highest priority.

foxx99 2 weeks, 3 days ago
Selected Answer: C
I think C is the correct answer
upvoted 1 times
------------------------

154. A Splunk administrator has been tasked with developing a retention strategy to have frequently accessed data sets on SSD storage and to have older, less frequently accessed data on slower NAS storage. They have set a mount point for the NAS. Which parameter do they need to modify to set the path for the older, less frequently accessed data in indexes.conf?
A. thawedPath
B. coldPath
C. homePath
D. summaryHomePath

foxx99 2 weeks, 3 days ago
Selected Answer: B
Answer is B
upvoted 1 times
------------------------

153. When should the Data Preview feature be used?
A. When reviewing data on the source host.
B. When extracting fields for ingested data.
C. When validating the parsing of data.
D. When previewing the data before searching.

foxx99 2 weeks, 3 days ago
Selected Answer: C
Answer is C
upvoted 1 times
------------------------

139. What event-processing pipelines are used to process data for indexing? (Choose all that apply.)
A. Parsing pipeline
B. Indexing pipeline
C. fifo pipeline
D. Typing pipeline

foxx99 2 weeks, 4 days ago
Selected Answer: AB
Answer is A & B
upvoted 1 times
------------------------

2. The universal forwarder has which capabilities when sending data? (Choose all that apply.)
A. Sending alerts
B. Compressing data
C. Obfuscating/hiding data
D. Indexer acknowledgement

Ashton_98 Highly Voted 3 years, 8 months ago
D AND B Compressing data is the answer.
upvoted 12 times
newrose 3 years, 7 months ago
I agree
upvoted 4 times
Princee Highly Voted 3 years, 5 months ago
B and D both: compressed=true This tells the forwarder to compress the data before it forwards the data to receiving indexers in the target groups. If you set compressed to "false", the forwarder sends raw data.
Splunk doc: https://docs.splunk.com/Documentation/Forwarder/8.1.1/Forwarder/Configureforwardingwithoutputs.conf#:~:text=compressed%3Dtrue%20This%20tells%20the,the%20forwarder%20sends%20raw%20data.
upvoted 6 times
leteke9429 Most Recent 2 weeks, 5 days ago
The capabilities of a universal forwarder when sending data include:

B. Compressing data
D. Indexer acknowledgement

Explanation:
- **Compressing data**: Universal forwarders can compress data before sending it to reduce bandwidth usage.
- **Indexer acknowledgement**: They can confirm receipt of data by the indexer to ensure data integrity and completeness in the indexing process. < https://bitly.cx/NyKD > I passed my SPLK exam with ease thanks to you. The dumps were accurate and the explanations were clear.
upvoted 1 times
dohatelo 3 months, 1 week ago
B and D is correct . C(masking) can be done with the Heavy Forwarder not the Universal. Universal only parses data.
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: B
B and D are correct
upvoted 1 times
Ibisc 1 year, 1 month ago
Selected Answer: C
I think C is also correct.
https://docs.splunk.com/Documentation/Splunk/latest/Data/Anonymizedata
"To anonymize data with Splunk Enterprise, you must configure a Splunk Enterprise instance as a heavy forwarder and anonymize the incoming data with that instance before sending it to Splunk Enterprise."
upvoted 1 times
Mntman77 1 year ago
In this case they are referring to "universal forwarder" not a heavy, so "C" is out.
upvoted 1 times
harrytbb 1 year, 5 months ago
Selected Answer: D
B & D are the answers
upvoted 1 times
emlch 1 year, 10 months ago
UF has the following capabilities:
- Index ack* (useACK=true in outputs.conf)
- Send data over HTTP
- Compressing the feed (compressed = true on both input.conf (indexer) and outputs.conf (uf))
- Securing the feed with SSL

So, D and B

C. that would be a HF
A. not sure if Forwarders in general can send alerts
upvoted 3 times
emlch 1 year, 10 months ago
But definetely (a) the UF can't send alerts
upvoted 1 times
Ailen_Man 2 years, 2 months ago
Selected Answer: B
Answer is B
upvoted 1 times
Marco63 2 years, 2 months ago
B AND D !!!
upvoted 2 times
RedYeti 2 years, 3 months ago
Selected Answer: D
B. Compressing data
D. Indexer acknowledgement
System Admin course, page 182
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: D
B and D are correct
upvoted 2 times
BMO 3 years, 1 month ago
Data Admin - Slide 65
upvoted 1 times
ZeusP 3 years, 1 month ago
Ans is B&D
upvoted 3 times
------------------------

93. In which phase do indexed extractions in props.conf occur?
A. Inputs phase
B. Parsing phase
C. Indexing phase
D. Searching phase

babusartop17 Highly Voted 3 years, 6 months ago
B is correct.

The following items in the phases below are listed in the order Splunk applies them (ie LINE_BREAKER occurs before TRUNCATE).

Input phase
inputs.conf
props.conf
CHARSET
NO_BINARY_CHECK
CHECK_METHOD
CHECK_FOR_HEADER (deprecated)
PREFIX_SOURCETYPE
sourcetype
wmi.conf
regmon-filters.conf
Structured parsing phase
props.conf
INDEXED_EXTRACTIONS, and all other structured data header extractions
Parsing phase
props.conf
LINE_BREAKER, TRUNCATE, SHOULD_LINEMERGE, BREAK_ONLY_BEFORE_DATE, and all other line merging settings
TIME_PREFIX, TIME_FORMAT, DATETIME_CONFIG (datetime.xml), TZ, and all other time extraction settings and rules
TRANSFORMS which includes per-event queue filtering, per-event index assignment, per-event routing
SEDCMD
MORE_THAN, LESS_THAN
transforms.conf
stanzas referenced by a TRANSFORMS clause in props.conf
LOOKAHEAD, DEST_KEY, WRITE_META, DEFAULT_VALUE, REPEAT_MATCH
upvoted 13 times
sesanchez88 3 years, 5 months ago
You're right.
Structured parsing phase:
---------------------------------------------
 props.conf
 INDEXED_EXTRACTIONS, and all other structured data header extractions

URL: https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Configurationparametersandthedatapipeline
upvoted 6 times
AnupamaManjunath Highly Voted 3 years, 7 months ago
A. Input phase
Data admin PDF - page 242
upvoted 10 times
Atch0071 Most Recent 3 weeks ago
 Indexed extractions are input phase props.conf settings
 In this scenario, the settings belong on forwarder
 Check props.conf.spec for more options 

Datadmin page: 341 

Correct Ans: A
upvoted 1 times
HNaka 5 months, 3 weeks ago
Selected Answer: B
My answer is B.
upvoted 1 times
adamsca 1 year ago
Selected Answer: B
B is correct in my opinion.
upvoted 2 times
erick165 1 year, 3 months ago
Selected Answer: B
" Structured parsing phase
props.conf
INDEXED_EXTRACTIONS, and all other structured data header extractions "
upvoted 1 times
tmmt 1 year, 4 months ago
Selected Answer: B
B, index extractions (INDEX_EXTRATIONS) is done in parsing phase
https://docs.splunk.com/Documentation/Splunk/9.0.4/Admin/Configurationparametersandthedatapipeline

Structured parsing phase
 props.conf INDEXED_EXTRACTIONS, and all other structured data header extractions
upvoted 2 times
anonyuser 1 year, 7 months ago
docs hint at A
Data Admin 9.0 pdf page 341 
"Indexed extractions are input phase props.conf settings"
upvoted 3 times
Mando22 1 year, 9 months ago
Correct Answer: B
upvoted 1 times
Steve2610 1 year, 11 months ago
Selected Answer: A
Data Admin Slide 262
upvoted 1 times
denominator 2 years ago
Selected Answer: A
Pg 262 data admin pdf
upvoted 1 times
Ailen_Man 2 years, 1 month ago
Answer is B,
Structured parsing phase
props.conf
INDEXED_EXTRACTIONS, and all other structured data header extractions
upvoted 1 times
tomod1 2 years, 2 months ago
Selected Answer: A
A is correct
"Structured Data Header Extraction and configuration


# These special string delimiters, which are single ASCII characters,
# can be used in the settings that follow, which state
# "You can use the delimiters for structured data header extraction with
# this setting.

INDEXED_EXTRACTIONS = <CSV|TSV|PSV|W3C|JSON|HEC>
* The type of file that Splunk software should expect for a given source
 type, and the extraction and/or parsing method that should be used on the file."

https://docs.splunk.com/Documentation/Splunk/8.2.6/Admin/Propsconf
upvoted 1 times
tomod1 2 years, 2 months ago
* This setting applies at input time, when data is first read by Splunk
 software, such as on a forwarder that has configured inputs acquiring the
 data.
upvoted 1 times
BlueRoselia 2 years, 4 months ago
Answer A&B
Generally, fields should be extracted at search time, however there are certain use cases when index time field extractions can be used
Provision the extraction during the input or parsing phase
On the forwarder for structured inputs
On the indexer for fields that may be negatively impacting search performance 
uses three configuration files props.conf, transforms.conf on the indexer and fields.conf on the search head

If I have to give one answer, I choose parsing the indexers can handle the extra load better.
upvoted 1 times
FishingZodiac 2 years, 7 months ago
Selected Answer: A
Data admin p263
upvoted 6 times
loky0 2 years, 10 months ago
P263 in Data Admin pdf says "Indexed Extractions are input phase props.conf settings". So it'd be A.

But detailed documentations break down the steps, "INDEXED_EXTRACTIONS, and all other structured data header extractions" are part of the Structured Parsing Phase. So it might be B as well.....
https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Configurationparametersandthedatapipeline
upvoted 4 times
AngusBlack 3 years ago
It's A. From https://docs.splunk.com/Documentation/Splunk/8.1.1/Admin/Propsconf
Heading: Structured Data Header Extraction and configuration
"This setting applies at input time, when data is first read by Splunk
 software, such as on a forwarder that has configured inputs acquiring the
 data."
INDEXED_EXTRACTIONS = <CSV|TSV|PSV|W3C|JSON|HEC>
upvoted 3 times
Load full discussion...
------------------------

161. What action could be taken to prevent a license warning with an ingest-based license?
A. Add a new license before midnight on the indexer(s).
B. Add a new license before midnight on the license manager.
C. Delete the data before midnight on the indexer(s).
D. Delete the data before midnight on the license manager.

Annelavanya 1 month ago
B
Need to add license on license master
upvoted 1 times
------------------------

152. In a customer managed Splunk Enterprise environment, what is the endpoint URI used to collect data?
A. services/data/collector
B. services/inputs?raw
C. services/collector
D. data/collector

foxx99 1 month, 1 week ago
Selected Answer: C
answer is C
upvoted 1 times
Frank_Rai 3 months, 1 week ago
https://docs.splunk.com/Documentation/Splunk/9.2.1/Data/HECExamples

curl "https://mysplunkserver.example.com:8088/services/collector" \
 -H "Authorization: Splunk CF179AE4-3C99-45F5-A7CC-3284AA91CF67" \
 -d '{"event": "Hello, world!", "sourcetype": "manual"}'
upvoted 1 times
Frank_Rai 3 months, 1 week ago
Should be C
upvoted 1 times
------------------------

94. Which of the following statements describes how distributed search works?
A. Forwarders pull data from the search peers.
B. Search heads store a portion of the searchable data.
C. The search head dispatches searches to the search peers.
D. Search results are replicated within the indexer cluster.

khyoung7410 Highly Voted 3 years, 7 months ago
My ans is C
upvoted 19 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/DistSearch/Configuredistributedsearch

"To activate distributed search, you add search peers, or indexers, to a Splunk Enterprise instance that you desingate as a search head. You do this by specifying each search peer manually."
upvoted 1 times
Fe01 2 years, 5 months ago
Agreed, Answer is C
upvoted 1 times
Shaq007 Highly Voted 3 years, 6 months ago
C. The search head dispatches searches to the search peers.
Sys Admin PDF Page 180 - 4. The search head dispatches searches to the indexer (Search Peers)
upvoted 6 times
VirtualSteveO Most Recent 1 month, 2 weeks ago
How can these official answers be so wrong! It's C. D makes no sense at all.
upvoted 1 times
solomone 1 year, 1 month ago
Selected Answer: C
The right answer is C
upvoted 2 times
useheee 1 year, 10 months ago
Selected Answer: C
ans is C
upvoted 2 times
dfggg 2 years, 3 months ago
D is wrong because buckets are replicated within indexer cluster. The correct answer is C, as per slide 185 from Splunk 7.3 System Admin course, from 2019
upvoted 1 times
Salman23 2 years, 9 months ago
C is correct System Admin page 190
upvoted 2 times
Jackall 3 years, 4 months ago
Should be C. Question not mentions the indexer cluster, so D is irrelevant
upvoted 3 times
------------------------

148. A non-clustered Splunk environment has three indexers (A,B,C) and two search heads (X,Y). During a search executed on search head X, indexer A crashes. What is Splunk’s response?
A. Inform the user in Splunk web that their results may be incomplete and have them attempt the search from search head Y.
B. Update the user in Splunk web informing them that the results of their search may be incomplete.
C. Update the user in Splunk web that their results may be incomple and that Splunk will try to re-execute the search.
D. Repeat the search request on indexer B without informing the user.

SCARODJ 1 month, 2 weeks ago
Selected Answer: B
Splunk does not re-attempt searches on its own and changing the SH makes no sense if the Idx has not been recovered.
upvoted 2 times
------------------------

138. What type of Splunk license is pre-selected in a brand new Splunk installation?
A. Enterprise license
B. Forwarder license
C. Free license
D. Enterprise trial license

SCARODJ 1 month, 2 weeks ago
Selected Answer: D
As per Frank_Rai
upvoted 1 times
Frank_Rai 3 months, 1 week ago
D. Enterprise trial license

In a brand new Splunk installation, the Enterprise trial license is pre-selected by default. This trial license typically allows you to index a certain amount of data per day (e.g., 500 MB) and access all Enterprise features for a limited time (usually 60 days). After the trial period expires, if you don't apply a new license, Splunk will revert to the Free license mode, which has limited features and indexing volume.
upvoted 1 times
------------------------

132. When using a directory monitor input, specific source type can be selectively overridden using which configuration file?
A. props.conf
B. sourcetypes.conf
C. transforms.conf
D. outputs.conf

cb42 1 year, 7 months ago
Selected Answer: A
see Splunk Enterprise 9.0 Data Administration, page 255
upvoted 1 times
toney_mu 1 year, 5 months ago
Why not A and D
upvoted 1 times
SCARODJ 1 month, 2 weeks ago
outputs.conf only has configurations regarding the server that will receive the data, being a Heavy Forwarder or an Indexer and how.
upvoted 1 times
------------------------

1. Which setting in indexes.conf allows data retention to be controlled by time?
A. maxDaysToKeep
B. moveToFrozenAfter
C. maxDataRetentionTime
D. frozenTimePeriodInSecs

DeltaPotato Highly Voted 2 years, 11 months ago
D - Sys Admin slide 131 for me (indexes.conf options listed on slide 126).
upvoted 7 times
Prasadthorat333 1 year, 10 months ago
Please help me with the Sys admin PDF.
upvoted 1 times
pcksplunk 5 months ago
D is correct
upvoted 1 times
nanaw770 Most Recent 1 month, 3 weeks ago
Selected Answer: D
D is right answer.
upvoted 1 times
emlch 1 year, 10 months ago
I did a quickly search and the only option seems to be frozenTimePeriodInSecs (the other options isn't additional configurations of indexes.conf)
upvoted 2 times
Nnatech 2 years ago
Answer is D
upvoted 1 times
king1993 2 years, 3 months ago
Answer: D
upvoted 3 times
RedYeti 2 years, 3 months ago
Selected Answer: D
D. frozenTimePeriodInSecs
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 1 times
BMO 3 years, 1 month ago
System Admin - Slide 116
upvoted 1 times
hesbee 2 years, 10 months ago
Please can you share with me the PDF file? Thank you!
upvoted 2 times
ZeusP 3 years, 1 month ago
Ans is D for sure
upvoted 1 times
MrHyde 4 years ago
understand better with this link:
https://docs.splunk.com/Documentation/Splunk/latest/Indexer/Setaretirementandarchivingpolicy
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the provided URL reference, "To specify the age at which data freezes, edit the frozenTimePeriodInSecs attribute in indexes.conf. This attribute specifies the number of seconds to elapse before data gets frozen. "
upvoted 1 times
jasytpeiotqxvohxma 4 years, 2 months ago
D is correct
upvoted 2 times
------------------------

147. Which file will be matched for the following monitor stanza in inputs.conf?

[monitor:///var/log/*/bar/.../*.txt]
A. /var/log/host_469386086/bar/file/foo.txt
B. /var/log/host_469336086/bar/foo.txt
C. /var/log/host_469386086/temp/bar/file/csv/foo.txt
D. /var/log/host_469386086/temp/bar/file/foo.txt

62d8e4c 1 month, 3 weeks ago
Selected Answer: A
"..." searches recursively through one or more directories.
upvoted 1 times
shesky17 2 months, 1 week ago
A. /var/log/host_469386086/bar/file/foo.txt

https://docs.splunk.com/Documentation/Splunk/9.2.1/Admin/Inputsconf

"..." searches recursively through one or more directories. This means that /foo/.../bar matches foo/1/bar, foo/1/2/bar, etc.

The asterisk (*) matches anything in a single path segment; unlike "...", it does not search recursively. 

For example, /foo/*/bar matches the files /foo/1/bar, /foo/2/bar, etc. However, it does not match /foo/bar or /foo/1/2/bar.
upvoted 3 times
not_another_user_007 2 months, 1 week ago
Selected Answer: B
The answer is B
upvoted 1 times
Frank_Rai 3 months, 1 week ago
Changed to B

The correct file that will be matched for the given monitor stanza [monitor:///var/log/*/bar/.../*.txt] is:

B. /var/log/host_469336086/bar/foo.txt

Explanation: The wildcard * matches any string, and the ... allows for any directories to be included in the path. However, the path must end in .txt due to the /*.txt specification. Option B fits this pattern as it directly falls under a bar directory and ends with .txt.
upvoted 2 times
Frank_Rai 3 months, 1 week ago
"A" looks good.
upvoted 2 times
------------------------

10. Where should apps be located on the deployment server that the clients pull from?
A. $SPLUNK_HOME/etc/apps
B. $SPLUNK_HOME/etc/search
C. $SPLUNK_HOME/etc/master-apps
D. $SPLUNK_HOME/etc/deployment-apps

AbuAli Highly Voted 4 years, 3 months ago
The Answer is etc/deployment-apps
upvoted 28 times
ames Highly Voted 3 years, 10 months ago
After an app is downloaded, it resides under $SPLUNK_HOME/etc/apps on the deployment clients. But it resided in the $SPLUNK_HOME/etc/deployment-apps location in the deployment server.

Hence, answer is D.
upvoted 19 times
62d8e4c Most Recent 1 month, 3 weeks ago
D option is the correct one.
upvoted 1 times
allahsal 4 months, 3 weeks ago
Selected Answer: D
Answer is D
upvoted 2 times
Maha86 10 months, 1 week ago
D is correct
upvoted 2 times
toney_mu 1 year, 5 months ago
Answer should be D
=====
Configuration files (such as inputs.conf) to be packaged into apps to
be deployed to the deployment clients
Reside in SPLUNK_HOME/etc/deployment-apps/
=====
upvoted 2 times
ayush_1995 2 years ago
Selected Answer: D
D is correct
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 3 times
neledov 2 years, 7 months ago
Selected Answer: D
answer is D - it's deployment-apps
upvoted 5 times
sachinkiet 2 years, 7 months ago
Selected Answer: D
https://docs.splunk.com/Documentation/Splunk/8.2.3/Updating/Createdeploymentapps
upvoted 3 times
Sandy_1988 3 years, 7 months ago
Ans id D
upvoted 3 times
rj88 3 years, 11 months ago
Answer D is correct
upvoted 4 times
jasytpeiotqxvohxma 4 years, 2 months ago
D is correct
upvoted 3 times
------------------------

8. Which Splunk component consolidates the individual results and prepares reports in a distributed environment?
A. Indexers
B. Forwarder
C. Search head
D. Search peers

giubal Highly Voted 4 years, 2 months ago
It is the Search Head role
https://docs.splunk.com/Documentation/Splunk/7.3.1/DistSearch/Howuserscancontroldistributedsearches
upvoted 23 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Quoting the reference URL

"From the user standpoint, specifying and running a distributed search is essentially the same as running any other search. Behind the scenes, the search head distributes the query to its search peers, and consolidates the results when presenting them to the user."
upvoted 6 times
rajpandey1512 Highly Voted 3 years, 2 months ago
PPT (Sys Admin) - Page 189 - "The search head consolidates the individual results and prepares reports."
upvoted 7 times
62d8e4c Most Recent 1 month, 3 weeks ago
C option is the correct one, SH.
upvoted 1 times
yybbb 5 months, 2 weeks ago
Selected Answer: C
Should be C
upvoted 1 times
KiranVM 1 year, 7 months ago
Selected Answer: C
As per the document, The indexers perform the actual searching of their own indexes, but the search heads manage the overall search process across all the indexers and present the consolidated search results to the user.
So answer is C
upvoted 4 times
Brinkster 1 year, 7 months ago
Correct answer is C. 

Literally on the page quoted it says it's the search head: "The indexers still perform the actual searching of their own indexes, but the search heads manage the overall search process across all the indexers and present the consolidated search results to the user"
upvoted 2 times
alejohu 1 year, 11 months ago
Selected Answer: C
C is correct
upvoted 3 times
Skandale 1 year, 11 months ago
Selected Answer: C
C is ans
upvoted 1 times
Yoho_1013 2 years, 5 months ago
Selected Answer: C
C should be the correct answer
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
neledov 2 years, 7 months ago
Selected Answer: C
C - it's a search head
upvoted 2 times
Shiviv 3 years, 3 months ago
C is correct. Search head does it
upvoted 1 times
Sandy_1988 3 years, 6 months ago
C is the correct answer
upvoted 2 times
IDM 3 years, 9 months ago
search heads is the correct answer
upvoted 3 times
oksey 3 years, 10 months ago
C is the correct Ans
upvoted 3 times
jasytpeiotqxvohxma 4 years, 2 months ago
Search heads is the correct answer
upvoted 4 times
------------------------

24. During search time, which directory of configuration files has the highest precedence?
A. $SPLUNK_HOME/etc/system/local
B. $SPLUNK_HOME/etc/system/default
C. $SPLUNK_HOME/etc/apps/app1/local
D. $SPLUNK_HOME/etc/users/admin/local

dwallen41 Highly Voted 3 years, 7 months ago
Very tricky!!! Answer is NOT D as etc/users/admin/local is not a valid directory . it is missing the <user app>.... to be correct it would look like this... etc/users/admin/<app name>/local .. so answer is C. Also reference Data Admin class PDF page 20 search time precedence diagram..
upvoted 29 times
SPLTony 10 months, 2 weeks ago
What if "admin" in this case was the name of the application?
upvoted 1 times
SCARODJ 2 months, 1 week ago
Apps don't go in the 'users' folder.
upvoted 1 times
giubal Highly Voted 4 years, 2 months ago
The question is about "search time" no "index time" (Global context) so the App/User context has the highest precedence, the answer is D

https://docs.splunk.com/Documentation/Splunk/7.3.0/Admin/Wheretofindtheconfigurationfiles
upvoted 14 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Adding further clarity and quoting same Splunk reference URL from @giubal"

"To keep configuration settings consistent across peer nodes, configuration files are managed from the cluster master, which pushes the files to the slave-app directories on the peer nodes. Files in the slave-app directories have the highest precedence in a cluster peer's configuration. Here is the expanded precedence order for cluster peers:
1.Slave-app local directories -- highest priority
2. System local directory
3. App local directories
4. Slave-app default directories
5. App default directories
6. System default directory --lowest priority
upvoted 2 times
AngusBlack 3 years ago
It would be, but the directory name isn't valid
upvoted 4 times
hesbee 2 years, 11 months ago
Can you explain better, please? On the documentation, it only says "$SPLUNK_HOME/etc/users/*". How is that invalid?
upvoted 1 times
Marco63 2 years, 2 months ago
In the answer the /app_name/" segment of the path is missing
upvoted 2 times
Frank_Rai Most Recent 3 months, 1 week ago
Its D.

During search time, the directory of configuration files with the highest precedence is:

**D. $SPLUNK_HOME/etc/users/admin/local**

The order of precedence for configuration files in Splunk, from highest to lowest, is as follows:

1. **$SPLUNK_HOME/etc/users/<username>/<appname>/local**
2. **$SPLUNK_HOME/etc/users/<username>/<appname>/default**
3. **$SPLUNK_HOME/etc/apps/<appname>/local**
4. **$SPLUNK_HOME/etc/apps/<appname>/default**
5. **$SPLUNK_HOME/etc/system/local**
6. **$SPLUNK_HOME/etc/system/default**

This hierarchy ensures that user-specific settings (which are stored in the `$SPLUNK_HOME/etc/users` directory) take precedence over app-specific settings and system-wide settings.
upvoted 1 times
lance_grown 8 months, 1 week ago
1. Current user directory for app etc/users/user/appname/local
2. App directory -running app etc/apps/appname/local etc/apps/appname/default
3. App directories -all other apps* etc/apps/appname/local etc/apps/appname/default
4. System directories etc/system/localetc/system/default
PDF Page 341

Since the path of D is wrong, I would go with C as the next in line to take precedence and its the highest for this question
upvoted 2 times
bobixaka 8 months, 2 weeks ago
Selected Answer: C
D is very tricky!
It would have been the correct answer if it was D. $SPLUNK_HOME/etc/users/admin/app_name/local
Since there is no app in the path it doesn't exist.
upvoted 1 times
Splunkor 8 months, 3 weeks ago
Selected Answer: D
The question is about search-time precedence, answer D is correct.
upvoted 1 times
Splunkor 8 months, 3 weeks ago
Answer D is correct.
upvoted 1 times
tmmt 1 year, 5 months ago
Selected Answer: C
If D have a correct dir (/etc/users/app_abcde/local) will be correct, but in this case is C
upvoted 1 times
pro12345 1 year, 9 months ago
Selected Answer: C
Answer C
upvoted 1 times
emlch 1 year, 10 months ago
Selected Answer: D
INDEX time: sys local, app local, app default, sys default
SEARCH time: user app (user directory), running app (local and defautl), other apps (local and default), sys directories (local and default).

so D!
upvoted 3 times
tmmt 1 year, 4 months ago
very clear, thanks!
upvoted 1 times
king1993 2 years, 3 months ago
Answer: C
upvoted 1 times
BlueRoselia 2 years, 4 months ago
global/index context
1.etc/system/local
2.etc/apps/app_name/local
3.etc/apps/app_name/default
4.etc/system/default

User/app/search context
1.etc/users/system/local fallow by default
2.etc/apps/currently_running_app/local fallow by default
3.etc/apps/all_other_apps/local fallow by default
4.etc/system/local fallow by default
upvoted 1 times
[Removed] 2 years, 5 months ago
A is correct, page 86-89 in System admin PDF
upvoted 2 times
[Removed] 2 years, 5 months ago
No Sorry, it says search time. Then it is D. Page 90, system admin PDF
upvoted 1 times
[Removed] 2 years, 5 months ago
Ok, don't listen to me. Like people has said. App is missing. Trick question. C all the way here
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
D is incorrect - path is missing app name (assuming local is not an app name)
upvoted 2 times
hieverybody 2 years, 9 months ago
Answer should be A.
upvoted 2 times
Splunk_Newb 3 years ago
It's C. If you have the 8.1 Data Admin PDF, look on page 259. "admin" still has to have an "app" directory under it. Also, according to Splunk, "admin" does not count as the "user"
upvoted 3 times
Lalithadevi 2 years, 10 months ago
Can you please share me Data Admin PDF lalithadevi.segu@gmail.com
upvoted 1 times
krishdee 2 years, 9 months ago
did you got the pdf?
upvoted 1 times
SasnycoN 2 years, 7 months ago
Those PDFs are watermarked with our names and we are forbidden to share them. Sign for the training course if you want to have access to them.
upvoted 2 times
hellonair 3 years ago
Answer is D ( considering search ) Reference - https://dev.splunk.com/enterprise/docs/developapps/manageknowledge/fileprecedence/
upvoted 1 times
AngusBlack 3 years ago
It would be, but the directory name isn't valid
upvoted 1 times
Load full discussion...
------------------------

149. What is the correct curl to send multiple events through HTTP Event Collector?
A.
B.
C.
D.

shesky17 2 months, 1 week ago
Selected Answer: B
B. curl "https://mysplunkserver.example.com:8088/services/collector" \
-H "Authorization: Splunk DF469ZE4-3G38-65F5-H708-6284GG91PF67" \
-d '{"event": "Hello World"}{"event": "Hola Mundo"}{"event": "Hallo Welt", "nested": {"key1": "value1"}}'

See:
https://docs.splunk.com/Documentation/Splunk/9.2.1/Data/HECExamples#:~:text=world!%22%2C%20%22sourcetype%22%3A%20%22manual%22%7D%27-,Example%202%3A%20Send%20multiple%20events%20to%20HEC,-This%20example%20demonstrates

curl "https://mysplunkserver.example.com:8088/services/collector" \
 -H "Authorization: Splunk CF179AE4-3C99-45F5-A7CC-3284AA91CF67" \
 -d '{"event": "Pony 1 has left the barn"}{"event": "Pony 2 has left the barn"}{"event": "Pony 3 has left the barn", "nested": {"key1": "value1"}}'
upvoted 1 times
MartinCaplan 3 months, 3 weeks ago
Selected Answer: C
C suppose to be : 

To send multiple events through HTTP Event Collector (HEC) using cURL, you would typically use a command similar to the following:

bash
Copy code
curl -k $SPLUNK_HEC_URL \
 -H "Authorization: Splunk $SPLUNK_HEC_TOKEN" \
 -d '{"event": "your_event_data1", "sourcetype": "your_sourcetype1", "source": "your_source1"}' \
 -d '{"event": "your_event_data2", "sourcetype": "your_sourcetype2", "source": "your_source2"}'
Replace $SPLUNK_HEC_URL with your HEC endpoint URL and $SPLUNK_HEC_TOKEN with your HEC token.

This example sends two events, each specified with -d option containing JSON data. Adjust the event, sourcetype, and source fields as needed for your events. You can continue adding -d options for additional events. Make sure each JSON data object is separated by a space.
upvoted 1 times
------------------------

145. What will the following inputs.conf stanza do?

[script://myscript.sh]
interval=0
A. The script will run at the default interval of 60 seconds.
B. The script will be run only once for each time Splunk is restarted.
C. The script will be run. As soon as the script exits, Splunk restarts it.
D. The script will not be run.

shesky17 2 months, 1 week ago
C.
See scripted input: 
https://docs.splunk.com/Documentation/Splunk/9.2.1/Admin/Inputsconf

[script://<cmd>]
interval = [<decimal>|<cron schedule>]
* The special value "0" forces this scripted input to be run continuously. As soon as the script exits, the input restarts it. 
* The special value "-1" causes the scripted input to run once on start-up. 
* NOTE: when you specify a cron schedule, the input does not run the script on start-up. 
* Default: 60.0
upvoted 2 times
Frank_Rai 3 months, 1 week ago
Should be B.

By setting 'interval=0', you're instructing Splunk to execute the script a single time at the startup of the Splunk service. After the initial execution, Splunk will not run the script again unless the service is restarted.
upvoted 1 times
------------------------

144. In inputs.conf, which stanza would mean Splunk was only reading one local file?
A. [monitor::/opt/log/crashlog/Jan27crash.txt]
B. [monitor:///opt/log/crashlogs/Jan27crash.txt]
C. [read://opt/log/crashlog/Jan27crash.txt]
D. [monitor:///opt/log/]

shesky17 2 months, 1 week ago
Selected Answer: B
File monitoring

https://docs.splunk.com/Documentation/Splunk/9.2.1/Data/Monitorfilesanddirectorieswithinputs.conf
upvoted 2 times
MartinCaplan 3 months, 2 weeks ago
Selected Answer: B
Answer is B due to documentation
upvoted 3 times
------------------------

108. An index stores its data in buckets. Which default directories does Splunk use to store buckets? (Choose all that apply.)
A. bucketdb
B. frozendb
C. colddb
D. db

Frank_Rai 3 months, 1 week ago
Yes C & D

The default directories Splunk uses to store buckets are:

C. `colddb` - This directory stores cold buckets, which are older indexed data that has been rolled from the "hot" and "warm" states but is still searchable.

D. `db` - This directory is typically associated with hot and warm buckets. "Hot" buckets are the current writeable buckets where new data is indexed. When they reach a certain size or age, they become "warm" buckets.

The `bucketdb` is not a standard directory for storing Splunk data buckets, and `frozendb` is where frozen data is stored, but it's important to note that frozen data is no longer searchable within Splunk, as it's considered archived or deleted based on the retention policy.
upvoted 1 times
adamsca 1 year, 3 months ago
Selected Answer: CD
Agree, CD
upvoted 1 times
Rodders2828 1 year, 7 months ago
Selected Answer: CD
Agree, C&D
upvoted 1 times
NickSplunk 1 year, 8 months ago
its c&d
upvoted 2 times
furiousjase 2 years, 10 months ago
Confirmed - C & D
also thaweddb
upvoted 4 times
loky0 2 years, 10 months ago
answer is CD, see P123 on sys admin pdf
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Using the splunk wiki URL reference ttps://wiki.splunk.com/Deploy:BucketRotationAndRetention
found the values colddb and db only
upvoted 1 times
kiranhar 2 years, 10 months ago
Sorry it CD
upvoted 3 times
kiranhar 2 years, 11 months ago
BC is correct answer
upvoted 1 times
------------------------

111. A log file contains 193 days worth of timestamped events. Which monitor stanza would be used to collect data 45 days old and newer from that log file?
A. followTail = -45d
B. ignore = 45d
C. includeNewerThan = 45d
D. ignoreOlderThan = 45d

Frank_Rai 3 months, 1 week ago
D.

The correct monitor stanza to collect data that is 45 days old and newer from a log file in Splunk is:

D. `ignoreOlderThan = 45d`

This configuration setting in the `inputs.conf` file tells Splunk to ignore any events in the monitored file that are older than 45 days from the current time. As a result, Splunk will only index events that are newer than 45 days, which is the desired behavior in this scenario.
upvoted 1 times
PrincePazol 5 months, 2 weeks ago
Selected Answer: D
ignoreOlderThan = <non-negative integer>[s|m|h|d]
* The monitor input compares the modification time on files it encounters
 with the current time. If the time elapsed since the modification time
 is greater than the value in this setting, Splunk software puts the file
 on the ignore list.

* Valid units are "d" (days), "h" (hours), "m" (minutes), and "s"
 (seconds).
upvoted 1 times
random0352 1 year, 5 months ago
Selected Answer: D
The answer is D. followTail needs to be a boolean as mentioned by others here.
upvoted 1 times
Rinkans 1 year, 7 months ago
Answer is D
upvoted 1 times
shergar 1 year, 7 months ago
Both A and D are wrong tbh, but I guess D is less wrong than A (followTail needs to be set to true or false), whereas D applies to files in a directory, not events in a file itself.
upvoted 1 times
G4ct756 1 year, 9 months ago
Selected Answer: D
Ans: likely D, due to syntax error in A.
for A, followTail is a setting is boolean, thus its either 1 or 0; true of false.
for D, although as some pointed out ignoreOlderThan uses file mod-time to filter out events, thus read directory of files not stand-alone log file. However, the syntax is correct.
upvoted 2 times
spicy7733 1 year, 11 months ago
Anser is A.
We use ignoreOlderThan for directories will lots of old files. Single file with 193 entries needs followTail
upvoted 2 times
Floyda 1 year, 11 months ago
Selected Answer: A
Answer is A.
D applies to multiple files. Question is about single file, so the limit must be applied based on extracted _time field, not last time a file was modified.
upvoted 2 times
Floyda 1 year, 11 months ago
Answer is A.
D applies to multiple files. Question is about single file, so the limit must be applied based on extracted _time field, not last time a file was modified.
upvoted 1 times
furiousjase 2 years, 10 months ago
Answer is D

Page 77 Splunk Enterprise System Administration
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Using the Splunk Community portal URL reference https://community.splunk.com/t5/Getting-Data-In/ignoreOlderThan-in-inputs-conf/m-p/358307

"When a monitoring is setup with ignoreOlderThan attribute, it'll exclude all the files which were last modified earlier than the set value."
upvoted 1 times
------------------------

60. With authentication methods are natively supported within Splunk Enterprise? (Choose all that apply.)
A. LDAP
B. SAML
C. RADIUS
D. Duo Multifactor Authentication

newrose Highly Voted 3 years, 7 months ago
A B D I think
upvoted 16 times
hwangho Highly Voted 3 years, 6 months ago
Answer: ABC
https://docs.splunk.com/Documentation/Splunk/8.1.1/Security/SetupuserauthenticationwithSplunk
upvoted 13 times
hwangho 3 years, 6 months ago
also reference this: https://docs.splunk.com/Splexicon:Userauthentication
upvoted 5 times
Frank_Rai Most Recent 3 months, 1 week ago
A, B & C

The authentication methods natively supported within Splunk Enterprise are:

A. LDAP (Lightweight Directory Access Protocol)
B. SAML (Security Assertion Markup Language)
C. RADIUS (Remote Authentication Dial-In User Service)
While Duo Multifactor Authentication can be integrated with Splunk, it is typically done through SAML or another authentication provider and not directly within Splunk Enterprise itself. Therefore, D. Duo Multifactor Authentication is not considered a natively supported authentication method within Splunk.
upvoted 2 times
bobixaka 8 months, 2 weeks ago
Selected Answer: AD
A B and D.
RADIUS requires scripting to be implemented, which means it's not "natively" supported by Splunk...
upvoted 2 times
BozhidarM 1 year ago
A B D 

https://docs.splunk.com/Documentation/Splunk/latest/Security/SetupuserauthenticationwithSplunk
upvoted 2 times
jswan382 8 months, 3 weeks ago
ABCD, in the document you referenced it includes "RADIUS":Use scripted authentication to integrate Splunk authentication with an external authentication system, such as Remote Authentication Dial-in User Service (RADIUS) or Pluggable Authentication Module (PAM).
upvoted 1 times
kolaturka 1 year, 3 months ago
A. LDAP
B. SAML
C. RADIUS

Splunk Enterprise natively supports LDAP, SAML, and RADIUS authentication methods. Duo Multifactor Authentication is not natively supported, but it can be integrated with Splunk using third-party plugins or custom scripts.
upvoted 2 times
erick165 1 year, 3 months ago
A & B are correct as we can see https://docs.splunk.com/Documentation/SplunkCloud/latest/Security/Setupbuilt-inauthentication#:~:text=Available%20in%20both%20Splunk%20Cloud,over%20any%20external%20authentication%20schemes.&text=Lightweight%20Directory%20Access%20Protocol%20(LDAP,scripted%20authentication%20(if%20enabled).
upvoted 1 times
shergar 1 year, 7 months ago
I would go for ABCD. In page 239 of System Admin slide deck, it shows the screenshot for Authentication Methods. Internal - Splunk authentication, always on. External: None/LDAP/SAML. Multifactor Authentication: None/DUO Security / RSA Security

Then in the note, it states: Scripted access to PAM, RADIUS or other user account systems are also supported. 

The unclear thing here is what exactly they mean with "natively"
upvoted 7 times
Mando22 1 year, 9 months ago
Correct Answer: A,B & C
upvoted 3 times
wts28 2 years ago
ABC - https://docs.splunk.com/Documentation/Splunk/latest/Security/Setupbuilt-inauthentication

Set up native Splunk authentication:

Native Splunk authentication lets you easily set up users to access Splunk platform resources. Available in both Splunk Cloud Platform and Splunk Enterprise, the native authentication scheme always takes precedence over any external authentication schemes.

The Splunk platform authenticates users in the following order:

Native Splunk authentication
Lightweight Directory Access Protocol (LDAP), Security Assertion Markup Language (SAML), or scripted authentication (if enabled). For more information, see the following topics:
Set up user authentication with LDAP
Set up user authentication with external systems. Scripted authentication is not available on Splunk Cloud Platform.
upvoted 1 times
denominator 2 years ago
I am still not sure because i see this:

The Splunk platform authenticates users in the following order:
https://docs.splunk.com/Documentation/Splunk/9.0.0/Security/Setupbuilt-inauthentication
1 - Native Splunk authentication
2 - Lightweight Directory Access Protocol (LDAP), Security Assertion Markup Language (SAML), or scripted authentication (if enabled). 

https://docs.splunk.com/Documentation/Splunk/9.0.0/Security/ConfigureSplunkToUsePAMOrRADIUSAuthentication
Native Splunk authentication takes precedence over any other type of authentication scheme. When you configure scripted authentication, the Splunk native authentication scheme still processes logins before passing the request onward to the scripted authentication scheme.
upvoted 1 times
king1993 2 years, 3 months ago
Answer: A and B
Supported: Splunk, LDAP, Scripted, SAML and ProxySSO
upvoted 2 times
Dori77777 2 years, 3 months ago
Selected Answer: AB
A & B
https://docs.splunk.com/Documentation/Splunk/latest/Security/Setupbuilt-inauthentication
Lightweight Directory Access Protocol (LDAP), Security Assertion Markup Language (SAML), or scripted authentication (if enabled). For more information, see the following topics:
upvoted 2 times
BlueRoselia 2 years, 4 months ago
Splunk Authentication Options
Native Splunk accounts
LDAP or AD
SAML
Scripted access to PAM, RADIUS, or other user account systems
 Saves the settings in authentication.conf
***ALSO Configuration Duo MFA -----AKA---- DUO MULTIFACTOR AUTHOTICATION
upvoted 2 times
huu_nguyen 2 years, 5 months ago
Selected Answer: AB
Only AB
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: AB
A & B for sure
D - however supported, 2FA is not listed as authentication method. It even says:
Multifactor Authentication
Not available with external authentication such as SAML.
upvoted 1 times
DeltaPotato 2 years, 10 months ago
ABD - System Admin PDF page 168 (Splunk Authentication Options). Screenshot shows LDAP/SAML/DUO. RADIUS is supported through scripting, but don't think that counts as "natively supported"?
upvoted 2 times
Load full discussion...
------------------------

61. Which configuration files are used to transform raw data ingested by Splunk? (Choose all that apply.)
A. props.conf
B. inputs.conf
C. rawdata.conf
D. transforms.conf

roblaw Highly Voted 3 years, 8 months ago
A & D, From Data Admin pdf, use transformations with props.conf and transforms.conf to:
 Mask or delete raw data as it is being indexed
Override sourcetype or host based upon event values
 Route events to specific indexes based on event content
 Prevent unwanted events from being indexed
upvoted 23 times
Frank_Rai Most Recent 3 months, 1 week ago
A & D.

The configuration files used to transform raw data ingested by Splunk are:

A. props.conf: This file is used to specify how Splunk formats incoming data, including settings for line breaking, timestamp recognition, character set encoding, and field extraction rules. It works in conjunction with transforms.conf for more advanced data transformation tasks.

D. transforms.conf: This file is used in conjunction with props.conf to define advanced data transformations, such as field extractions, data masking, and data filtering. It allows for the specification of regular expressions and other settings to extract, transform, and manipulate data.

While inputs.conf (B) is indeed a crucial configuration file in Splunk, it's used for specifying the input data settings, such as the type of input, the path for data ingestion, and various parameters for data collection, rather than transforming the data.

rawdata.conf (C) is not a standard configuration file in Splunk.
upvoted 1 times
PKUSER 5 months, 3 weeks ago
A (props.conf) is more about parsing and interpreting data, while D (transforms.conf) is focused on transforming raw data before indexing

So probably D
upvoted 1 times
k_alex 7 months ago
with SEDCMD, props.conf is ok but using transformation command, props.conf and transforms.conf will be required.
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: D
Combination of props.conf and transforms.conf is the answer.
Some transformations could be done only within props.conf, but since transforms.conf is in the possible answers, it is also a true answer.
upvoted 1 times
raizen11 1 year, 2 months ago
ABD
for transformation of raw all the three files needed
upvoted 1 times
kirtak 1 year, 2 months ago
inputs.conf is not relevant in the parsing phase
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A & D are correct
upvoted 2 times
hwangho 3 years, 6 months ago
Answer: AD
https://docs.splunk.com/Documentation/Splunk/8.1.1/Knowledge/Configureadvancedextractionswithfieldtransforms
upvoted 4 times
------------------------

83. An admin is running the latest version of Splunk with a 500 GB license. The current daily volume of new data is 300 GB per day. To minimize license issues, what is the best way to add 10 TB of historical data to the index?
A. Buy a bigger Splunk license.
B. Add 2.5 TB each day for the next 5 days.
C. Add all 10 TB in a single 24 hour period.
D. Add 200 GB of historical data each day for 50 days.

tinzs Highly Voted 2 years, 1 month ago
Selected Answer: C
getting one warning is better than risking 50 times getting warnings and end up with a violation
upvoted 5 times
mpmp22 5 months, 2 weeks ago
Disagree. It says 24h period but the License warning is based on the incoming Data until the day ends/Midnight. So if you end up putting in 10TB over 24h you might risk getting two violations. In an example like this where there arent any other variables that could mess up the 300/200 Split, D is the correct answer to minimize. In a real World example it may look different though..
upvoted 1 times
jakal12345 Highly Voted 2 years, 11 months ago
D .
300GB is already coming in daily... now you can add only 200GB more each day... this way you'll have to split the 10TB historical data over 50 days ... and this'll solve the problem 300GB + 200GB Historical day = 500GB - which is under the license violation
upvoted 5 times
Frank_Rai Most Recent 3 months, 1 week ago
D.

The best way to add 10 TB of historical data to the index without violating the daily license volume is option D: Add 200 GB of historical data each day for 50 days.

Here's why each option stands as it does:

A. Buying a bigger Splunk license would indeed solve the problem, but it's not the most cost-effective solution if you only need to index the historical data once.

B. Adding 2.5 TB each day for the next 5 days would exceed the daily license volume of 500 GB, likely causing a license violation.

C. Adding all 10 TB in a single 24-hour period would far exceed the daily volume allowed by the license, leading to a significant license violation.

D. Adding 200 GB of historical data each day for 50 days would keep the total daily volume (new data + historical data) at 500 GB, avoiding any license violations. This approach utilizes the full capacity of the license without exceeding it, allowing the historical data to be indexed systematically over time without incurring additional costs or license issues.
upvoted 1 times
adamsca 1 year ago
Selected Answer: D
I vote D it makes more sense and would minimize any license issues.
upvoted 2 times
mngesha 1 year, 5 months ago
from the discussion on the following link it seems D is the better option maybe not the right but the better.
https://docs.splunk.com/Documentation/Splunk/8.1.2/Admin/Aboutlicenseviolations
upvoted 2 times
Rolonar 1 year, 10 months ago
minimize-
reduce (something, especially something unwanted or unpleasant) to the smallest possible amount or degree.

Answer has to be D as it will allow you to ingest all the data with the least amount of license issues within the givne parameters. Also there is no way splunk is going to suggest that the best way to ingest large amounts of data is to violate their license agreement.
upvoted 3 times
BlueRoselia 2 years, 4 months ago
Answer D
Do NOT ever go over your license quota violations are pricy; IT IS NOT BEST PRACTICE
If we are only indexing 10TB of data once; no need for a license increase because it cost more money data will be a one shot upload
upvoted 2 times
Salman23 2 years, 9 months ago
C looks good to me. Adding the 10TB historical data within 24hours. Exeeding Indexing daily quota onece will trigger 1 alert then a warning but just when hitting 5 warnings in a 30 days will trigger a violation.
upvoted 1 times
jm130106 2 years, 11 months ago
https://docs.splunk.com/Documentation/Splunk/8.1.2/Admin/Aboutlicenseviolations
"An Enterprise license stack with a license volume of 100 GB of data per day or more does not currently violate."
Then the answer is C Add all 10 TB in a single 24 hour period given the 500GB license will not have a violation.
upvoted 2 times
Hudda 3 years ago
Friends, could you please confirm this answer C or D?
upvoted 1 times
mybox1 3 years, 6 months ago
C sounds better, it's one time shot. Adding 200GB data during next 50 days doesn't minimize issue since it causes 50 chances of license warnings (5 warnings causes violation)
upvoted 3 times
hwangho 3 years, 6 months ago
it does minimize license issue. we are not talking about data migration issue here. we are talking about license issue. if you are getting alert or warning, or violation....those are the license issue.
upvoted 2 times
hwangho 3 years, 6 months ago
Answer is D, since the question is asking "To minimize license issues, what is the best way to add 10 TB of historical data to the index?"....I think the key word is "minimize".
upvoted 4 times
mybox1 3 years, 6 months ago
I agree (that C) but it will be license warning, not violation.
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Question does not explicitly say license warning or even violation, instead it states, "To minimize license issues"
upvoted 1 times
AngusBlack 3 years ago
I agree. The question is not very "real world", but D would incur no license violations and therefore "minimize" license issues
upvoted 2 times
SasnycoN 2 years, 7 months ago
It says "To minimize license issues". Transferring 200GB every day when you are using 300GB daily means that even 1MB above that will trigger an alert. 
As the Splunk Documentations says: To avoid license warnings, monitor the license usage over time and ensure that you have sufficient license volume to support your daily license use
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Reference URL https://docs.splunk.com/Documentation/Splunk/8.1.2/Admin/Aboutlicenseviolations
Scrolling down to the section titled, Avoiding license warnings, reads

To avoid license warnings, monitor the license usage over time and ensure that you have sufficient license volume to support your daily license use:
- Use the license usage report view on the license to troubleshoot index volume.
- Enable an alert on the monitoring console to monitor daily license usage.
upvoted 1 times
ugo1 3 years, 8 months ago
I think the Ans is C
Adding the 10TB historical data within 24hours of license usage will trigger license violation only once.
upvoted 3 times
------------------------

150. Which of the following methods will connect a deployment client to a deployment server? (Choose all that apply.)
A. Create and edit a deploymentserver.conf file in $SPLUNK_HOME/erc/system/local on the deployment server.
B. Run $SPLUNK_HOME/bin/splunk set deploy-poll : from the command line of the deployment server.
C. Create and edit a deploymentclient.conf file in $SPLUNK_HOME/etc/system/local on the deployment client.
D. Run $SPLUNK_HOME/bin/splunk set deploy-poll : from the command line of the deployment client.

BeeQ 3 months, 2 weeks ago
Selected Answer: D
C and D are correct, You can either edit the deployment client file or set-deploy on the CLI on the deployment client.
https://docs.splunk.com/Documentation/Splunk/9.2.0/Updating/Configuredeploymentclients
upvoted 3 times
------------------------

151. What is the supported compatibility between search heads and search peers?
A. Search heads and search peers must be on the same version.
B. Search heads must be at the same version or lower than the search peers.
C. Search heads must be at the same version or higher than the search peers.
D. Search heads and search peers must be on the latest version.

MartinCaplan 3 months, 3 weeks ago
Selected Answer: C
The following rules define compatibility requirements between search heads and search peers: All instances must be running supported versions of Splunk Enterprise. The search head must be at the same or a higher level than the search peers.
upvoted 1 times
------------------------

142. Which of the following describes a Splunk deployment server?
A. A Splunk app installed on a Splunk Enterprise server.
B. A Splunk Forwarder that deploys data to multiple indexers.
C. A server that automates the deployment of Splunk Enterprise to remote servers.
D. A Splunk Enterprise server that distributes apps.

MartinCaplan 3 months, 3 weeks ago
Selected Answer: D
D is the correct answer
upvoted 2 times
------------------------

141. What is a role in Splunk? (Choose all that apply.)
A. A classification that determines if a Splunk server can remotely control another Splunk server.
B. A classification that determines what indexes a user can search.
C. A classification that determines what capabilities a user has.
D. A classification that determines what functions a Splunk server controls.

MartinCaplan 3 months, 3 weeks ago
Selected Answer: BC
BCD needs to be the answer as even server roles are also is valid as mentioned at D
upvoted 1 times
------------------------

140. Which of the following are valid methods to create a Splunk user? (Choose all that apply.)
A. API
B. None of these
C. Command line
D. SplunkWeb

MartinCaplan 3 months, 3 weeks ago
Selected Answer: ACD
ACD is the answer
upvoted 2 times
------------------------

29. What options are available when creating custom roles? (Choose all that apply.)
A. Restrict search terms.
B. Whitelist search terms.
C. Limit the number of concurrent search jobs.
D. Allow or restrict indexes that can be searched.

Asami Highly Voted 4 years ago
A. Restrict search terms.
C. Limit the number of concurrent search jobs.
D. Allow or restrict indexes that can be searched
upvoted 26 times
ucsdmiami2020 2 years, 9 months ago
Agreed A,C, D. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/SplunkCloud/8.2.2106/Admin/ConcurrentLimits

"Set limits for concurrent scheduled searches. You must have the edit_search_concurrency_all and edit_search_concurrency_scheduled capabilities to configure these settings."
upvoted 4 times
Amith Highly Voted 4 years, 2 months ago
Yes C also
upvoted 9 times
NashP Most Recent 4 months, 1 week ago
A,C,D as per sys admin pdf (P 157 158 159)
upvoted 1 times
StevenBzh 8 months, 2 weeks ago
Selected Answer: AC
Agreed too:
A. Restrict search terms.
C. Limit the number of concurrent search jobs.
D. Allow or restrict indexes that can be searched
upvoted 1 times
tmmt 1 year, 5 months ago
is ACD
upvoted 3 times
xouu 1 year, 5 months ago
Selected Answer: AC
ACD : https://docs.splunk.com/Documentation/Splunk/latest/Security/Rolesandcapabilities 
edit_search_concurrency_all :Lets a user edit settings related to maximum concurrency of searches.
upvoted 1 times
emlch 1 year, 10 months ago
ACD are the correct answer
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: AC
A, C & D are correct
upvoted 2 times
loky0 2 years, 10 months ago
ACD. P 157 158 159 in Sys admin pdf
upvoted 2 times
ckmunich 2 years, 11 months ago
A, C, D

C because: 
https://docs.splunk.com/Documentation/SplunkCloud/8.2.2106/Admin/ConcurrentLimits
upvoted 1 times
Dejavuu 2 years, 11 months ago
The answers are ACD
upvoted 1 times
hellonair 3 years ago
C must be included. So answer is ACD
At the resources tab on creating the user, Role search job limit can be set
upvoted 1 times
Sandy_1988 3 years, 7 months ago
ACD is the options
upvoted 2 times
AbuAli 4 years, 3 months ago
C. Limit the number of concurrent search jobs. >>> is true Also
upvoted 5 times
------------------------

101. Which of the following accurately describes HTTP Event Collector indexer acknowledgement?
A. It requires a separate channel provided by the client.
B. It is configured the same as indexer acknowledgement used to protect in-flight data.
C. It can be enabled at the global setting level.
D. It stores status information on the Splunk server.

kiranhar Highly Voted 2 years, 11 months ago
Answer is A
upvoted 7 times
not_another_user_007 2 years, 10 months ago
https://docs.splunk.com/Documentation/SplunkCloud/8.2.2107/Data/AboutHECIDXAck

Sending events to HEC with indexer acknowledgment active is similar to sending them with the setting off. There is one crucial difference: when you have indexer acknowledgment turned on, you must specify a channel when you send events.

The concept of a channel was introduced in HEC primarily to prevent a fast client from impeding the performance of a slow client. When you assign one channel per client, because channels are treated equally on Splunk Enterprise, one client can't affect another.

You must include a matching channel identifier both when sending data to HEC in an HTTP request and when requesting acknowledgment that events contained in the request have been indexed. If you don't, you will receive the error message, "Data channel is missing." Each request that includes a token for which indexer acknowledgment has been enabled must include a channel identifier, as shown in the following example cURL statement, where <data> represents the event data portion of the request:
upvoted 3 times
allahsal Most Recent 4 months, 2 weeks ago
Selected Answer: A
Each client request must provide a channel(a unique identifier created by the client) Data p 172
upvoted 1 times
yaman778 11 months ago
Selected Answer: B
B should be right answer. While HEC has precautions in place to prevent data loss, its impossible to completely prevent such an occurrence, especially in event of NW failure or HW crash.
https://docs.splunk.com/Documentation/Splunk/9.1.0/Data/AboutHECIDXAck
upvoted 1 times
Seba0297 2 years, 2 months ago
Selected Answer: A
Answer is A
https://docs.splunk.com/Documentation/SplunkCloud/8.2.2107/Data/AboutHECIDXAck#:~:text=when%20you%20have%20indexer%20acknowledgment%20turned%20on%2C%20you%20must%20specify%20a%20channel%20when%20you%20send%20events.
upvoted 3 times
furiousjase 2 years, 10 months ago
Confirmed it is A
https://docs.splunk.com/Documentation/Splunk/8.2.2/Data/AboutHECIDXAck - Section: About channels and sending data
upvoted 3 times
------------------------

71. The volume of data from collecting log files from 50 Linux servers and 200 Windows servers will require multiple indexers. Following best practices, which types of
Splunk component instances are needed?
A. Indexers, search head, universal forwarders, license master
B. Indexers, search head, deployment server, universal forwarders
C. Indexers, search head, deployment server, license master, universal forwarder
D. Indexers, search head, deployment server, license master, universal forwarder, heavy forwarder

roblaw Highly Voted 3 years, 8 months ago
C. All search heads and indexers should use a license master
upvoted 17 times
Splunkv Highly Voted 3 years, 3 months ago
Did anybody notice that "s" is missing from "universal forwarder" in option C. whereas all other components are given as plural. so I would go with A.
upvoted 9 times
allahsal Most Recent 4 months, 2 weeks ago
Selected Answer: C
https://community.splunk.com/t5/Knowledge-Management/The-volume-of-data-from-collecting-log-files-from-50-Linux/m-p/522684
upvoted 1 times
kolaturka 1 year, 3 months ago
Option C is the correct answer.

According to best practices, a distributed deployment architecture is recommended for large-scale data ingestion and search operations. In this scenario, the volume of data from 50 Linux servers and 200 Windows servers requires multiple indexers, a search head, a deployment server, a license master, and universal forwarders.

The indexers are responsible for storing and indexing the data, while the search head is responsible for managing and processing search requests. The deployment server is used to centrally manage configurations across multiple components in the deployment, and the license master is used to centrally manage Splunk licenses. Finally, the universal forwarder is installed on the servers that generate the data to forward the data to the indexers.
upvoted 2 times
erick165 1 year, 3 months ago
Selected Answer: B
B is the correct one because it says needed, the license master and the HF are recomendations for best practice but not needed. also the option B as the UFs in plural and the opcion C doesn't
upvoted 1 times
nupacniyiveli 1 year, 11 months ago
Selected Answer: C
C is correct
upvoted 2 times
cagdaskarabag 1 year, 11 months ago
https://community.splunk.com/t5/Knowledge-Management/The-volume-of-data-from-collecting-log-files-from-50-Linux/m-p/522684
Answer is A.
upvoted 1 times
BlueRoselia 2 years, 4 months ago
C System Admin module 9 pg 196
upvoted 1 times
loky0 2 years, 10 months ago
I'd say C. License master is definitely recommended with multiple indexers. Since we have multiple servers, well likely use a lot of UFs, so deployment servers will be good to monitor UFs.
upvoted 1 times
Hudda 3 years ago
what is the final answer here pls confirm friends :)
upvoted 2 times
toney_mu 1 year, 5 months ago
C option
upvoted 1 times
Robo187 3 years, 3 months ago
I would add two heavy forwarders as intermediate forwarders for each linux and unix inputs
upvoted 2 times
toney_mu 1 year, 5 months ago
You may add for better design, but its not necessary
upvoted 1 times
Sandy_1988 3 years, 7 months ago
I think C is the correct answer
upvoted 1 times
hsing 3 years, 7 months ago
B, since the license master can reside on the search head/deployment instance
upvoted 2 times
Racgud 3 years, 7 months ago
Wrong, C i correct
upvoted 3 times
Ashton_98 3 years, 7 months ago
Because it asks for 'component', it doesn't matter where it sits.
upvoted 2 times
------------------------

19. Which Splunk component requires a Forwarder license?
A. Search head
B. Heavy forwarder
C. Heaviest forwarder
D. Universal forwarder

BMO Highly Voted 3 years, 1 month ago
B is correct
Data Admin - Slide 83
System Admin - Slide 42
upvoted 15 times
tjwe 2 years, 8 months ago
I agree, on system admin - slide 44 it says: Forwarder license: Sets the server up as a heavy forwarder.
upvoted 1 times
dpharker Highly Voted 3 years, 9 months ago
This is a tricky question, because both HF and UF require a license, but the question asks which require a Forwarder License.
The HF uses a Enterprise License to be able to parse or index data.
The UF comes with a built-in license, but it is a license for forwarding.

So when they ask which component requires a Forwarder License, it's the UF
Correct answer is D.
upvoted 15 times
happy_and_lucky 3 years, 6 months ago
I think D too because https://docs.splunk.com/Documentation/Splunk/8.0.1/Admin/Distdeploylicenses
"***Universal forwarders only need a Forwarder license.***

 If a heavy forwarder is performing additional functions such as indexing data or managing searches, it requires access to an Enterprise license."
upvoted 5 times
allahsal Most Recent 4 months, 3 weeks ago
Selected Answer: B
Forwarder license
 Sets the server up as a heavy forwarder
 Applies to non-indexing forwarders
 Allows authentication, but no indexing
upvoted 1 times
allahsal 4 months, 2 weeks ago
I was wrong, the answer is D
upvoted 1 times
allahsal 4 months, 3 weeks ago
Selected Answer: D
https://docs.splunk.com/Documentation/Splunk/8.0.1/Admin/Distdeploylicenses
"***Universal forwarders only need a Forwarder license.***
upvoted 1 times
allahsal 4 months, 3 weeks ago
D is the correct answer.
https://docs.splunk.com/Documentation/Splunk/8.0.1/Admin/Distdeploylicenses
"***Universal forwarders only need a Forwarder license.***
upvoted 1 times
Vidomina 4 months, 3 weeks ago
Selected Answer: D
I should be D. 
The universal forwarder has the Forwarder license applied automatically.
Heavy Forwarder has Enterprise or Forwarder license
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: D
HF requires Enterprise License
UF comes with a free "Forwarding License"
upvoted 3 times
emlch 1 year, 10 months ago
Selected Answer: B
The HF Requires a Splunk Enterprise Instance with the Forwarder License enabled.

The UF is provided as separate installation with a built-in license
upvoted 2 times
MxQ3 1 year, 11 months ago
Heavy Forwarder for sure
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 1 times
gabo1969 2 years, 7 months ago
UF Dont require licence to work
upvoted 1 times
inwigboji 2 years, 9 months ago
Correct answer is B. UF already has an inbuilt FL and won't require it. HF need manual configuration to use FL
upvoted 1 times
not_another_user_007 2 years, 10 months ago
This question is worded wrong. It should be asking which forwarding component requires it's own licence. The answer then would be D UF. The HF uses pool licence (ie enterprise licence). This question is also covered in Splunk Architect.

I would go D UF
upvoted 3 times
loky0 2 years, 10 months ago
It's B. See P42 on the latest Sys Admin pdf or P67 on the Data Admin pdf. "Forwarder License sets the server up as heavy forwarder"
upvoted 2 times
gsplunker 3 years, 5 months ago
It should be B as
 Forwarder license
 Sets the server up as a heavy forwarder
 Applies to non-indexing forwarders
 Allows authentication, but no indexing
upvoted 5 times
amsinha 3 years, 5 months ago
UF is the answer. refer to this:
https://docs.splunk.com/Splexicon:Forwardinglicense
The universal forwarder package includes its own license. The license is enabled or applied automatically. This license allows forwarding but not indexing of unlimited data, and also enables security on the forwarder so that users must supply a user name and password to access it.
The heavy forwarder should have access to an Enterprise license stack if you plan to perform indexing on the forwarder or to enable authentication on the forwarder.
The light forwarder uses the same license as the universal forwarder. The light forwarder has been deprecated in Splunk Enterprise 6.2.0.
upvoted 2 times
sargeholik 3 years, 6 months ago
pag 47 data admin, heavy forwarder is correct
upvoted 2 times
Load full discussion...
------------------------

51. How often does Splunk recheck the LDAP server?
A. Every 5 minutes.
B. Each time a user logs in.
C. Each time Splunk is restarted.
D. Varies based on LDAP_refresh setting.

giubal Highly Voted 4 years, 2 months ago
I think the correct answer is B
upvoted 11 times
Vidomina Most Recent 4 months, 3 weeks ago
Selected Answer: B
The LDAP server is rechecked each time a user logs into Splunk
upvoted 1 times
toney_mu 1 year, 5 months ago
Answer would be B for sure, but if there and multiple choice D is also an option
upvoted 1 times
Marco63 2 years, 2 months ago
Selected Answer: B
See appendix on Data Admin slides
upvoted 3 times
Marco63 2 years, 2 months ago
System Admin, sorry.
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 3 times
Pratik18 2 years, 11 months ago
answer is B Page number 223 sys admin pdf
upvoted 2 times
CCSHAO 3 years ago
B is correct. Sys Admin - Appendix A Splunk Authentication Management. Slide 218.
upvoted 4 times
Marco63 2 years, 2 months ago
Confirm. See the appendix.
upvoted 1 times
ArDeKu 3 years, 3 months ago
It will be B..Page 218 of System Admin..
upvoted 3 times
mybox1 3 years, 6 months ago
" The LDAP server is rechecked each time a user logs into Splunk" (System Administration PDF, page 237), so answer B is correct one
upvoted 2 times
dpharker 3 years, 9 months ago
correct answer is B
this doc describe how it works.
https://docs.splunk.com/Documentation/Splunk/8.0.6/Security/ManageSplunkuserroleswithLDAP
upvoted 2 times
Amith 4 years, 2 months ago
B lah bro this one
upvoted 3 times
japm 4 years, 2 months ago
I've check on Splunk Administrator pdf and you are right giubal
upvoted 2 times
------------------------

17. What is required when adding a native user to Splunk? (Choose all that apply.)
A. Password
B. Username
C. Full Name
D. Default app

Racgud Highly Voted 3 years, 7 months ago
Splunk system admin slides page 144 CLEARLY shows that Name and Password is REQUIRED, while the rest is optional/set by default.
Thus A and B are correct
upvoted 31 times
newrose 3 years, 7 months ago
Agreed
upvoted 6 times
Vidomina Most Recent 4 months, 3 weeks ago
Correct are: A and B, a role is required too (not an option here). C Full Name is optional, not required and D is just a default app which is preselected. Doesn't look required.
upvoted 1 times
gatundu_ 6 months ago
Full name is optional. Answer is A&B.
upvoted 1 times
Ibisc 1 year, 1 month ago
Selected Answer: AB
Agree with Racgud. The page for version 9.0 is 223
upvoted 1 times
akamit225 1 year, 2 months ago
A B, rest are optional
upvoted 1 times
ALX951 1 year, 9 months ago
Selected Answer: AB
La A y B con correctas
upvoted 4 times
da_stingo 2 years, 2 months ago
Selected Answer: AB
vote for AB
upvoted 2 times
king1993 2 years, 3 months ago
Answer: A and B
upvoted 3 times
RedYeti 2 years, 3 months ago
Selected Answer: AB
A. Password
B. Username
System Admin course, page 154
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: AB
A and B are correct
upvoted 2 times
teems5uk 2 years, 7 months ago
This is confusing. https://docs.splunk.com/Documentation/Splunk/8.2.3/Security/Addandeditusers
upvoted 1 times
M9201715 2 years, 7 months ago
You need to define the username, password and role when creating a user. It's not obvious if you do it through the web interface since there are a lot of fields that get filled in with defaults so it's unclear which are required and which are optional. But if you do it with the "add user" command in the CLI, you have to specify username, password and role. Since role is not one of the options in this question, the answer is just A and B.
upvoted 1 times
Powdered_Sugar 2 years, 8 months ago
I just made a new user in Splunk.
Username, Password, and Default App are all required. Full Name is optional.

A, B, and D are correct
upvoted 2 times
[Removed] 2 years, 9 months ago
Username, Password and a Role are required. Everything else the user can make those changes.
upvoted 1 times
malice4 3 years, 2 months ago
Only a password and a username are required, hence A and B.
upvoted 2 times
goal1860 3 years, 3 months ago
A, B are pretty sure. D is vague, as a default app is pre-selected for you anyway.
upvoted 3 times
ArDeKu 3 years, 3 months ago
It will be ABCD refer page 153
upvoted 1 times
ArDeKu 3 years, 3 months ago
ACD same page as username doesn't exist it is mentioned as Name
upvoted 1 times
Load full discussion...
------------------------

66. Which additional component is required for a search head cluster?
A. Deployer
B. Cluster Master
C. Monitoring Console
D. Management Console

hwangho Highly Voted 3 years, 6 months ago
A is correct.
https://docs.splunk.com/Documentation/Splunk/8.1.1/DistSearch/SHCdeploymentoverview
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Per the provided URL reference
In addition to the set of search head members that constitute the actual cluster, a functioning cluster requires several other components:

The deployer. This is a Splunk Enterprise instance that distributes apps and other configurations to the cluster members. It stands outside the cluster and cannot run on the same instance as a cluster member. It can, however, under some circumstances, reside on the same instance as other Splunk Enterprise components, such as a deployment server or an indexer cluster master node.
upvoted 1 times
Gycu Most Recent 5 months, 1 week ago
Selected Answer: A
DEPLOYER
upvoted 1 times
kolaturka 1 year, 3 months ago
According to the Splunk documentation on search head clustering, a Deployer is an optional component that can be used to distribute app and configuration updates to the members of a search head cluster. Therefore, Option A is the correct answer to the question, "Which additional component is required for a search head cluster?"
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 2 times
------------------------

74. Which configuration file would be used to forward the Splunk internal logs from a search head to the indexer?
A. props.conf
B. inputs.conf
C. outputs.conf
D. collections.conf

hwangho Highly Voted 3 years, 6 months ago
C is correct.
https://docs.splunk.com/Documentation/Splunk/8.1.1/DistSearch/Forwardsearchheaddata
upvoted 9 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Splunk reference URL by @hwangho, scroll to section Forward search head data, subsection titled, 2. Configure the search head as a forwarder.
"Create an outputs.conf file on the search head that configures the search head for load-balanced forwarding across the set of search peers (indexers)."
upvoted 1 times
PrincePazol Most Recent 5 months, 2 weeks ago
Selected Answer: C
In outputs.conf:
[tcpout]
defaultGroup = indexers1

[indexAndForward]
index=false

[tcpout:indexers1]
server = 10.1.1.197:9997, 10.1.1.200:9997
upvoted 1 times
CactiAZ 7 months, 4 weeks ago
Selected Answer: C
This community usually gets these questions right, but I'm surprised by how many are putting the wrong answer. The correct answer is C. See the link in hwangho's post. Search heads, and all Splunk instances, already have inputs built to read internal logs by default. They just need an outputs.conf to create a tcpout stanza to your indexers to get them to send their internal logs, which is what this question is asking about. In our Splunk environment we have NEVER set up an inputs for internal logs, we only deploy an outputs.conf with our indexers listed in a tcpout stanza, and we get all of our internal logs just fine.

If you had other logs on a search head (like from a script or something), then yes, you would need an inputs.conf to get those to be read. But that is definitely not what this question is asking about.
upvoted 1 times
yaman778 11 months, 1 week ago
Selected Answer: B
B for sure. inputs.conf allows you to define data inputs that the Splunk instance should monitor and forward to indexers.
Use monitor stanza specifying the path to log files and destination indexers host name, port.
Stanza Sample
[monitor:///opt/splunk/var/log/splunk]
Index = _internal
Soucetype = Splunkd
Disabled = false
_TCP_ROUTING = indexer_group
upvoted 1 times
kolaturka 1 year, 3 months ago
he correct answer is B. inputs.conf is used to configure the inputs on a Splunk instance, including forwarding data from one instance to another. In this case, to forward the Splunk internal logs from a search head to the indexer, you would need to add a stanza to inputs.conf on the search head that specifies the indexer as the destination for the logs. The props.conf file is used to configure how data is processed after it has been indexed, outputs.conf is used to configure the destination of data for specific stanzas, and collections.conf is used for managing data in collections.
upvoted 1 times
anonyuser 1 year, 7 months ago
Just for a little clarification, configuring the sh as a forwarder using outputs.conf does not necessarily tell the sh to send a certain type of data that you would use inputs.conf for. However, this is talking about _internal, which I believe is data that is sent by default, without the need for inputs.conf. Please correct me if I am wrong here
upvoted 1 times
Hudda 3 years ago
Friends, could you please confirm this answer?
upvoted 2 times
------------------------

106. Which of the following monitor inputs stanza headers would match all of the following files?
/var/log/www1/secure.log
/var/log/www/secure.l
/var/log/www/logs/secure.logs
/var/log/www2/secure.log
A. [monitor:///var/log/.../secure.*]
B. [monitor:///var/log/www1/secure.*]
C. [monitor:///var/log/www1/secure.log]
D. [monitor:///var/log/www*/secure.*]

furiousjase Highly Voted 2 years, 10 months ago
Confirmed - A - "The ellipsis wildcard recursesthrough directories and subdirectories to match." 

"The asterisk wildcard matches anything in that specific directory path segment but does not go beyond that segment in the path. Normally it should be used at the end of a path."
upvoted 10 times
ucsdmiami2020 2 years, 10 months ago
Per Splunk docs reference URL https://docs.splunk.com/Documentation/Splunk/8.2.1/Data/Specifyinputpathswithwildcards
Wildcard Description Regex equivalent Examples
... The ellipsis wildcard searches recursively through directories and any number of levels of subdirectories to find matches.
If you specify a folder separator (for example, //var/log/.../file), it does not match the first folder level, only subfolders.

.* /foo/.../bar.log matches /foo/1/bar.log, /foo/2/bar.log, /foo/1/2/bar.log, and so on. It does not match /foo/bar.log or /foo/3/notbar.log.

Because a single ellipse searches recursively through all folders and subfolders, /foo/.../bar.log matches /foo/.../.../bar.log.
upvoted 1 times
yybbb Most Recent 5 months, 2 weeks ago
Selected Answer: A
The third file has one more layer of subdir so "..." is needed. Therefore A is the only answer.
upvoted 1 times
HNaka 5 months, 3 weeks ago
Answer is A!!!!!!
upvoted 1 times
solomone 1 year, 1 month ago
Selected Answer: A
There are two different WWW folders. You need ... (three dots variable) to traverse all of them.
upvoted 1 times
pro12345 1 year, 9 months ago
Selected Answer: A
Confirmed - A
upvoted 1 times
fabtastic 2 years, 1 month ago
Selected Answer: A
Answer is A
upvoted 1 times
kiranhar 2 years, 10 months ago
answer is A
upvoted 1 times
kiranhar 2 years, 10 months ago
Answer is A
upvoted 1 times
------------------------

3. In case of a conflict between a whitelist and a blacklist input setting, which one is used?
A. Blacklist
B. Whitelist
C. They cancel each other out.
D. Whichever is entered into the configuration first.

yybbb 5 months, 2 weeks ago
Selected Answer: A
A. blacklist
upvoted 1 times
emlch 1 year, 10 months ago
In case of a conflict the blacklist prevails
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 4 times
BengieQuesada 2 years, 11 months ago
A is correct Data Admin slide 123
upvoted 4 times
ZeusP 3 years, 1 month ago
Blacklist always overrides Whitelist
upvoted 1 times
Kobi 3 years, 4 months ago
Blacklist Overides Whitelist
upvoted 4 times
newrose 3 years, 7 months ago
"It is not necessary to define both an allow list and a deny list in a configuration stanza. The settings are independent. If you do define both filters and a file matches them both, Splunk Enterprise does not index that file, as the blacklist filter overrides the whitelist filter."
Source: https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/Whitelistorblacklistspecificincomingdata
upvoted 4 times
Praf7 3 years, 8 months ago
A. Blacklist
upvoted 2 times
------------------------

78. The CLI command splunk add forward-server indexer:<receiving-port> will create stanza(s) in which configuration file?
A. inputs.conf
B. indexes.conf
C. outputs.conf
D. servers.conf

ugo1 Highly Voted 3 years, 8 months ago
The Ans is C
The CLI command "Splunk add forward-server indexer:<receiving-port>" is used to define the indexer and the listening port on forwards.
The command creates this kind of entry "[tcpout-server://<ip address>:<port>]" in the outputs.conf file.
upvoted 24 times
ucsdmiami2020 2 years, 10 months ago
Refer to the Splunk docs URL for "Configure forwarding with outputs.conf" found in the Forwarder Manual
https://docs.splunk.com/Documentation/Forwarder/8.2.2/Forwarder/Configureforwardingwithoutputs.conf
Section titled, Examples of outputs.conf supports the answer being C
upvoted 2 times
HNaka Most Recent 5 months, 3 weeks ago
Selected Answer: C
To configure target indexers on forwarders, either:
 Run splunk add forward-server <indexer:receiving_port> 
 Modify outputs.conf
upvoted 1 times
toney_mu 1 year, 5 months ago
I would say its option C

Set up forwarding
You can use the CLI as a quick way to enable forwarding.

You can also enable, as well as configure, forwarding by creating an outputs.conf file on the Splunk instance. Although setting up forwarders with outputs.conf requires a bit more initial knowledge, there are obvious advantages to performing all forwarder configurations in a single location
upvoted 1 times
mngesha 1 year, 5 months ago
This is the exact quote from https://docs.splunk.com/Documentation/Splunk/9.0.3/Forwarding/Deployaforwarder
"Set up forwarding
You can use the CLI as a quick way to enable forwarding.
You can also enable, as well as configure, forwarding by creating an outputs.conf file on the Splunk instance.
upvoted 2 times
toney_mu 1 year, 5 months ago
Yes, its option C
upvoted 1 times
emlch 1 year, 10 months ago
outputs.conf (you're defining target indexers on the forwarder)
upvoted 1 times
snarbles 2 years, 2 months ago
Selected Answer: C
This command adds an indexer to the outputs.conf.
upvoted 2 times
janogerr32 2 years, 9 months ago
Ans is C. pg 181 in Sys Admin
upvoted 2 times
Salman23 2 years, 9 months ago
add forwarder-server is the command to let forwarder know where to send the data so this will be data going out to the indexer. so this will be a stanza on the outputs.conf file
upvoted 1 times
Hudda 3 years ago
Friends, C is final. Could you please confirm this answer?
upvoted 4 times
mosematt 3 years, 1 month ago
correct answer is C
upvoted 3 times
------------------------

34. Which of the following statements apply to directory inputs? (Choose all that apply.)
A. All discovered text files are consumed.
B. Compressed files are ignored by default.
C. Splunk recursively traverses through the directory structure.
D. When adding new log files to a monitored directory, the forwarder must be restarted to take them into account.

Eku Highly Voted 4 years, 2 months ago
Answer should be A and C
upvoted 15 times
Mimi88 Highly Voted 3 years, 9 months ago
Ans. A & C. See Monitoring Directories in the Data Administration PDF
upvoted 6 times
HNaka Most Recent 5 months, 3 weeks ago
Selected Answer: C
Answer is A and C
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: A
A and C
upvoted 1 times
emlch 1 year, 10 months ago
Monitoring directories: recursively traverses directory and monitors all discovered text files, unzips compressed files, includes new files added to the directories.
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A &C are correct
upvoted 4 times
akkki 3 years, 9 months ago
From where, can I download data administration pdf?
upvoted 2 times
Racgud 3 years, 7 months ago
you can't. You have to sign up and pay for the Data admin course. Then you will receive a PDF which it is illegal to share (watermarked with your name)
upvoted 13 times
Amith 4 years, 2 months ago
A and C are the answers, Most multiple answers are not selected in this site
upvoted 6 times
AbuAli 4 years, 3 months ago
I think A is True Also
upvoted 4 times
------------------------

137. Which data pipeline phase is the last opportunity for defining event boundaries?
A. Input phase
B. Indexing phase
C. Parsing phase
D. Search phase

HNaka 5 months, 3 weeks ago
Selected Answer: C
1. Input phase: Handled at the source (usually a forwarder)
 The data sources are being opened and read
 Data is handled as streams; configuration settings are applied to the entire stream
2. Parsing phase: Handled by indexers (or heavy forwarders)
 Data is broken up into events and advanced processing can be performed
3. Indexing phase: Handled by indexers
 License meter runs as data is initially written to disk, prior to compression
 After data is written to disk, it cannot be changed
upvoted 1 times
cb42 1 year, 7 months ago
Selected Answer: C
Parsing phase respects LINE_BREAKER, SHOULD_LINEMERGE, BREAK_ONLY_BEFORE_DATE, and all other line merging settings in props.conf
upvoted 2 times
------------------------

126. What is the command to reset the fishbucket for one source?
A. rm -r ~/splunkforwarder/var/lib/splunk/fishbucket
B. splunk clean eventdata -index _thefishbucket
C. splunk cmd btprobe -d SPLUNK_HOME/var/lib/splunk/fishbucket/splunk_private_db --file --reset
D. splunk btool fishbucket reset

HNaka 5 months, 3 weeks ago
Answer is C.
B is for reset all sources
upvoted 2 times
Hemnaath 1 year, 6 months ago
Splunk system admin slides page 141 CLEARLY shows that to reset individually for each source using the below command 
splunk cmd btprobe -d SPLUNK_HOME/var/lib/splunk/fishbucket/splunk_private_db --file <source> --reset
upvoted 4 times
cb42 1 year, 7 months ago
C: seems to be the best answer, but the --file <argument> is missing before --reset.
upvoted 1 times
------------------------

103. Which of the following is accurate regarding the input phase?
A. Breaks data into events with timestamps.
B. Applies event-level transformations.
C. Fine-tunes metadata.
D. Performs character encoding.

islamjy Highly Voted 2 years, 10 months ago
D see data admin 193
upvoted 9 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/latest/Deploy/Datapipeline

"The data pipeline segments in depth. INPUT - In the input segment, Splunk software consumes data. It acquires the raw data stream from its source, breaks it into 64K blocks, and annotates each block with some metadata keys. The keys can also include values that are used internally, such as the character encoding of the data stream, and values that control later processing of the data, such as the index into which the events should be stored. PARSING Annotating individual events with metadata copied from the source-wide keys. Transforming event data and metadata according to regex transform rules."
upvoted 2 times
HNaka Most Recent 5 months, 3 weeks ago
During the input phase, Splunk sets all input data to UTF-8 encoding
by default
upvoted 1 times
saurabhsood 2 years ago
Selected Answer: D
ddddddddddddddddddd
upvoted 1 times
Seba0297 2 years, 2 months ago
Selected Answer: D
p.206 Data Admin - Character Encoding
"During the input phase, Splunk sets all input data to UTF-8 encoding by default"
upvoted 2 times
BlueRoselia 2 years, 4 months ago
Answer D
UF inside props.conf limited parsing such as character encoding, refine metadata, event breaks
indexer inside props.conf refines metadata at event level, event break, time extraction, tx, data transformation
upvoted 1 times
Fe01 2 years, 5 months ago
Answer is D. "Character coding...", Data Admin, p192
upvoted 2 times
Salman23 2 years, 9 months ago
A is parsing phase , correct is D.
upvoted 2 times
furiousjase 2 years, 10 months ago
Confirmed D - "During the input phase, Splunk sets all input data to UTF-8 encoding by default  Can be overridden, if needed, by setting the CHARSETattribute"
upvoted 2 times
kiranhar 2 years, 10 months ago
Answer is C
upvoted 2 times
kiranhar 2 years, 10 months ago
Sorry answe is A
upvoted 1 times
kiranhar 2 years, 11 months ago
Answer is C
upvoted 2 times
------------------------

99. When running a real-time search, search results are pulled from which Splunk component?
A. Heavy forwarders and seach peers
B. Heavy forwarders
C. Search heads
D. Search peers

[Removed] Highly Voted 2 years, 11 months ago
search peers
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Agree Answer is D. Using the Splunk reference URL https://docs.splunk.com/Splexicon:Searchpeer

"search peer is a splunk platform instance that responds to search requests from a search head. The term "search peer" is usally synonymous with the indexer role in a distributed search topology. However, other instance types also have access to indexed data, particularly internal diagnostic data, and thus function as search peers when they respond to search requests for that data."
upvoted 1 times
Seba0297 Highly Voted 2 years, 2 months ago
Selected Answer: D
Regardless of real-time searches or not, should it always be "Search peers"?
upvoted 6 times
HNaka Most Recent 5 months, 3 weeks ago
According to "Search Phase: The Big Picture" in Data Admin pdf....

- Normal search-> access to Index in Indexer
- Real Time search-> access to Indexing Queue between Parsing pipeline and Indexing Pipeline in Indexer

That means Search Peer.
upvoted 2 times
------------------------

89. How is data handled by Splunk during the input phase of the data ingestion process?
A. Data is treated as streams.
B. Data is broken up into events.
C. Data is initially written to disk.
D. Data is measured by the license meter.

Toanbego Highly Voted 3 years, 8 months ago
A. Data is handled as streams during input phase.
ref. Data Admin course pdf, slide 14
upvoted 17 times
ucsdmiami2020 2 years, 9 months ago
Agreed answer is A. Quoting the reference URL https://docs.splunk.com/Documentation/Splunk/8.0.5/Deploy/Datapipeline

"In the input segment, Splunk software consumes data. It acquires the raw data stream from its source, breaks in into 64K blocks, and annotates each block with some metadata keys."
upvoted 1 times
trevero Highly Voted 3 years ago
Correct Ans is ( A) per Data Admin pdf page 201
upvoted 5 times
HNaka Most Recent 5 months, 3 weeks ago
Ans is A.

DataAdmin
Index-Time Process
1. Input phase: Handled at the source (usually a forwarder)
 The data sources are being opened and read
 Data is handled as streams; configuration settings are applied to the entire stream
2. Parsing phase: Handled by indexers (or heavy forwarders)
 Data is broken up into events and advanced processing can be performed
3. Indexing phase: Handled by indexers
 License meter runs as data is initially written to disk, prior to compression
 After data is written to disk, it cannot be changed
upvoted 3 times
toney_mu 1 year, 5 months ago
A. Data is treated as streams.
upvoted 1 times
mngesha 1 year, 5 months ago
The correct answer is A which is done in the input phase while the writing to disk is complete in the indexing phase
https://docs.splunk.com/Documentation/Splunk/8.0.5/Deploy/Datapipeline
upvoted 1 times
denominator 2 years ago
Selected Answer: A
as streams
upvoted 3 times
denominator 2 years ago
Data Admin pg 200 | input phase data handled as streams Answer A
upvoted 1 times
gsplunker 3 years, 5 months ago
Data is handled as streams then parsed and written to disk so answer is A
upvoted 2 times
Shaq007 3 years, 6 months ago
A. Data is treated as streams.
Correct Answer per Data Admin pdf
upvoted 3 times
------------------------

25. Within props.conf, which stanzas are valid for data modification? (Choose all that apply.)
A. Host
B. Server
C. Source
D. Sourcetype

Praf7 Highly Voted 3 years, 8 months ago
Option - A,C & D are correct.
upvoted 24 times
happy_and_lucky 3 years, 6 months ago
https://docs.splunk.com/Documentation/Splunk/8.1.1/Admin/Propsconf
upvoted 1 times
sargeholik Highly Voted 3 years, 7 months ago
ACD correct answer, page 151 data admin
upvoted 5 times
gatundu_ Most Recent 6 months ago
Host, source and source type.
A, C & D are correct
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: A
A, C and D
upvoted 1 times
Mando22 1 year, 9 months ago
A,C & D are correct.
https://docs.splunk.com/Documentation/Splunk/8.0.4/Admin/Propsconf#props.conf.spec
https://docs.splunk.com/Documentation/Splunk/8.1.1/Admin/Propsconf
upvoted 1 times
denominator 2 years ago
Data Admin pdf, pg 205. A, C, D
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: CD
A, C & D are correct
upvoted 3 times
loky0 2 years, 10 months ago
ACD. P206 in the new data admin pdf
upvoted 3 times
gsplunker 3 years, 5 months ago
A,C,D is the correct answer
upvoted 4 times
Sandy_1988 3 years, 7 months ago
ACD are the options
upvoted 3 times
Praf7 3 years, 8 months ago
I have used source type in my env not sure about the source. Haven't used
upvoted 1 times
Toanbego 3 years, 8 months ago
I would assume it is bad practice to alter the source type at different stages. Not something that often change in my experience, haha. Would stick with A, B and D. I know those can change
upvoted 1 times
Toanbego 3 years, 8 months ago
Nevermind. Seems you are correct. Reviewing the data admin PDF shows that data modification at least has the variables for host, source and sourcetype
upvoted 3 times
------------------------

54. Which Splunk component performs indexing and responds to search requests from the search head?
A. Forwarder
B. Search peer
C. License master
D. Search head cluster

ames Highly Voted 3 years, 10 months ago
True, B. <https://docs.splunk.com/Splexicon:Searchpeer>
upvoted 10 times
ucsdmiami2020 2 years, 9 months ago
Per the Splunk provided URL reference
"A Splunk platform instance that responses to search requests from a search head. The term "Search peer" is usually synonymous with the indexer role in a distributed search topology..."
upvoted 1 times
amporiik Highly Voted 3 years, 11 months ago
B. Search peer
upvoted 5 times
k_alex Most Recent 7 months ago
Search peer is another name of the indexer
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 2 times
1M4hqQ9G 2 years, 10 months ago
search peer a.k.a. indexer
upvoted 1 times
------------------------

68. Assume a file is being monitored and the data was incorrectly indexed to an exclusive index. The index is cleaned and now the data must be reindexed. What other index must be cleaned to reset the input checkpoint information for that file?
A. _audit
B. _checkpoint
C. _introspection
D. _thefishbucket

roblaw Highly Voted 3 years, 8 months ago
D. _thefishbucket, it's purpose is to contain checkpoint information for file monitoring inputs.
upvoted 21 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: D
Absolutely, the answer is D.
upvoted 1 times
BozhidarM 1 year ago
D is correct
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 3 times
rawghav 2 years, 7 months ago
D is the right option
upvoted 2 times
Salman23 2 years, 9 months ago
D is correct, Sysadmin page 105
upvoted 3 times
loky0 2 years, 10 months ago
D. P132 Data admin pdf
upvoted 3 times
hwangho 3 years, 6 months ago
Answer: D 
 --reset Reset the fishbucket for the given key or file in the btree.
 Resetting the checkpoint for an active monitor input reindexes data, resulting in increased license use.
https://docs.splunk.com/Documentation/Splunk/8.1.1/Troubleshooting/CommandlinetoolsforusewithSupport
upvoted 4 times
radskman 3 years, 8 months ago
Shouldn't be D ?
https://community.splunk.com/t5/Getting-Data-In/How-to-reindex-data-from-a-forwarder/m-p/93310
upvoted 4 times
------------------------

65. Which is a valid stanza for a network input?
A. [udp://172.16.10.1:9997] connection = dns sourcetype = dns
B. [any://172.16.10.1:10001] connection_host = ip sourcetype = web
C. [tcp://172.16.10.1:9997] connection_host = web sourcetype = web
D. [tcp://172.16.10.1:10001] connection_host = dns sourcetype = dns

roblaw Highly Voted 3 years, 8 months ago
D. connection_host attributes: dns (TCP), ip (UDP), none (UI)
upvoted 19 times
Hamiltonian 3 years ago
Confirmation in inputs.conf under TCP: connection_host = [ip|dns|none]. Thus, web does not exist as an option and must be answer D.
upvoted 4 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: D
D is the correct answer
upvoted 1 times
Marco63 2 years, 2 months ago
Selected Answer: D
connection_host = web is not supported attribute value, instead connection_host=dns (answer D) is correct.
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 1 times
Salman23 2 years, 9 months ago
D is correct.... Option C is incorrect because web is not valid for connection_host, 
Data admin page 142
upvoted 2 times
DeltaPotato 2 years, 10 months ago
D - page 142 in Data Admin pdf for options/examples.
upvoted 2 times
ckmunich 2 years, 11 months ago
C is right!
Port 9997 on TCP is in Splunk the standard port for communication between the forwarders and indexers
Port 10001 is a non standard configuration
upvoted 1 times
AngusBlack 3 years ago
D is correct. Although in theory you could use 9997 when I tried to configure it Splunk said it was not available.
upvoted 2 times
sargeholik 3 years, 4 months ago
C. PORT 9997 TCP Splunk port for communication between the forwarders and indexers
upvoted 4 times
hwangho 3 years, 6 months ago
Answer: D 
https://docs.splunk.com/Documentation/Splunk/8.1.1/Data/Monitornetworkports
upvoted 3 times
------------------------

64. Which of the following are available input methods when adding a file input in Splunk Web? (Choose all that apply.)
A. Index once.
B. Monitor interval.
C. On-demand monitor.
D. Continuously monitor.

jgab Highly Voted 3 years, 8 months ago
The correct answers are A & D
upvoted 31 times
ucsdmiami2020 2 years, 9 months ago
Agreed A and D. Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Data/Howdoyouwanttoadddata

The fastest way to add data to your Splunk Cloud instance or Splunk Enterprise deployment is to use Splunk Web. After you access the Add Data page, choose one of three options for getting data into your Splunk platform deployment with Splunk Web: (1) Upload, (2) Monitor, (3) Forward

The Upload option lets you upload a file or archive of files for indexing. When you choose Upload option, Splunk Web opens the upload process page.
Monitor. For Splunk Enterprise installations, the Monitor option lets you monitor one or more files, directories, network streams, scripts, Event Logs (on Windows hosts only), performance metrics, or any other type of machine data that the Splunk Enterprise instance has access to.
upvoted 2 times
toney_mu 1 year, 5 months ago
I don't see an option to turn on monitor once. Spluk continuasly monitor the file for updates.
upvoted 1 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: A
A and D
upvoted 1 times
anotherme1013 12 months ago
A & D would be the answers
upvoted 1 times
BozhidarM 1 year ago
AD is correct
upvoted 1 times
toney_mu 1 year, 5 months ago
Answer would be D
upvoted 1 times
toney_mu 1 year, 5 months ago
There is no option to monitor once, as splunk will continusly check for update so A is not valid
upvoted 1 times
erick165 1 year, 3 months ago
But there is an option to index once and that is what the option is so would be A&D
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A &D are correct
upvoted 2 times
mosematt 3 years, 1 month ago
ans is AD
upvoted 4 times
------------------------

56. In this sourcetype definition the MAX_TIMESTAMP_LOOKAHEAD is missing. Which value would fit best?
[sshd_syslog]
TIME_PREFIX = ^
TIME_FORMAT = %Y-%m-%d %H:%M:%S.%3N %z
LINE_BREAKER = ([\r\n]+)\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}

SHOULD_LINEMERGE = false -

TRUNCATE = 0 -
Event example:
2018-04-13 13:42:41.214 -0500 server sshd[26219]: Connection from 172.0.2.60 port 47366
A. MAX_TIMESTAMP_LOOKAHEAD = 5
B. MAX_TIMESTAMP_LOOKAHEAD = 10
C. MAX_TIMESTAMP_LOOKAHEAD = 20
D. MAX_TIMESTAMP_LOOKAHEAD = 30

AbuAli Highly Voted 4 years, 3 months ago
D. MAX_TIMESTAMP_LOOKAHEAD = 30 >>> is right

Please find below link
https://docs.splunk.com/Documentation/Splunk/6.2.0/Data/Configuretimestamprecognition
upvoted 28 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: D
2018-04-13 13:42:41.214 -0500 is much more than 10 characters long.
30 will catch it.
upvoted 2 times
Marco63 2 years, 2 months ago
Selected Answer: D
MAX_TIMESTAMP_LOOKAHEAD=10 is not enough to catch the whole timestamp
upvoted 3 times
royjn1981 2 years, 5 months ago
Selected Answer: D
https://docs.splunk.com/Documentation/Splunk/8.1.1/Data/Configuretimestamprecognition
"Specify how far (how many characters) into an event Splunk software should look for a timestamp."
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 4 times
leratel 3 years, 4 months ago
Is C a better choice ? Because date + time is 19 characters, 20 is ok or am I wrong ?
upvoted 3 times
leratel 3 years, 3 months ago
sorry for my question, I stupidly look at the format.... 
30 is good
upvoted 6 times
happy_and_lucky 3 years, 6 months ago
https://docs.splunk.com/Documentation/Splunk/8.1.1/Data/Configuretimestamprecognition
"Specify how far (how many characters) into an event Splunk software should look for a timestamp."

since TIME_PREFIX = ^ and timestamp is from 0-29 position, so D=30 will pick up the WHOLE timestamp correctly.
upvoted 3 times
------------------------

23. Which of the following statements describe deployment management? (Choose all that apply.)
A. Requires an Enterprise license.
B. Is responsible for sending apps to forwarders.
C. Once used, is the only way to manage forwarders.
D. Can automatically restart the host OS running the forwarder.

Praf7 Highly Voted 3 years, 8 months ago
Option A & B
upvoted 27 times
ucsdmiami2020 2 years, 9 months ago
Agreed A and B. Quoting two Splunk Reference URLs https://docs.splunk.com/Documentation/Splunk/8.2.2/Admin/Distdeploylicenses#:~:text=License%20requirements,do%20not%20index%20external%20data.

"All Splunk Enterprise instances functioning as management components needs access to an Enterprise license. Management components include the deployment server, the indexer cluster manager node, the search head cluster deployer, and the monitoring console."

https://docs.splunk.com/Documentation/Splunk/8.2.2/Updating/Aboutdeploymentserver

"The deployment server is the tool for distributing configurations, apps, and content updates to groups of Splunk Enterprise instances."
upvoted 6 times
tommot Highly Voted 3 years, 11 months ago
C is wrong. we can still use CLI and direct editing even after enabling DS.
upvoted 15 times
SasnycoN 2 years, 7 months ago
You can but they will be overwritten by the DS on the nest communication.
upvoted 4 times
EnidV 1 year, 9 months ago
The CLI could be used even on DS.
upvoted 2 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: B
A and B
upvoted 1 times
bobixaka 8 months, 2 weeks ago
Selected Answer: B
A and B are correct
upvoted 1 times
tmmt 1 year, 5 months ago
Selected Answer: B
Is A and B.
upvoted 1 times
mr56 1 year, 6 months ago
AB not C - For some complex configuration requirements, however, you might need to edit serverclass.conf directly. Important: If you switch from forwarder management to direct editing of serverclass.conf, you might not be able to use forwarder management for any subsequent configuration. This is because the forwarder management interface can handle only a subset of the configurations possible through serverclass.conf.
upvoted 1 times
PKV27 2 years, 2 months ago
A&B for sure, 
C,D - is discussable, by design not applicable
C - is it possible use sys/local/*.conf, DS not overriding this confs with apps
D - what about run ps script Restart-Computer on win OS? so it is possible to restart host OS
upvoted 2 times
BlueRoselia 2 years, 4 months ago
Data Admin pg 97 A&B 
Deployment Server is a built-in tool for managing configuration of Splunk instances
Allows you to manage remote Splunk instances centrallyRequires an Enterprise License
Handles the job of sending configurations (inputs.conf, outputs.conf, etc.) packaged as apps
Can automatically restart remote Splunk instances
Forwarder management is a graphical interface on top of deployment server
Monitoring Console Forwarder dashboards help you monitor the deployment
Best Practice: The Deployment Server should be a dedicated Splunk instanceIn this class, you will use your test server as a deployment server
upvoted 2 times
Alusine 2 years, 5 months ago
Its ABCD....page 68 of PDF. Can Automatically restart the remote splunk instances, manages forwarder configurations
upvoted 1 times
kurzer_ 2 years, 3 months ago
Exactly what you said, "It can restart remote SPLUNK INSTANCES" but not the Host OS.
D is wrong!
upvoted 5 times
Mntman77 1 year ago
exactly!
upvoted 1 times
Hurshbabe 10 months ago
Restarting as instance is equivalent to restarting the OS so D is right
upvoted 1 times
Hurshbabe 9 months, 3 weeks ago
never mind my answer, its wrong
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A and B are correct
upvoted 3 times
ivaanovich 2 years, 10 months ago
Tricky one, but I'd say that C is also correct: once an app is put under Deployment Management, the app's folder (the whole of it) will be overwritten each time the UF detects that there's a mismatch between it's own content and that from the DS. So yes: once used, Deployment Management is the only way to manage THOSE APPS in the forwarders. (apps that are not under Deployment Management can still be managed locally. And of course you can always disable Deployment Management on an app and go back to manual updates, if you so wish).
upvoted 2 times
toney_mu 1 year, 5 months ago
You cna disable the DS and still push apps or update apps
upvoted 2 times
ckmunich 2 years, 11 months ago
Only A and B are right
C is wrong. You can still use the CLI or edit the .conf files
D is wrong. No Splunk component can cause the underlying OS to reboot.
upvoted 6 times
SasnycoN 2 years, 7 months ago
About C - But even if you made any changes to the files via CLI they will be overwritten by the DS in the next communication.
upvoted 1 times
mjl79 3 years ago
A & B. C is wrong because you can still use the CLI or edit the .conf files and D is a sneaky answer designed to catch you out; No Splunk component can cause the underlying OS to reboot.
upvoted 1 times
SasnycoN 2 years, 7 months ago
What will happen if you use CLi to edit the .conf files and in the next communication DS detects that there are changes?!
upvoted 1 times
gsplunker 3 years, 5 months ago
Guess A,B and D
upvoted 4 times
lollo1234 3 years, 2 months ago
NO! You cant restart fw os
upvoted 1 times
IDM 3 years, 9 months ago
A and B 
C is a trick as, it can restart the forwarder on the client NOT the client/HOST OS.
upvoted 5 times
oksey 3 years, 10 months ago
I would go for ABD
upvoted 4 times
ames 3 years, 10 months ago
I would say A, B, D.
https://docs.splunk.com/Splexicon:Deploymentserver
upvoted 4 times
Load full discussion...
------------------------

33. User role inheritance allows what to be inherited from the parent role? (Choose all that apply.)
A. Parents
B. Capabilities
C. Index access
D. Search history

Shaq007 Highly Voted 3 years, 6 months ago
B and C are correct - checkout the "role inheritance" portion of Sys Admin PDF.
Inheritance:
- Can be based on one or more existing roles
- Provides inherited capabilities and index access
upvoted 14 times
jgab Highly Voted 3 years, 8 months ago
it could be B and C
upvoted 9 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: C
B and C
upvoted 1 times
Marco63 2 years, 2 months ago
B AND C are correct.
upvoted 2 times
king1993 2 years, 3 months ago
Answer: B and C
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: B
B & C are correct
upvoted 3 times
malice4 3 years, 2 months ago
B and C
upvoted 3 times
newrose 3 years, 7 months ago
I think the answer is B and C: https://docs.splunk.com/Documentation/Splunk/latest/Security/Aboutusersandroles#Role_inheritance
upvoted 4 times
------------------------

13. You update a props.conf file while Splunk is running. You do not restart Splunk and you run this command: splunk btool props list `"-debug. What will the output be?
A. A list of all the configurations on-disk that Splunk contains.
B. A verbose list of all configurations as they were when splunkd started.
C. A list of props.conf configurations as they are on-disk along with a file path from which the configuration is located.
D. A list of the current running props.conf configurations along with a file path from which the configuration was made.

giubal Highly Voted 4 years, 2 months ago
should be C
https://docs.splunk.com/Documentation/Splunk/8.0.1/Troubleshooting/Usebtooltotroubleshootconfigurations
upvoted 21 times
dpharker Highly Voted 3 years, 9 months ago
Answer is C.
See this phrase in the docs -> "The btool command simulates the merging process using the on-disk conf files and creates a report showing the merged settings."
https://docs.splunk.com/Documentation/Splunk/latest/Troubleshooting/Usebtooltotroubleshootconfigurations
upvoted 15 times
newrose 3 years, 7 months ago
Yes, and right after it says "The report does not necessarily represent what's loaded in memory. If a conf file change is made that requires a service restart, the btool report shows the change even though that change isn't active."
upvoted 8 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: C
it troubleshoots config on disk not in memory. Answer: C
upvoted 1 times
erick165 1 year, 7 months ago
C is the correct answer
The report does not necessarily represent what's loaded in memory. If a conf file change is made that requires a service restart, the btool report shows the change even though that change isn't active.
upvoted 2 times
RedYeti 2 years, 3 months ago
Selected Answer: C
Answer is C.
upvoted 5 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 3 times
malice4 3 years, 2 months ago
Correct answer is C because debug displays the exact .conf file location
upvoted 2 times
ArDeKu 3 years, 3 months ago
It should be C as btool is used for ondisk and debug is used for file path..
upvoted 1 times
Jackall 3 years, 4 months ago
btool key words: btool for on-disk ,and --debug for file location
upvoted 1 times
Tony_123 3 years, 5 months ago
C
splunk btool props list 
-- shows on-disk configuration for requested 
Use --debug to display the exact .conf file location
upvoted 1 times
Ashton_98 3 years, 8 months ago
btool will only show what is running / in memory, not what's on disk. D is correct.
upvoted 3 times
Ashton_98 3 years, 8 months ago
That's wrong actually, it's dependent on the conf and giubal and dphacker are correct.
upvoted 4 times
------------------------

11. This file has been manually created on a universal forwarder:
/opt/splunkforwarder/etc/apps/my_TA/local/inputs.conf
[monitor:///var/log/messages]
sourcetype=syslog
index=syslog
A new Splunk admin comes in and connects the universal forwarders to a deployment server and deploys the same app with a new inputs.conf file:
/opt/splunk/etc/deployment-apps/my_TA/local/inputs.conf
[monitor:///var/log/maillog]
sourcetype=maillog
index=syslog
Which file is now monitored?
A. /var/log/messages
B. /var/log/maillog
C. /var/log/maillog and /var/log/messages
D. none of the above

Stressplein Highly Voted 4 years, 1 month ago
https://answers.splunk.com/answers/728155/what-happens-if-you-deploy-an-inputsconf-from-a-ds.html
B
upvoted 17 times
Apis Highly Voted 2 years, 6 months ago
Selected Answer: B
B is correct. Apps from deployment server will overwrite any existing configuration
upvoted 5 times
bobixaka Most Recent 8 months, 2 weeks ago
Selected Answer: B
The client phones home to the DS, performs a checksum match on the apps and configs, finds a mismatch in that particular app and conf file, downloads the app from the DS and overwrites the mismatched inputs.conf
upvoted 4 times
InfoSec_RC53 1 year, 4 months ago
This is a great example of the poorly written questions in a Splunk exam. Notice the path, it is in the "deployment-apps" folder which means it is on the DS, not the forwarder. Once it gets to the forwarder, it will then overwrite the inputs, and be located in the $SPLUNK_HOME/etc/apps folder.
upvoted 1 times
gibla1929 2 years, 1 month ago
Selected Answer: B
deployment client will reinstall the app with the same name that matches its expected hash.
upvoted 1 times
ZeusP 3 years, 1 month ago
B is correct as soon as UF try to connect with DS it will pull updated conf and over write the existing conf.
upvoted 4 times
Tony_123 3 years, 5 months ago
Once UF (DS client) connects DS server, it will pull the /opt/splunk/etc/deployment-apps/my_TA/local/inputs.conf from DS server , so B is the correct answer.
upvoted 5 times
pucca012 3 years, 5 months ago
A is the correct answer, because the local always take precedence.
upvoted 1 times
Hamiltonian 3 years ago
This question has nothing to do with precedence. In the first case, the inputs.conf is written locally on the forwarder. In the second case, this original inputs.conf is overwritten by the new inputs.conf settings because the configurations been redeployed from a DS.
upvoted 4 times
Hamiltonian 3 years ago
Better to say "deployed" rather than redeployed, because it's the first time a DS is being used with the forwarder.
upvoted 3 times
sargeholik 3 years, 6 months ago
b correct answer
upvoted 4 times
Sandy_1988 3 years, 7 months ago
B is the correct answer
upvoted 5 times
sergito095 4 years ago
I think that the C is the correct answer, because inputs.conf file from forwarder is set up to monitor "messages" file and "maillog" file is monitored by Depolyment Server. Files are differents.
upvoted 3 times
Ashton_98 3 years, 8 months ago
That would be true if they didn't have the same app name. When you deploy an app with the same name, it will overwrite the inputs.conf file instead of merging.
upvoted 4 times
Hamiltonian 3 years ago
It doesn't matter. The DS is deploying the configuration setting sunder the given app name. The forwarder, once cnnected to the DS, will do whatever the DS tells it to do from the app configuration settings.
upvoted 2 times
mker 4 years, 1 month ago
A is the correct answer, becouse the file inputs.conf will by overwrite by deployment
upvoted 2 times
mker 4 years, 1 month ago
sorry B is the correct
upvoted 7 times
------------------------

81. Where are deployment server apps mapped to clients?
A. Apps tab in forwarder management interface or clientapps.conf.
B. Clients tab in forwarder management interface or deploymentclient.conf.
C. Server Classes tab in forwarder management interface or serverclass.conf.
D. Client Applications tab in forwarder management interface or clientapps.conf.

hwangho Highly Voted 3 years, 6 months ago
C is correct.
upvoted 13 times
ucsdmiami2020 2 years, 10 months ago
Per the Splunk docs URL https://docs.splunk.com/Documentation/Splunk/8.0.5/Updating/Useserverclass.conf
"Use serverclass.conf to define server classes" 
"The most important settings define the set of deployment clients and the set of apps for each server class."
upvoted 3 times
adamsca Most Recent 1 year ago
Selected Answer: C
C is correct
upvoted 1 times
mngesha 1 year, 5 months ago
I believe the answer is C from the following doucmentation
https://docs.splunk.com/Documentation/Splunk/8.1.0/Updating/Createdeploymentapps & 
https://docs.splunk.com/Documentation/Splunk/8.0.5/Updating/Useserverclass.conf#
upvoted 1 times
denominator 2 years ago
Key word here is mapped, so apps that are chosen for the particular server
upvoted 1 times
newrose 3 years, 7 months ago
what about D?
upvoted 1 times
leratel 3 years, 3 months ago
clientapp.conf does not exist
upvoted 2 times
------------------------

75. When configuring HTTP Event Collector (HEC) input, how would one ensure the events have been indexed?
A. Enable indexer acknowledgment.
B. Enable forwarder acknowledgment.
C. splunk check-integrity -index <index name>
D. index=_internal component=ACK | stats count by host

shivi Highly Voted 3 years, 4 months ago
A. Enable indexer acknowledgment.
upvoted 12 times
ucsdmiami2020 2 years, 10 months ago
Per the provided Splunk reference URLhttps://docs.splunk.com/Documentation/Splunk/8.0.5/Data/AboutHECIDXAck
"While HEC has precautions in place to prevent data loss, it's impossible to completely prevent such an occurrence, especially in the event of a network failure or hardware crash. This is where indexer acknolwedgment comes in."
upvoted 2 times
adamsca Most Recent 1 year ago
Selected Answer: A
A is correct
upvoted 1 times
mngesha 1 year, 5 months ago
A would be the most appropriate answer in this case. C looks like just a query. The document in the link should clarify.
upvoted 2 times
rockhorse 2 years, 1 month ago
D for sure
upvoted 1 times
------------------------

77. Which of the following is a benefit of distributed search?
A. Peers run search in sequence.
B. Peers run search in parallel.
C. Resilience from indexer failure.
D. Resilience from search head failure.

jgab Highly Voted 3 years, 8 months ago
Correct answer is B
upvoted 14 times
ucsdmiami2020 2 years, 9 months ago
Using Splunk docs URL reference https://docs.splunk.com/Documentation/Splunk/8.2.2/DistSearch/Whatisdistributedsearch
Parallel reduce search processing
If you struggle with extremely large high-cardinality searches, you might be able to apply parallel reduce processing to them to help them complete faster. You must have a distributed search environment to use parallel reduce search processing.
upvoted 1 times
adamsca Most Recent 1 year ago
Selected Answer: B
B is correct
upvoted 1 times
kolaturka 1 year, 3 months ago
B. Peers run search in parallel.

Distributed search allows a search to be split across multiple indexers and searched in parallel, significantly reducing search time. Additionally, distributed search provides resilience from search head failure, as the search can be restarted from another search head in the cluster.
upvoted 1 times
toney_mu 1 year, 5 months ago
Option B is correct, if there were option to select more than 1 then B and C
upvoted 1 times
mngesha 1 year, 5 months ago
in my humble opinion B & C would be the right answers yet if only one answer is deemed correct I would prioritize B.
upvoted 2 times
splunkuser03 1 year, 6 months ago
B & D
upvoted 1 times
Mando22 1 year, 9 months ago
Correct Answer: B
upvoted 1 times
emlch 1 year, 10 months ago
B and C may be correct.
upvoted 3 times
emlch 1 year, 10 months ago
But C isn't since doesnt provide indexer resilience
upvoted 1 times
toney_mu 1 year, 5 months ago
When indexer goes down remaining indexers handle all indexing
upvoted 1 times
splunkkid 2 years ago
Selected Answer: B
B - Distributed search reduce search processing by running in parallel to indexers
upvoted 1 times
denominator 2 years ago
Selected Answer: B
i agree with B
upvoted 1 times
denominator 2 years ago
Who cares if a search head goes down, only negative is it takes longer to complete your search.
upvoted 1 times
tomod1 2 years, 2 months ago
Selected Answer: D
Distributed search provides horizontal scaling, so that a single Splunk Enterprise deployment can search and index arbitrarily large amounts of data. Distributed search is also useful for correlating data across data silos.

https://docs.splunk.com/Splexicon:Distributedsearch
upvoted 1 times
BlueRoselia 2 years, 4 months ago
Answer B system admin "distributed Search"
Users log on to the search head and run reportsThe search head dispatches searches to the peersPeers run searches in parallel and return their portion of resultsThe search head consolidates the individual results and prepares reports
upvoted 1 times
Salman23 2 years, 9 months ago
B is correct. as per document Sys Admin documentation page 190. C and D are incorrect because the question does not mention about clusters. A is not correct, I've never heard about search in sequence on peers.
upvoted 2 times
Hudda 3 years ago
Friends, could you please confirm this answer?
upvoted 1 times
PaulT 3 years, 1 month ago
I think B and C are correct.
According to SysAdmin pdf in Module 10: Distributed Search "when an indexer goes down:
 The offline indexer does not participate in searches;
 The remaining indexers handle all indexing and searches"
=> the very definition of "C. Resilience from indexer failure."
upvoted 3 times
lilsem 2 years, 10 months ago
But other indexers won't have the data, that would be otherwise in the indexer that went down. Indeed, in the indexer, even if one indexer would go down, other indexers (if it was properly configurated) would have the same copies of data.
upvoted 1 times
lilsem 2 years, 10 months ago
in the indexer cluster*
upvoted 1 times
gsplunker 3 years, 5 months ago
I would go with B and D
upvoted 1 times
gsplunker 3 years, 5 months ago
Sorry B
upvoted 1 times
TeeCeeP 3 years, 6 months ago
think its c
upvoted 1 times
mybox1 3 years, 6 months ago
B is correct, C would be correct if question was about indexers cluster. In this case is just about standalone indexers.
upvoted 2 times
PaulT 3 years, 1 month ago
If B is correct, how do "standalone indexers" "run searches in parallel"?
upvoted 2 times
AngusBlack 3 years ago
Because when you search the data you are searching for could be on one or more indexers. So if half your forwarders send to indexer A and half to indexer B, when you run a search across a sourcetype it would run in parallel across multiple indexers
upvoted 2 times
------------------------

80. Social Security Numbers (PII) data is found in log events, which is against company policy. SSN format is as follows: 123-44-5678.
Which configuration file and stanza pair will mask possible SSNs in the log events?
A. props.conf [mask-SSN] REX = (?ms)^(.)\<[SSN>\d{3}-?\d{2}-?(\d{4}.*)$" FORMAT = $1<SSN>###-##-$2 KEY = _raw
B. props.conf [mask-SSN] REGEX = (?ms)^(.)\<[SSN>\d{3}-?\d{2}-?(\d{4}.*)$" FORMAT = $1<SSN>###-##-$2 DEST_KEY = _raw
C. transforms.conf [mask-SSN] REX = (?ms)^(.)\<[SSN>\d{3}-?\d{2}-?(\d{4}.*)$" FORMAT = $1<SSN>###-##-$2 DEST_KEY = _raw
D. transforms.conf [mask-SSN] REGEX = (?ms)^(.)\<[SSN>\d{3}-?\d{2}-?(\d{4}.*)$" FORMAT = $1<SSN>###-##-$2 DEST_KEY = _raw

ugo1 Highly Voted 3 years, 8 months ago
The Ans is D 
because transforms.conf is the right configuration file to state the regex expression.
https://docs.splunk.com/Documentation/Splunk/8.1.0/Admin/Transformsconf
upvoted 20 times
Racgud 3 years, 7 months ago
Wrong - B is correct
"Props.conf is commonly used for:
Anonymizing certain types of sensitive incoming data, such as credit
 card or social security numbers, using sed scripts."
src: https://docs.splunk.com/Documentation/Splunk/8.1.0/Admin/Propsconf
upvoted 2 times
Racgud 3 years, 7 months ago
looks like both are correct, the same sentence can be found in:
https://docs.splunk.com/Documentation/Splunk/8.1.0/Admin/Transformsconf
upvoted 1 times
Splunkv 3 years, 3 months ago
Correct Answer is D. DEST_KEY is not a attribute in props.conf spec
upvoted 4 times
lollo1234 3 years, 4 months ago
No, props.conf doesn't have a REGEX config-parameter, transforms.conf does.
upvoted 2 times
Hamiltonian 3 years ago
Hense the phrase "sed scripts". SEDCMDs are not being used in any of the answers. Thus, the answer is dealing with TRANSFORMS. TRANSFORMS are invoked by the props.conf file, but the transformations, i.e., REGEX, FORMAT, etc are specified in the transforms.conf.
upvoted 3 times
TeeCeeP Highly Voted 3 years, 6 months ago
D slide 242 in data admin
upvoted 8 times
Mntman77 Most Recent 1 year ago
"D" is correct... see ref: https://bluefletch.com/the-magic-of-splunk-how-to-throw-away-data/#:~:text=DEST_KEY%20%3D%20queue%20FORMAT%20%3D%20nullQueue%20Notice%20the,it%20to%20the%20indexer%29%20or%20%E2%80%98nullQueue%E2%80%99%20%28ignore%20it%29.
upvoted 1 times
kolaturka 1 year, 3 months ago
The correct answer is D.

The transforms.conf file is used to define field extractions and data masking rules. In this case, we want to mask Social Security Numbers (SSN) found in log events. We can achieve this using a regular expression and the FORMAT parameter to replace the SSN with a masked value. The DEST_KEY parameter is used to specify the destination field for the masked data.

Here is an explanation of the stanza:
[mask-SSN]
REGEX = (?ms)^(.)\<[SSN>\d{3}-?\d{2}-?(\d{4}.*)$"
FORMAT = $1<SSN>###-##-$2
DEST_KEY = _raw
upvoted 1 times
Mando22 1 year, 9 months ago
Correct Answer: D
upvoted 2 times
emlch 1 year, 10 months ago
props.conf invokes transforms.conf
upvoted 1 times
alejohu 1 year, 11 months ago
Selected Answer: D
Ans is D
upvoted 1 times
splunkkid 2 years ago
Selected Answer: D
Ans: D
A & B is definitely incorrect as format & dest_key are not attributes under props.conf, C is incorrect as the attribute should be REGEX instead of REX.
upvoted 1 times
lilsem 2 years, 10 months ago
The confusion comes from the fact, that without props.conf, transform.conf won't work, as you have to reference the "options" from the latter in the props.conf file; but the configurations we see in the answers, are proper to transforms.conf file, and REGEX attribute is to be used in this file, not REX, so that leads us to the D answer.
upvoted 3 times
ckmunich 2 years, 11 months ago
Answer is D: 

Configure the transforms.conf file

The Splunk platform uses the transforms.conf file to perform the transformation of the data
upvoted 2 times
Hudda 3 years ago
Friends, many views on this question, could you please confirm the final answer?
upvoted 1 times
AngusBlack 3 years ago
D is the least wrong answer. All of the REGEXs are wrong.
upvoted 1 times
gsplunker 3 years, 5 months ago
Ans is D
upvoted 1 times
hwangho 3 years, 6 months ago
Answer is D.
Please check "Using Transforms" from Data Administration training Doc.
-define the transformation in transforms.conf
-invoke the transformation from props.conf
upvoted 4 times
afroben 3 years, 7 months ago
Props.conf specifies the transforms to use to anonymize your data. Correct answer is B.
upvoted 1 times
MariuszSem 2 years, 11 months ago
The correct answer is D
upvoted 3 times
------------------------

121. When Splunk is integrated with LDAP, which attribute can be changed in the Splunk UI for an LDAP user?
A. Default app
B. LDAP group
C. Password
D. Username

Hemnaath Highly Voted 1 year, 6 months ago
Splunk system admin slides page 152 CLEARLY shows that only time zone and default app can be changed on LDAP and other non-splunk native users.
upvoted 6 times
adamsca Most Recent 1 year ago
Selected Answer: A
Yes I agree A
upvoted 1 times
toney_mu 1 year, 5 months ago
As per ssysadmin document 
====
Only time zone and default app can be changed on LDAP or other
users
====

Option A
upvoted 2 times
harrytbb 1 year, 5 months ago
Selected Answer: A
A is confirmed correct.
upvoted 2 times
AS1515 1 year, 6 months ago
Agreed. A is correct.
upvoted 2 times
Nitsua 1 year, 7 months ago
Likely the LDAP group for assignment of user roles
 so B
upvoted 1 times
cb42 1 year, 7 months ago
LDAP group is managed on the ldap-Server. Therefore A should be correct.
upvoted 2 times
------------------------

124. All search-time field extractions should be specified on which Splunk component?
A. Deployment server
B. Universal forwarder
C. Indexer
D. Search head

shergar Highly Voted 1 year, 7 months ago
Selected Answer: D
Search Time field extractions are stored on the search head
upvoted 6 times
adamsca Most Recent 1 year ago
Selected Answer: D
Yes I would go with D
upvoted 1 times
toney_mu 1 year, 5 months ago
I would go for option D
upvoted 1 times
anonyuser 1 year, 7 months ago
Selected Answer: D
I was thinking D as well
upvoted 1 times
------------------------

131. A new forwarder has been installed with a manually created deploymentclient.conf.

What is the next step to enable the communication between the forwarder and the deployment server?
A. Restart Splunk on the deployment server.
B. Enable the deployment client in Splunk Web under Forwarder Management.
C. Restart Splunk on the deployment client.
D. Wait for up to the time set in the phoneHomeIntervalInSecs setting.

adamsca 1 year ago
Selected Answer: C
C vote
upvoted 1 times
adamsca 1 year ago
Agree Answer is C
upvoted 1 times
toney_mu 1 year, 5 months ago
Answer is C
upvoted 1 times
Rodders2828 1 year, 7 months ago
Selected Answer: C
C is correct.
upvoted 1 times
anonyuser 1 year, 7 months ago
Selected Answer: C
Yeah, this should be C I think. Assuming you just modified the deploymentclient.conf on the forwarder, you would restart the service on the forwarder, not the DS, to load the config into the service.
upvoted 1 times
cb42 1 year, 7 months ago
Selected Answer: C
if "installed" means, that the splunk software was installed and the DS is configured already by deploymentclient.conf the next step would be to restart splunk on the forwarder.
upvoted 2 times
------------------------

134. An add-on has configured field aliases for source IP address and destination IP address fields. A specific user prefers not to have those fields present in their user context. Based on the default props.conf below, which SPLUNK_HOME/etc/users/buttercup/myTA/local/props.conf stanza can be added to the user’s local context to disable the field aliases?
A.
B.
C.
D.

Mntman77 1 year ago
B is correct

Open the outputs.conf file in your local directory.
Find the forwardedindex.0.whitelist setting and change the value to null. For example:

forwardedindex.0.whitelist =
upvoted 1 times
harrytbb 1 year, 5 months ago
Selected Answer: B
I would go for B. 
"You clear a setting by changing its value to null."
Reference: https://docs.splunk.com/Documentation/Splunk/latest/Admin/Howtoeditaconfigurationfile#Clear%20a%20setting
upvoted 3 times
beastmode1 1 year, 5 months ago
B place no content to remove from output
upvoted 3 times
random0352 1 year, 5 months ago
Selected Answer: D
Answer D looks correct. The field alias has been commented out using '#'
upvoted 2 times
pieter543 1 year, 5 months ago
Why not D?
upvoted 1 times
------------------------

57. Which of the following are required when defining an index in indexes.conf? (Choose all that apply.)
A. coldPath
B. homePath
C. frozenPath
D. thawedPath

Mntman77 1 year ago
In the GUI these are all listed as optional, but in the documentation for indexes.conf they are required (except frozen)
upvoted 1 times
BlueRoselia 2 years, 4 months ago
D is wrong frozen bucket are not required thus thawedPath is also not required
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: ABD
A, B & D are correct
upvoted 4 times
appopay 1 year, 6 months ago
confirmed here: https://docs.splunk.com/Documentation/Splunk/9.0.3/Admin/Indexesconf
upvoted 1 times
islamjy 2 years, 10 months ago
thawed path is optional not required from sys admin page 103
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Answers are A, B, and D. Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/6.5.0/Admin/Indexesconf

homePath = <path on index server>
* An absolute path that contains the hotdb and warmdb for the index.

coldPath = <path on index server>
* An absolute path that contains the colddbs for the index.

thawedPath = <path on index server>
* An absolute path that contains the thawed (resurrected) databases for the index.
upvoted 1 times
AxlF 2 years, 9 months ago
Thawed path is REQUIRED.
upvoted 2 times
lilsem 2 years, 10 months ago
Check splunk docs on indexes.conf:

thawedPath = <string>
* An absolute path that contains the thawed (resurrected) databases for the
 index.
* CANNOT contain a volume reference.
* Path must be writable.
* Required. Splunkd does not start if an index lacks a valid thawedPath. <-----
upvoted 3 times
DeltaPotato 2 years, 10 months ago
ABD - System Admin PDF, page 125. Paths must be specified, even when using the defaults.
upvoted 2 times
hwangho 3 years, 6 months ago
Answer: ABD
https://docs.splunk.com/Documentation/Splunk/8.1.1/Admin/Indexesconf

thawedPath = <string>
* Required. Splunkd does not start if an index lacks a valid thawedPath.
upvoted 3 times
newrose 3 years, 7 months ago
A B D looks correct to me
upvoted 3 times
------------------------

87. In a distributed environment, which Splunk component is used to distribute apps and configurations to the other Splunk instances?
A. Indexer
B. Deployer
C. Forwarder
D. Deployment server

Shaq007 Highly Voted 3 years, 6 months ago
D. Deployment server
https://docs.splunk.com/Documentation/Splunk/8.0.5/Updating/Updateconfigurations
First line says it all: "The deployment server distributes deployment apps to clients."
upvoted 6 times
kirtak Most Recent 1 year, 2 months ago
Selected Answer: D
Distributed, not clustered, so D for forwarder management
upvoted 1 times
mngesha 1 year, 5 months ago
Deployment server. Answer is D.
upvoted 1 times
appopay 1 year, 6 months ago
Since the question is specifically about a distributed environment, I'd say the awaited response is: Deployer
upvoted 1 times
ComeUp 2 years, 5 months ago
The correct answer is B.
https://docs.splunk.com/Documentation/Splunk/8.2.4/DistSearch/PropagateSHCconfigurationchanges
upvoted 2 times
not_another_user_007 2 years, 10 months ago
The answer is D as it is asking about deploying apps to other Splunk instances (assuming it is UFs/HFs)

If the question was asking what component sends data to the Search Head Cluster, then it would be a deployer. As admins (hence this exam) does not look at SHC, the answer would be D
upvoted 3 times
pucca012 3 years, 5 months ago
A deployer is used to deploy apps to a search head cluster.
A cluster master is used to deploy apps and manage replication within an indexer cluster (single or multi-site)
A deployment server is used to deploy apps to forwarders (and technically could be used to deploy apps to other Splunk servers as well but with a number of caveats)
upvoted 1 times
Splunkv 3 years, 3 months ago
so answer is B & D, correct?
upvoted 2 times
Splunkv 3 years, 3 months ago
The deployer is a Splunk Enterprise instance that you use to distribute apps and certain other configuration updates to search head cluster members. The set of updates that the deployer distributes is called the configuration bundle.

https://docs.splunk.com/Documentation/Splunk/8.1.3/DistSearch/PropagateSHCconfigurationchanges#:~:text=The%20deployer%20is%20a%20Splunk,is%20called%20the%20configuration%20bundle.
upvoted 2 times
BlueRoselia 2 years, 4 months ago
Yes Answer D & B
If I have to choose one, I will choose D because the system admin and data admin talks mainly about the deployment server. diagrams with the deployer are shown but not discussed
upvoted 2 times
anonyuser 1 year, 7 months ago
It is not talking about a cluster here
upvoted 1 times
mybox1 3 years, 6 months ago
D is correct
upvoted 3 times
newrose 3 years, 7 months ago
I dont think that answer is correct
upvoted 1 times
------------------------

28. How does the Monitoring Console monitor forwarders?
A. By pulling internal logs from forwarders.
B. By using the forwarder monitoring add-on.
C. With internal logs forwarded by forwarders.
D. With internal logs forwarded by deployment server.

Josi12 Highly Voted 4 years, 1 month ago
Best answer is C. MC, which is search head of search heads relies on internal logs forwarded by forwarders.
upvoted 16 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Quoting the following Splunk URL reference https://docs.splunk.com/Documentation/Splunk/8.2.2/DMC/DMCprerequisites

"Monitoring Console setup prerequisites. Forward internal logs (both $SPLUNK_HOME/car/log/splunk and $SPLUNK_HOME/var/log/introspection) to indexers from all other components. Without this step, many dashboards will lack data."
upvoted 2 times
Asami Highly Voted 4 years ago
C. With internal logs forwarded by forwarders.
upvoted 6 times
uptightuptight Most Recent 1 year, 2 months ago
Selected Answer: C
Uses internal logs from forwarders
upvoted 2 times
Mando22 1 year, 9 months ago
The correct answer is C
upvoted 1 times
thissiteisgreat 2 years, 2 months ago
Selected Answer: C
C......
upvoted 2 times
Marco63 2 years, 2 months ago
Selected Answer: C
uses internal logs from forwarders
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 1 times
loky0 2 years, 10 months ago
C. P109 Data admin pdf
upvoted 2 times
Mimi88 3 years, 9 months ago
Ans. C. See Forwarder Monitoring with Monitoring Console section of Data Administration PDF
upvoted 5 times
Sammy33 4 years, 1 month ago
Correct answer is C, based on the Data Admin Power Point from Splunk Training
upvoted 5 times
giubal 4 years, 2 months ago
I'm not really sure but the answer could be C
"The Monitoring Console dashboards use data from Splunk Enterprise's internal log files"
https://docs.splunk.com/Documentation/Splunk/latest/DMC/DMCoverview
upvoted 5 times
------------------------

46. Which valid bucket types are searchable? (Choose all that apply.)
A. Hot buckets
B. Cold buckets
C. Warm buckets
D. Frozen buckets

newrose Highly Voted 3 years, 7 months ago
A B C indeed
upvoted 8 times
ucsdmiami2020 2 years, 9 months ago
Agreed A, B, C. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Indexer/Bucketsandclusters?

"A hot bucket is a bucket that's still being written to. When an indexer finishes writing to a hot bucket (for example, because the bucket reaches a maximum size), it rolls the bucket to warm and begins writing to a new hot bucket. Warm buckets are readable (for example, for searching) but the indexer does not write new data to them. Eventually, a bucket rolls to cold and then to frozen, at which point it gets archived or deleted.
Searches occur across hot, warm, and cold buckets.
upvoted 2 times
adamsca Most Recent 1 year, 3 months ago
Selected Answer: ABC
I agree,ABC
upvoted 2 times
kolaturka 1 year, 3 months ago
Hot, cold, and warm buckets are all searchable in Splunk, whereas frozen buckets are not searchable.

Hot buckets contain recently indexed data and are actively being written to by the indexer. Warm buckets contain data that has been rolled from hot buckets and is no longer being actively written to, but is still available for search. Cold buckets contain data that has been rolled from warm buckets and is not currently in use, but is still available for search. Frozen buckets contain data that has been rolled from cold buckets and is no longer searchable, but is retained for long-term storage or compliance purposes.

In general, only hot and warm buckets are actively queried during typical Splunk searches. Cold and frozen buckets are used for long-term storage and are generally accessed less frequently.
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: ABC
A, B & C are correct
upvoted 1 times
mikey_76 2 years, 10 months ago
The frozen bucket cannot be searched it needs to be thawed which places it back into cold. So it's A, B and C
upvoted 3 times
------------------------

70. How can native authentication be disabled in Splunk?
A. Remove the $SPLUNK_HOME/etc/passwd file
B. Create an empty $SPLUNK_HOME/etc/passwd file
C. Set SPLUNK_AUTHENTICATION=false in splunk-launch.conf
D. Set nativeAuthentication=false in authentication.conf

roblaw Highly Voted 3 years, 8 months ago
B. A blank passwd file disables native authentication.
upvoted 15 times
kolaturka Most Recent 1 year, 3 months ago
reating an empty passwd file can disable native authentication in Splunk. This can be achieved by creating an empty file named passwd in the $SPLUNK_HOME/etc directory. This method is useful if you want to use an external authentication provider such as LDAP or SAML for user authentication.

Option D (nativeAuthentication=false in authentication.conf) can also be used to disable native authentication, but it is a more granular option as it only disables certain parts of the native authentication system.
upvoted 1 times
toney_mu 1 year, 5 months ago
Option B
https://docs.splunk.com/Documentation/Splunk/9.0.3/Security/Usernameprecedence#:~:text=On%20the%20Splunk%20Enterprise%20instance,Restart%20Splunk%20Enterprise.
upvoted 2 times
Lewist 2 years, 5 months ago
Selected Answer: B
Answer is B
upvoted 2 times
loky0 2 years, 10 months ago
B. P151 sys admin pdf
upvoted 3 times
Sandy_1988 3 years, 7 months ago
B is the answer. Refer system admin pdf.
upvoted 4 times
newrose 3 years, 7 months ago
B. Create an empty $SPLUNK_HOME/etc/passwd file
upvoted 3 times
------------------------

69. If an update is made to an attribute in inputs.conf on a universal forwarder, on which Splunk component would the fishbucket need to be reset in order to reindex the data?
A. Indexer
B. Forwarder
C. Search head
D. Deployment server

newrose Highly Voted 3 years, 7 months ago
Isn't it B? The files checkpoints reside in the UF's fishbucket index, right? So we should reset in the UF
upvoted 15 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk Reference URL https://www.splunk.com/en_us/blog/tips-and-tricks/what-is-this-fishbucket-thing.html

"Every Splunk instance has a fishbucket index, except the lightest of hand-tuned lightweight forwarders, and if you index a lot of files it can get quite large. As any other index, you can change the retention policy to control the size via indexes.conf"
upvoted 1 times
hwangho Highly Voted 3 years, 6 months ago
Answer: B
-change the inputs.conf on the deployment server (or forwarders)
-reset the fishbucket checkpoint on the involved forwarders
upvoted 8 times
kolaturka Most Recent 1 year, 3 months ago
Option A is incorrect because resetting the fishbucket on the indexer would not have any effect on the universal forwarder.

Option C is incorrect because resetting the fishbucket on the search head is not necessary in this scenario.

Option D is incorrect because the deployment server is used to manage and distribute configurations to forwarders, but resetting the fishbucket would need to be done on the forwarder itself.
upvoted 1 times
Fe01 2 years, 5 months ago
Selected Answer: B
https://www.splunk.com/en_us/blog/tips-and-tricks/what-is-this-fishbucket-thing.html
Answer is B
upvoted 2 times
IndicatorDeStafeta 2 years, 7 months ago
B is the correct answer
upvoted 1 times
Powdered_Sugar 2 years, 8 months ago
Answer: B
Data Admin Slide 132: To re-index step 3: Reset the fishbucket checkpoint on the involved forwarders.
upvoted 3 times
Salman23 2 years, 9 months ago
A is Correct, universal forwarders don't index data. Indexing always on indexers.
upvoted 2 times
Lerd15 3 years, 6 months ago
The correct ANS is B.
upvoted 6 times
------------------------

67. When are knowledge bundles distributed to search peers?
A. After a user logs in.
B. When Splunk is restarted.
C. When adding a new search peer.
D. When a distributed search is initiated.

hwangho Highly Voted 3 years, 6 months ago
D is correct
https://docs.splunk.com/Documentation/Splunk/8.0.5/DistSearch/Whatsearchheadssend
upvoted 11 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the Splunk reference URL...

"The search head replicates the knowledge bundle periodically in the background or when initiating a search. "
"As part of the distributed search process, the search head replicates and distributes its knowledge objects to its search peers, or indexers. Knowledge objects include saved searches, event types, and other entities used in searching accorss indexes. The search head needs to distribute this material to its search peers so that they can properly execute queries on its behalf."
upvoted 2 times
kolaturka Most Recent 1 year, 3 months ago
The correct answer is D. When a distributed search is initiated.

Knowledge bundles are collections of configuration files, saved searches, and other knowledge objects that are used to share knowledge across the distributed environment in Splunk. When a distributed search is initiated, the search head distributes the relevant knowledge bundle to the search peers that are participating in the search.

Option A is incorrect because knowledge bundles are not distributed to search peers after a user logs in.
Option B is incorrect because restarting Splunk does not trigger the distribution of knowledge bundles to search peers.
Option C is also incorrect because knowledge bundles are not distributed to search peers when adding a new search peer. Instead, when a new search peer is added to a search head cluster or a distributed search environment, the knowledge bundle is automatically distributed to the new search peer.
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 1 times
Bianchi 2 years, 11 months ago
D is correct. Pag 193 Sys Adm PDF
upvoted 4 times
------------------------

59. What hardware attribute would you need to be changed to increase the number of simultaneous searches (ad-hoc and scheduled) on a single search head?
A. Disk
B. CPUs
C. Memory
D. Network interface cards

mker Highly Voted 4 years, 1 month ago
B is the correct
upvoted 10 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Splunk URL reference https://docs.splunk.com/Documentation/Splunk/7.3.1/DistSearch/SHCarchitecture

Scroll down to section titled, How the cluster handles concurrent search quotas, "Overall search quota. This quota determines the maximum number of historical searches (combined scheduled and ad hoc) that the cluster can run concurrently. This quota is configured with max_Searches_per_cpu and related settings in limits.conf."
upvoted 1 times
kolaturka Most Recent 1 year, 3 months ago
B. CPUs

To increase the number of simultaneous searches (ad-hoc and scheduled) on a single search head, you would need to increase the number of CPUs. This is because searches are a CPU-intensive operation, and adding more CPUs would allow the search head to handle more search requests at the same time. While increasing the disk, memory, or network interface cards could improve other aspects of search performance, they would not directly increase the number of searches that can be run simultaneously.
upvoted 1 times
Apis 2 years, 6 months ago
B is correct
upvoted 1 times
hwangho 3 years, 6 months ago
Answer: B
https://docs.splunk.com/Documentation/Splunk/8.1.1/Capacity/Accommodatemanysimultaneoussearches
upvoted 3 times
------------------------

58. Which of the following apply to how distributed search works? (Choose all that apply.)
A. The search head dispatches searches to the peers.
B. The search peers pull the data from the forwarders.
C. Peers run searches in parallel and return their portion of results.
D. The search head consolidates the individual results and prepares reports.

giubal Highly Voted 4 years, 2 months ago
The correct answer is A, C and D 

as reported from system administrato pdf

Users log on to the search head and run reports:
 The search head dispatches searches to the peers
 Peers run searches in parallel and return their portion of results
 The search head consolidates the individual results and prepares reports
upvoted 37 times
Amith Highly Voted 4 years, 2 months ago
A,C and D
upvoted 11 times
kolaturka Most Recent 1 year, 3 months ago
A. The search head dispatches searches to the peers.
C. Peers run searches in parallel and return their portion of results.
D. The search head consolidates the individual results and prepares reports.

In a distributed search architecture, the search head is responsible for dispatching searches to the search peers. The peers then run the searches in parallel and return their portion of the results to the search head. The search head then consolidates the individual results and prepares reports based on the search criteria. The search peers do not pull data from forwarders; they only process search requests from the search head.
upvoted 1 times
ANALYSTBK 1 year, 10 months ago
Selected Answer: A
A, C & D are correct, as reported from system administrator pdf
upvoted 4 times
Marco63 2 years, 2 months ago
Of course is A,C,D!
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: A
A, C & D are correct
upvoted 5 times
Sandy_1988 3 years, 7 months ago
A,C and D are the options correct
upvoted 4 times
oksey 3 years, 10 months ago
yea, ACD it is
upvoted 6 times
------------------------

55. When deploying apps, which attribute in the forwarder management interface determines the apps that clients install?
A. App Class
B. Client Class
C. Server Class
D. Forwarder Class

ames Highly Voted 3 years, 10 months ago
True, C. 
<https://docs.splunk.com/Documentation/Splunk/8.0.6/Updating/Deploymentserverarchitecture>
<https://docs.splunk.com/Splexicon:Serverclass>
upvoted 9 times
tmmt Most Recent 1 year, 5 months ago
Selected Answer: C
C, server class > host list > app list
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 1 times
------------------------

52. Where are license files stored?
A. $SPLUNK_HOME/etc/secure
B. $SPLUNK_HOME/etc/system
C. $SPLUNK_HOME/etc/licenses
D. $SPLUNK_HOME/etc/apps/licenses

amporiik Highly Voted 3 years, 11 months ago
C. $SPLUNK_HOME/etc/licenses
upvoted 10 times
tmmt Most Recent 1 year, 5 months ago
Is C, /etc/licenses, because you can have multiple licenses in stack.
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
------------------------

41. Which of the following are methods for adding inputs in Splunk? (Choose all that apply.)
A. CLI
B. Splunk Web
C. Editing inpits.conf
D. Editing monitor.conf

tmmt 1 year, 5 months ago
ABC if C have was inputs.conf
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: AB
A & B for sure
C assuming it is a typo
upvoted 4 times
sargeholik 3 years, 6 months ago
A and B
upvoted 4 times
Ashton_98 3 years, 8 months ago
AB and C.
upvoted 1 times
Toanbego 3 years, 8 months ago
Really depends if C is mispelled or not. If it is supposed to be inputs.conf, then i agree. Else i would stay away :P
upvoted 12 times
Ashton_98 3 years, 7 months ago
Good spotting! Attention to detail is the real challenge with these questions.
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
Agreed A,B,C (assuming its a typo, inputs.conf). Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Data/Configureyourinputs

Add your data to Splunk Enterprise. With Splunk Enterprise, you can add data using Splunk Web or Splunk Apps. In addition to these methods, you also can use the following methods.
-The Splunk Command Line Interface (CLI)
-The inputs.conf configuration file. When you specify your inputs with Splunk Web or the CLI, the details are saved in a configuartion file on Splunk Enterprise indexer and heavy forwarder instances.
upvoted 2 times
------------------------

36. Which of the following is a valid distributed search group?
A. [distributedSearch:Paris] default = false servers = server1, server2
B. [searchGroup:Paris] default = false servers = server1:8089, server2:8089
C. [searchGroup:Paris] default = false servers = server1:9997, server2:9997
D. [distributedSearch:Paris] default = false servers = server1:8089; server2:8089

giubal Highly Voted 4 years, 2 months ago
I'm sorry ... D is wrong separator is ';' (not permitted) instead ','
upvoted 10 times
AngusBlack 3 years ago
It's true. They are all wrong.
upvoted 2 times
toney_mu 1 year, 5 months ago
I think its a typo, option D would be the closet
upvoted 3 times
Asami Highly Voted 4 years ago
D. [distributedSearch:Paris] default = false servers = server1:8089; server2:8089
upvoted 5 times
ames 3 years, 10 months ago
But the separator is incorrect
upvoted 5 times
tmmt Most Recent 1 year, 5 months ago
Is D but the separator in incorrect
upvoted 1 times
toney_mu 1 year, 5 months ago
as per latest splunk document https://docs.splunk.com/Documentation/Splunk/9.0.0/DistSearch/Distributedsearchgroups

option is D
upvoted 1 times
huu_nguyen 2 years, 5 months ago
Selected Answer: D
D is the answer but there's a typo in the answer. It should be ',' not ';'
upvoted 3 times
huu_nguyen 2 years, 5 months ago
Selected Answer: D
D is the answer
https://docs.splunk.com/Documentation/Splunk/8.2.4/Admin/Distsearchconf
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: D
D is the correct answer, however with a typo
I checked and you have to provide port number, otherwise you get the following error:
Failed to parse uri for peer:Paris. This search peer will be ignored.
upvoted 1 times
M9201715 2 years, 7 months ago
B and C are definitely wrong. A is not correct since no port number is given, and that is required. See https://docs.splunk.com/Documentation/Splunk/8.0.6/Admin/Distsearchconf Distributed Search Group Definitions:

servers = <comma-separated list>
* A list of search peers that are members of this group.
* The list must use peer identifiers (i.e. hostname:port)

Answer D must be a typo, and supposed to show a comma and not a semi colon. In that case it is correct.
upvoted 1 times
L4Best 3 years ago
It is A, read the documentation : "The servers attribute lists groups of search peers by IP address and management port" , so a server always contains already a port, it is not listed as a seperate attribute.
upvoted 1 times
ArDeKu 3 years, 3 months ago
The answer is B..
Refer link - https://docs.splunk.com/Documentation/Splunk/8.0.3/DistSearch/Distributedsearchgroups
upvoted 1 times
Shaq007 3 years, 6 months ago
I just tested this and a port is required. So, with given choices I would go with D
upvoted 3 times
newrose 3 years, 7 months ago
distsearch.conf specification says:

servers = <comma-separated list>
* An initial list of servers.
* Each member of this list must be a valid URI in the format of
 scheme://hostname:port

I haven't tested, but in my understanding the port value is needed, and in that case it couldn't be alternative A. The separator ";" in alternative D makes it wrong too (maybe a test typo?), although it certainly would be the correct one if the separator was a comma.
upvoted 2 times
dpharker 3 years, 9 months ago
A is the correct one
correct stanza name -> [distributedSearch:xxxx] 
correct separator -> ,
servers listed don't need to have the port defined, and Splunk will use the default attribute listed in distsearch.conf.spec
https://docs.splunk.com/Documentation/Splunk/8.0.6/Admin/Distsearchconf#distsearch.conf.example
upvoted 4 times
Josi12 4 years, 1 month ago
The correct answer is D. The stanza is <DS1_IP:8089>, <DS2_IP:8089>,....
upvoted 3 times
giubal 4 years, 2 months ago
I think it is "D"
<<The servers attribute lists groups of search peers by IP address and management port>>
upvoted 3 times
Amith 4 years, 2 months ago
Sorry A
upvoted 2 times
Amith 4 years, 2 months ago
Answer is C
upvoted 2 times
Load full discussion...
------------------------

53. In which scenario would a Splunk Administrator want to enable data integrity check when creating an index?
A. To ensure that hot buckets are still open for writers and have not been forced to roll to a cold state.
B. To ensure that configuration files have not been tampered with for auditing and/or legal purposes.
C. To ensure that user passwords have not been tampered with for auditing and/or legal purposes.
D. To ensure that data has not been tampered with for auditing and/or legal purposes.

ames Highly Voted 3 years, 10 months ago
True D
upvoted 13 times
toney_mu Most Recent 1 year, 5 months ago
Option D.
This is mentioned in sysadmin pdf
upvoted 2 times
akrmmar 1 year, 10 months ago
D is correct
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 4 times
------------------------

40. Which layers are involved in Splunk configuration file layering? (Choose all that apply.)
A. App context
B. User context
C. Global context
D. Forwarder context

newrose Highly Voted 3 years, 7 months ago
ABC seems right to me
upvoted 11 times
toney_mu Most Recent 1 year, 5 months ago
A,b and C
=====

In case of conflicts, priority is based on the context:
- Global context (index-time)
 -App/User context (search-time)
===
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: ABC
A, B & C are correct
upvoted 2 times
ckmunich 2 years, 11 months ago
A B Cf
C: About configuration file context

To determine the order of directories for evaluating configuration file precedence, Splunk software considers each file's context. Configuration files operate in either a global context or in the context of the current app and user:

 Global. Activities like indexing take place in a global context. They are independent of any app or user. For example, configuration files that determine monitoring or indexing behavior occur outside of the app and user context and are global in nature.

 App/user. Some activities, like searching, take place in an app or user context. The app and user context is vital to search-time processing, where certain knowledge objects or actions might be valid only for specific users in specific apps.
upvoted 4 times
Iona 3 years, 3 months ago
The answer are A B C.
reference link is below
https://docs.splunk.com/Documentation/Splunk/latest/Admin/Wheretofindtheconfigurationfiles
upvoted 3 times
sargeholik 3 years, 4 months ago
AB seems right to me
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
You forgot C.
upvoted 1 times
------------------------

7. Which forwarder type can parse data prior to forwarding?
A. Universal forwarder
B. Heaviest forwarder
C. Hyper forwarder
D. Heavy forwarder

Asami Highly Voted 4 years ago
D. Heavy forwarder
upvoted 12 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Reference URL https://docs.splunk.com/Documentation/Splunk/7.3.1/Forwarding/Typesofforwarders

"A heavy forwarder parses data before forwarding it and can route data based on criteria such as source or type of event."
upvoted 1 times
toney_mu Most Recent 1 year, 5 months ago
Option D
Parsing phase: Handled by indexers (or heavy forwarders)
- Data is broken up into events and advanced processing can be performed
upvoted 1 times
nedwons 2 years, 4 months ago
Heavy forwarder
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 2 times
sargeholik 3 years, 6 months ago
both indexers and heavy forwarders parse events
upvoted 1 times
newrose 3 years, 7 months ago
"The universal forwarder does not parse data. You cannot use it to route data to different Splunk indexers based on its contents.", so the answer is Heavy Forwarder

Source: https://docs.splunk.com/Documentation/Splunk/latest/Forwarding/Typesofforwarders
upvoted 1 times
------------------------

4. In which Splunk configuration is the SEDCMD used?
A. props.conf
B. inputs.conf
C. indexes.conf
D. transforms.conf

emlch 1 year, 10 months ago
There's two transformation methods: SEDCMD or TRANSFORMS

SEDCMD: uses props.conf (used to mask or truncate raw data)
TRANSFORM: uses props.conf and transforms.conf (transforms matching events based on metadata)
upvoted 3 times
toney_mu 1 year, 5 months ago
Yes, this is from data admin pdf
Thank you
upvoted 1 times
alejohu 1 year, 11 months ago
Selected Answer: A
A is correct
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 1 times
ZeusP 3 years, 1 month ago
A in props.conf
upvoted 3 times
matsumo 3 years, 1 month ago
A is correct

<https://docs.splunk.com/Documentation/Splunk/8.2.0/Data/Anonymizedata>
Use the SEDCMD setting. This setting exists in the props.conf configuration file, which you configure on the heavy forwarder.
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the Reference URL

"There are two ways to anonymize data with a heavy forwarder:
- Use the SEDCMD setting. This setting exists in the props.conf configuration file, which you configure on the heavy forwarder. It acts like a sed *nix script to do replacements and substitutions."
upvoted 1 times
sargeholik 3 years, 6 months ago
page 182 data admin
upvoted 1 times
ames 3 years, 11 months ago
"You can specify a SEDCMD configuration in props.conf to address data that contains characters that the third-party server cannot process. " <https://docs.splunk.com/Documentation/Splunk/8.0.5/Forwarding/Forwarddatatothird-partysystemsd>
upvoted 3 times
ames 3 years, 11 months ago
So yea answer is A.
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the Reference URL

"By default, Splunk software does not change the content of an event to make its character set compliant with the third-party server. You can specify a SEDCMD configuration in props.conf to address data that contains characters that the third-part server can't process."
upvoted 1 times
Asami 4 years ago
answer is A
upvoted 1 times
------------------------

82. Which Splunk configuration file is used to enable data integrity checking?
A. props.conf
B. global.conf
C. indexes.conf
D. data_integrity.conf

gsplunker Highly Voted 3 years, 5 months ago
C is correct https://docs.splunk.com/Documentation/Splunk/8.1.2/Security/Dataintegritycontrol#:~:text=When%20you%20enable%20data%20integrity%20control%2C%20Splunk%20Enterprise%20computes%20hashes,it%20to%20a%20l1Hashes%20file.
upvoted 9 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Per the provided Reference URL

"To configure Data Integrity Control, edit indexes.conf to enable the enableDataIntegrityControl attributed for each index. The default value for all indexes is false (off).
upvoted 1 times
toney_mu Most Recent 1 year, 5 months ago
option C
enableDataIntegrityControl=true can be given in indexes.conf
upvoted 1 times
mngesha 1 year, 5 months ago
I also believe the correct answer is C.
https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/Dataintegritycontrol
upvoted 2 times
------------------------

5. Which of the following are supported configuration methods to add inputs on a forwarder? (Choose all that apply.)
A. CLI
B. Edit inputs.conf
C. Edit forwarder.conf
D. Forwarder Management

AngusBlack Highly Voted 3 years ago
It's A and B. On forwarder management you assigned server classes and apps, but adding a new input would be done by editing the inputs.conf on the DS, not using the Forwarder Management web interface.
upvoted 36 times
ucsdmiami2020 2 years, 9 months ago
Agreed A and B. Quoting the Reference URL https://docs.splunk.com/Documentation/Forwarder/7.3.1/Forwarder/
HowtoforwarddatatoSplunkEnterprise#Define_inputs_on_the_universal_forwarder_with_configuration_files

"You can collect data on the universal forwarder using several methods. Define inputs on the universal forwarder with the CLI. You can use the CLI to define inputs on the universal forwarder. After you define the inputs, the universal forwarder collects data based on those definitions as long as it has access to the data that you want to monitor. 
Define inputs on the universal forwarder with configuration files. If the input you want to configure does not have a CLI argument for it, you can configure inputs with configuration files. Create an inputs.conf file in the directory, $SPLUNK_HOME/etc/system/local
upvoted 1 times
Hamiltonian 3 years ago
This needs more upvotes. The main purpose of the forwarder manager is to create server classes and overview the deployment. If you want to change the inputs, you do it via CLI on the forwarder, or change the settings in the inputs.conf file in the associated app on the DS which then will automatically update the settings on the forwarder after some phone home interval.
upvoted 5 times
Hamiltonian 3 years ago
This is what the Add Data does on the Web UI, i.e., updates/creates the inputs.conf in the deployable app on the DS. It then automatically deploys/re-deploys the app to the remote forwarder.
upvoted 3 times
toney_mu 1 year, 5 months ago
Add inputs on forwarders, by either: 
- Editing inputs.conf manually 
- Using Deployment Server
- Running Splunk commands (CLI)
upvoted 2 times
khyoung7410 Highly Voted 3 years, 7 months ago
My ans is A,B,D
upvoted 17 times
Hamiltonian 3 years ago
Wrong. D is not part of the answer.
upvoted 4 times
newrose 3 years, 7 months ago
Agreed. CLI, inputs.conf editing, and via Deployment Server (forwarder maangement).

Source: "Configure the universal forwarder to send data to Splunk Enterprise
" section at https://docs.splunk.com/Documentation/Forwarder/latest/Forwarder/HowtoforwarddatatoSplunkEnterprise
upvoted 5 times
toney_mu 1 year, 5 months ago
A,B and D, you cna see the below notes in data admin pdf
Add inputs on forwarders, by either: 
- Editing inputs.conf manually 
- Using Deployment Server
- Running Splunk commands (CLI)
upvoted 1 times
toney_mu Most Recent 1 year, 5 months ago
I would say options AB and D
===
Add inputs on forwarders, by either: 
- Editing inputs.conf manually 
- Using Deployment Server
- Running Splunk commands (CLI)
====
upvoted 1 times
emlch 1 year, 10 months ago
You can add data inputs with: apps and add-ons, splunk web, CLI, editing inputs.conf
upvoted 2 times
cagdaskarabag 1 year, 11 months ago
Selected Answer: AB
ABD
Forwarder Mgmt from a deployment server.
upvoted 1 times
marda 2 years, 1 month ago
A, B, and D - Check page 175 of the SA PDF and you'll see it says:
-Forwarder management
-CLI
-Edit inputs.conf manually
upvoted 4 times
thissiteisgreat 2 years, 2 months ago
Selected Answer: AB
agreed for AB
upvoted 2 times
RichieL 2 years, 3 months ago
If you already have an input set up for a different forwarder then you can add that same input to a different forwarder using Forwarder Management. My answer is A, B and D
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: AB
A and B for sure, maybe D as well, however D is not configured **on a forwarder**
upvoted 1 times
aallpp 2 years, 6 months ago
Hi ,
I'm stuck between a,b or a,b,d but the answer is a,b,d
upvoted 2 times
M9201715 2 years, 7 months ago
I think it's A, B and D. That's what Forwarder Management is for - to set up the inputs.conf file on the Deployment Server to push out to your Forwarders. Forwarder Management lets you set the apps (what this question is talking about), set the clients, and define the server classes that connect them together
upvoted 3 times
Sunny38 2 years, 11 months ago
A,B and D, check data admin slide 53
upvoted 6 times
BengieQuesada 2 years, 11 months ago
Forwarder Management is no right, answer is A and B. This is because we can add a forward via Deployment Manager but it is no the same that the Forwarder Manager that is used to create server classes and send apps.
upvoted 2 times
hesbee 2 years, 11 months ago
The correct answer is A & B. Please see the reference for clarification - https://docs.splunk.com/Documentation/Forwarder/8.2.1/Forwarder/HowtoforwarddatatoSplunkEnterprise
upvoted 1 times
navotfk 3 years ago
answer is A & D...page 46 Data Admin
upvoted 2 times
hesbee 2 years, 10 months ago
Please can you share the pdf file with me? Thank you!
upvoted 1 times
navotfk 3 years ago
Oops! I meant A B D. (correction)
upvoted 3 times
ZeusP 3 years, 1 month ago
A, B & D
upvoted 4 times
amrit116 3 years, 3 months ago
The questions says add inputs on a Forwarder - 
We can add inputs via the CLI ./splunk add monitor 
We can add inputs editing the inputs.conf 
and also we can push the inputs.conf via the deployment server 
So for me A, B , D
upvoted 7 times
Load full discussion...
------------------------

76. What is the valid option for a [monitor] stanza in inputs.conf?
A. enabled
B. datasource
C. server_name
D. ignoreOlderThan

thomass Highly Voted 3 years, 3 months ago
answer : d
upvoted 5 times
alejohu Most Recent 1 year, 11 months ago
Selected Answer: D
D is correct
upvoted 1 times
ucsdmiami2020 2 years, 10 months ago
Per the Splunk docs / provided URL reference, scroll down to Monitor syntax
Setting: ignoreOlderThan = <time_window>
Description: "Causes the input to stop checking files for updates if the file modification time has passed the <time_window> threshold."
Default: 0 (disabled)
upvoted 4 times
toney_mu 1 year, 5 months ago
Well explained, thank you
upvoted 1 times
------------------------

107. What are the values for host and index for [stanza1] used by Splunk during index time, given the following configuration files?
A. host=server1 index=unixinfo
B. host=server1 index=searchinfo
C. host=searchsvr1 index=searchinfo
D. host=unixsvr1 index=unixinfo

furiousjase Highly Voted 2 years, 10 months ago
The answer is B
Index Time Precedence Order:
1 - System Local directory [etc/system/local]
2 - App Local directories [etc/apps/appname/local] (lexicographical order A..Z)
3 - App default directories [etc/apps/appname/default] (lexicographical order A..Z)
4 - System default directory [etc/system/default]
upvoted 15 times
ucsdmiami2020 2 years, 10 months ago
Confirmed per Splunk documentation https://docs.splunk.com/Documentation/Splunk/8.2.2/Admin/Wheretofindtheconfigurationfiles 
"When consuming a global configuration, such as inputs.conf, Splunk software first uses the attributes from any copy of the file in system/local. Then it looks for any copies of the file located in the app directories, adding any attributes found in them, but ignoring attributes already discovered in system/local."
upvoted 2 times
Rodders2828 Highly Voted 1 year, 7 months ago
Selected Answer: B
Just did the Admin test today (passed), and got this question. In the actual question is not the same provided here - as mentioned below, the two local paths have different app names 'search' and 'unix', and not both as 'search'. In that case 'search' will take precedence over 'unix' - and so B is correct.
upvoted 5 times
toney_mu 1 year, 5 months ago
Yes, you are right.
There is a typo in the queston the last input .conf is as below.
SPLUNK_HOME/etc/app/unix/local/inputs.conf ( not search )
Option B
upvoted 1 times
toney_mu Most Recent 1 year, 5 months ago
There is a typo in the queston the last input .conf is as below.
SPLUNK_HOME/etc/app/unix/local/inputs.conf ( not search )
Option B
upvoted 1 times
Rodders2828 1 year, 7 months ago
Selected Answer: A
Should be 'A'.
Assuming the two 'apps' inputs are in the order provided, then the last stanza will override the first, meaning the index will be set to 'unixinfo'.
The host will be set by the system/local file, which takes precedence over app/local.
upvoted 1 times
Rodders2828 1 year, 7 months ago
Wrong - see my later comment. The actual question has different app names for the local paths, meaning the answer will be B, not A.
upvoted 2 times
kgcykyzoxjxhvfazje 1 year, 7 months ago
I took the test recently and the question here is wrong. With the question as stated here, the answer is A because it takes host from system/local and it takes the last valid stanza from apps/search/local. However in the actual question one of the stanzas comes from apps/unix/local instead of apps/search/local. In that case, it still takes host from system/local, but it takes index from apps/search/local because s comes before u.
upvoted 3 times
denominator 2 years ago
data admin pdf pg 257. Precedence at index-time. 1 - etc/system/local . I think the ans is A, which it had the index name though
upvoted 1 times
toney_mu 1 year, 5 months ago
There is a typo in the queston the last input .conf is as below.
SPLUNK_HOME/etc/app/unix/local/inputs.conf ( not search )
Option B
upvoted 1 times
Seba0297 2 years, 2 months ago
Selected Answer: B
Answer is B for index-time precedence order (see other comments) and, with replicated stanza in the same .conf file, the last one overrides the previous one.
Verify the configuration with btool and you get the last listed entry rule
upvoted 1 times
Seba0297 2 years, 2 months ago
Sorry, my explanation was about A that is correct, i was remembering B... ahah
upvoted 1 times
Helaros 2 years, 2 months ago
I guess the questions is no correct...this way it just cannot be answered correctly because you cannot say which of the stanza1 entries is first in the /apps/search/local/inputs.conf...I would think that the second entry (with "host=unixsvr1" and index="unixinfo" should be located in /etc/apps/unix/local/inputs.conf..this would be inline with other examples used in the Administration courses (see System Administration Slide 82). Therefore answer B would be correct as the 'search' app comes before the 'unix' app in lexicographical order.
B is correct. Question has a typo.
upvoted 2 times
aallpp 2 years, 7 months ago
I think the answe is A.
upvoted 2 times
sam_1215 2 years, 8 months ago
I believe answer is A.
- etc/system/local/ has better precedence at index time
- for identical settings in the same file, the last one overwrite others, see :
https://community.splunk.com/t5/Getting-Data-In/What-is-the-precedence-for-identical-stanzas-within-a-single/m-p/283566
upvoted 3 times
Salman23 2 years, 9 months ago
Answer is A. during index time, higher prevalence is for /etc/system/local/ with: host=server1 this combined with second prevalence wich is /etc/apps/search/local/ witch index=unixinfo.
upvoted 2 times
tom888 2 years, 10 months ago
why not A?
upvoted 2 times
------------------------

127. Which setting allows the configuration of Splunk to allow events to span over more than one line?
A. SHOULD_LINEMERGE = true
B. BREAK_ONLY_BEFORE_DATE = true
C. BREAK_ONLY_BEFORE =
D. SHOULD_LINEMERGE = false

shergar Highly Voted 1 year, 7 months ago
Selected Answer: A
Line merging, which uses the SHOULD_LINEMERGE setting to merge previously separated lines into events. By default, the Splunk platform performs line merging, and the value for SHOULD_LINEMERGE is true. You don't normally need to adjust this setting, but in cases where it is necessary, you must configure this setting in the props.conf configuration file on the forwarder that sends the data to Splunk Cloud Platform. If you configure the Splunk platform to not perform line merging by setting the SHOULD_LINEMERGE attribute to false, then the platform splits the incoming data into lines according to what the LINE_BREAKER setting determines.
upvoted 8 times
KiyaK 1 year, 5 months ago
shouldn't the answer be D (SHOULD_LINEMERGE=false) then?
upvoted 2 times
toney_mu 1 year, 5 months ago
Nope, if you look at the question it should merge two lines so option A is correct
upvoted 1 times
mr56 Most Recent 1 year, 5 months ago
D - Question states over multiple lines. SHOULD_LINEMERGE = [true|false] When set to true, the Splunk platform combines several input lines into a single event,
upvoted 1 times
toney_mu 1 year, 5 months ago
It should combine multiple lines as one so option A si correct
upvoted 1 times
lulu98 1 year, 6 months ago
Selected Answer: D
I would say it's D. Because the question indicates that the events should span over multiple lines, so line merging should be turned off.
upvoted 1 times
------------------------

122. Using the CLI on the forwarder, how could the current forwarder to indexer configuration be viewed?
A. splunk btool server list --debug
B. splunk list forward-indexer
C. splunk list forward-server
D. splunk btool indexes list --debug

toney_mu 1 year, 5 months ago
Option C
upvoted 1 times
Hemnaath 1 year, 6 months ago
Splunk system admin slides page 182 CLEARLY shows that to verify the configuration on the forwarder side by executing CLI command splunk list forward-server and on the indexer, side execute splunk display list
upvoted 1 times
cb42 1 year, 7 months ago
C is correct: splunk list forward-server
upvoted 1 times
------------------------

86. When does a warm bucket roll over to a cold bucket?
A. When Splunk is restarted.
B. When the maximum warm bucket age has been reached.
C. When the maximum warm bucket size has been reached.
D. When the maximum number of warm buckets is reached.

toney_mu 1 year, 5 months ago
Option D
maxWarmDBCount : Determines rolling behavior, warm to cold. 
The maximum number of warm buckets. When the maximum is reached, warm buckets begin rolling to cold
upvoted 1 times
mngesha 1 year, 5 months ago
D is the answer.
maxWarmDBCount reaches 300.
https://docs.splunk.com/Documentation/Splunk/8.1.2/Indexer/Configureindexstorage
upvoted 1 times
denominator 2 years ago
Selected Answer: D
Warm bucket count (default 300), once its reached, it switches to cold. Answer D
upvoted 1 times
BlueRoselia 2 years, 4 months ago
Answer D
maxWarmDBCount Determines rolling behavior, warm to cold. The maximum number of warm buckets. When the maximum is reached, warm buckets begin rolling to cold.
https://docs.splunk.com/Documentation/Splunk/8.2.5/Indexer/Configureindexstorage
upvoted 1 times
BlueRoselia 2 years, 4 months ago
The answer is also A 
Hot buckets also roll to warm automatically when the indexer is restarted
upvoted 1 times
loky0 2 years, 10 months ago
D. While the order of roll is based on age, warm buckets will not roll to cold just because of age. A max limit number of buckets need to be reached in order to trigger the rolling.
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the Splunk Wiki reference URL https://wiki.splunk.com/Deploy:BucketRotationAndRetention

"Bucket Stages. A bucket rolls from one stage to another depending on certain conditions: Hot -> Warm -> Cold -> Frozen (-> Thawed). From hot to warm if its size reaches a limit 'maxDataSize' or its lifetime is older than 'maxHotSpanSecs', or by using a manual command to roll the buckets. From warm to cold; once the number of maxWarmDBCount is reached, the older will be rolled."
upvoted 1 times
jm130106 2 years, 11 months ago
D - From warm to cold; once the number of maxWarmDBCount is reached, the older will be rolled.
https://wiki.splunk.com/Deploy:BucketRotationAndRetention
upvoted 3 times
MariuszSem 2 years, 11 months ago
D is correct
upvoted 1 times
AngusBlack 3 years ago
It is D. From the System Admin course slides:
Warm buckets roll to cold when: The Home Path maximum size is reached (not the bucket size as in the wrong answer) or the maximum warm bucket count (maxWarmDBCount) is reached
upvoted 3 times
ngum 3 years, 2 months ago
B i think.
As buckets age, they "roll" from one state to the next. When data is first indexed, it gets written to a hot bucket. Hot buckets are buckets that are actively being written to. An index can have several hot buckets open at a time. Hot buckets are also searchable.

When certain conditions are met (for example, the hot bucket reaches a certain size or the indexer gets restarted), the hot bucket becomes a warm bucket ("rolls to warm"), and a new hot bucket is created in its place. The warm bucket is renamed but it remains in the same location as when it was a hot bucket. Warm buckets are searchable, but they are not actively written to. There can be a large number of warm buckets.
upvoted 1 times
ngum 3 years, 2 months ago
I could be D .
upvoted 2 times
thomass 3 years, 3 months ago
the question is a little bit tricky, thats can be size or age reached to roll to cold
upvoted 1 times
hwangho 3 years, 6 months ago
D is the correct answer.
https://docs.splunk.com/Documentation/Splunk/8.1.1/Indexer/HowSplunkstoresindexes
Once further conditions are met (for example, the index reaches some maximum number of warm buckets), the indexer begins to roll the warm buckets to cold, based on their age. It always selects the oldest warm bucket to roll to cold. Buckets continue to roll to cold as they age in this manner. Cold buckets reside in a different location from hot and warm buckets. You can configure the location so that cold buckets reside on cheaper storage.
upvoted 4 times
newrose 3 years, 7 months ago
Is that answer correct?
upvoted 2 times
------------------------

136. Which of the following Splunk components require a separate installation package?
A. Deployment server
B. License master
C. Universal forwarder
D. Heavy forwarder

Hemnaath 1 year, 6 months ago
Splunk system admin slides page 175 CLEARLY shows that UF provided as separate installation binary with a built-in license (no limits).
upvoted 4 times
toney_mu 1 year, 5 months ago
I agree
upvoted 1 times
------------------------

123. Which artifact is required in the request header when creating an HTTP event?
A. ackID
B. Token
C. Manifest
D. Host name

harrytbb 1 year, 5 months ago
Selected Answer: B
B... A token is required.
upvoted 2 times
toney_mu 1 year, 5 months ago
I agree
upvoted 1 times
cb42 1 year, 7 months ago
The question probably refers to the HEC. A token is needed, so B is correct
upvoted 1 times
------------------------

120. Which default Splunk role could be assigned to provide users with the following capabilities?


Create saved searches -

Edit shared objects and alerts -
Not allowed to create custom roles
A. admin
B. power
C. user
D. splunk-system-role

toney_mu 1 year, 5 months ago
Answer is B
admin: This role has the most capabilities.
power: This role can edit all shared objects and alerts, tag events, and other similar tasks.
user: This role can create and edit its own saved searches, run searches, edit preferences, create and edit event types, and other similar tasks.
upvoted 1 times
Rodders2828 1 year, 7 months ago
Selected Answer: B
B is correct
upvoted 1 times
------------------------

14. When running the command shown below, what is the default path in which deploymentserver.conf is created? splunk set deploy-poll deployServer:port
A. SPLUNK_HOME/etc/deployment
B. SPLUNK_HOME/etc/system/local
C. SPLUNK_HOME/etc/system/default
D. SPLUNK_HOME/etc/apps/deployment

Jackall Highly Voted 3 years, 4 months ago
question description has a mistake for deploymentclient.conf not depoymentserver.conf
upvoted 13 times
Shafiqul 3 years, 1 month ago
True. Question should have been asking for the client conf since command has deploy poll in there which is normally configured in the client side. Answer should be D while question needs to be updated..
upvoted 3 times
Marco63 2 years, 2 months ago
Answer should be B, but question is wrong, because the file created is "deploymentclient.conf"
upvoted 1 times
RedYeti Highly Voted 2 years, 3 months ago
Selected Answer: B
B. SPLUNK_HOME/etc/system/local
Data Admin course, page 101
upvoted 6 times
toney_mu Most Recent 1 year, 5 months ago
deploymentclient.conf si created, option B
upvoted 1 times
BlueRoselia 2 years, 4 months ago
Data Admin pg 108 Creates deploymentclient.conf in SPLUNK_HOME/etc/system/local
upvoted 1 times
Lewist 2 years, 5 months ago
Selected Answer: B
answer is b
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct with assumption the question was for deploymentclient.conf
upvoted 1 times
Apis 2 years, 6 months ago
Sorry, I meant B is correct: etc/system/local
upvoted 1 times
M9201715 2 years, 7 months ago
Answer is B, deploymentclient.conf is created in /etc/system/local
upvoted 2 times
furiousjase 2 years, 10 months ago
In regards to deploymentclient.conf
deploymentclient.conf for connecting to a deployment server.

Configure the universal forwarder to connect to a deployment server
From a shell or command prompt on the forwarder, run the command:
./splunk set deploy-poll <host name or ip address>:<management port>

The forwarder writes configurations for forwarding data to outputs.conf in $SPLUNK_HOME/etc/system/local/). See Configure forwarding with outputs.conf, for information on outputs.conf.

https://docs.splunk.com/Documentation/Forwarder/8.2.2/Forwarder/Configuretheuniversalforwarder
upvoted 4 times
CCSHAO 3 years ago
Refer to here. By the way, there is no "depoymentserver.conf", only "deploymentclient.conf"

https://docs.splunk.com/Documentation/Splunk/latest/Updating/Configuredeploymentclients
upvoted 2 times
BMO 3 years, 1 month ago
Despite the issue in the question, the response is C
Data Admin - Slide 103
upvoted 1 times
BMO 3 years, 1 month ago
The answer is B not C. (etc/system/local)
upvoted 5 times
happy_and_lucky 3 years, 6 months ago
https://docs.splunk.com/Documentation/Splunk/8.1.1/Updating/Definedeploymentclasses#Ways_to_define_server_classes
"When you use forwarder management to create a new server class, it saves the server class definition in a copy of serverclass.conf under $SPLUNK_HOME/etc/system/local. If, instead of using forwarder management, you decide to directly edit serverclass.conf, it is recommended that you create the serverclass.conf file in that same directory, $SPLUNK_HOME/etc/system/local."
upvoted 1 times
Asami 4 years ago
B. SPLUNK_HOME/etc/system/local
upvoted 3 times
------------------------

62. What conf file needs to be edited to set up distributed search groups?
A. props.conf
B. search.conf
C. distsearch.conf
D. distibutedsearch.conf

newrose Highly Voted 3 years, 7 months ago
C. distsearch.conf
upvoted 8 times
toney_mu Most Recent 1 year, 5 months ago
https://docs.splunk.com/Documentation/Splunk/9.0.0/DistSearch/Distributedsearchgroups

Option C
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
DeltaPotato 2 years, 10 months ago
Confirmed C. https://docs.splunk.com/Documentation/Splunk/8.2.2/DistSearch/Distributedsearchgroups
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Using the provided URL reference you read 

"You can group your search peers to facilitate searching on a subset of them. Groups of search peers are known as "distributed search groups." You specify distributed search groups in the distsearch.conf file"
upvoted 1 times
------------------------

125. In addition to single, non-clustered Splunk instances, what else can the deployment server push apps to?
A. Universal forwarders
B. Splunk Cloud
C. Linux package managers
D. Windows using WMI

harrytbb 1 year, 5 months ago
Selected Answer: A
A is correct
upvoted 1 times
tender_tangerine 1 year, 5 months ago
"The deployment server is the tool for distributing configurations, apps, and content updates to groups of Splunk Enterprise instances. You can use it to distribute updates to most types of Splunk Enterprise components: forwarders, non-clustered indexers, and search heads."
upvoted 1 times
cb42 1 year, 7 months ago
only A seems valid. Deployment server can't push apps to splunk cloud.
upvoted 2 times
------------------------

92. Which of the following must be done to define user permissions when integrating Splunk with LDAP?
A. Map Users
B. Map Groups
C. Map LDAP Inheritance
D. Map LDAP to Active Directory

afroben Highly Voted 3 years, 7 months ago
Answer is B. Map LDAP groups to Splunk roles.
upvoted 12 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Not A and B per the Splunk Reference URL Quoting below https://docs.splunk.com/Documentation/Splunk/8.1.3/Security/ConfigureLDAPwithSplunkWeb

"You can map either users or groups, but not both. If you are using groups, all users must be members of an appropriate group. Groups inherit capabilities form the highest level role they're a member of." "If your LDAP environment does not have group entries, you can treat each user as its own group."
upvoted 2 times
mngesha Most Recent 1 year, 5 months ago
I would say B is the answer though that is not explicitly stated in the following link
https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/ConsiderationsfordecidinghowtomonitorWindowsdata
upvoted 1 times
cagdaskarabag 1 year, 11 months ago
https://docs.splunk.com/Documentation/Splunk/9.0.0/Security/ConfigureLDAPwithSplunkWeb#:~:text=There%20are%20three,their%20connection%20order.
upvoted 1 times
Splunkv 3 years, 3 months ago
Once you have configured Splunk Enterprise to authenticate via your LDAP server, map your LDAP groups to Splunk roles. If you do not use groups, you can map users individually.

from https://docs.splunk.com/Documentation/Splunk/8.1.3/Security/ConfigureLDAPwithSplunkWeb
upvoted 3 times
Splunkv 3 years, 3 months ago
A and B are correct. as both groups and users can be mapped to splunk roles
upvoted 2 times
Hamiltonian 3 years ago
Confirmed A and B. "After you have configured the Splunk platform to authenticate using your LDAP server, map LDAP groups in your environment to Splunk roles. If you do not use groups, you can map users individually."
https://docs.splunk.com/Documentation/Splunk/8.2.0/Security/ConfigureLDAPwithSplunkWeb
upvoted 1 times
thomass 3 years, 3 months ago
answer: B
upvoted 3 times
newrose 3 years, 7 months ago
thoughts on that one?
upvoted 1 times
------------------------

113. Consider a company with a Splunk distributed environment in production. The Compliance Department wants to start using Splunk; however, they want to ensure that no one can see their reports or any other knowledge objects. Which Splunk Component can be added to implement this policy for the new team?
A. Indexer
B. Deployment server
C. Universal forwarder
D. Search head

inwigboji Highly Voted 2 years, 9 months ago
Search head
upvoted 6 times
xouu Most Recent 1 year, 5 months ago
Selected Answer: D
D.
User authorization
All authorization for a distributed search originates from the search head. At the time it sends the search request to its search peers, the search head also distributes the authorization information. It tells the search peers the name of the user running the search, the user's role, and the location of the distributed authorize.conf file containing the authorization information.
https://docs.splunk.com/Documentation/Splunk/9.0.3/DistSearch/Knowledgebundlereplication
upvoted 1 times
denominator 2 years ago
I thought it was a dedicated Indexer, but that's incorrect, it's definitely another search head since the key word is dedicated knowledge objects, so its search heads.
upvoted 3 times
Helaros 2 years, 2 months ago
D - Search Head ist correct.
Sys Admin Slide 199
upvoted 2 times
[Removed] 2 years, 5 months ago
Selected Answer: D
Search Head
upvoted 2 times
Powdered_Sugar 2 years, 8 months ago
B. The Deployment Server will distribute the configurations to the machines that will prevent access to those reports and Knowledge Objects
upvoted 2 times
------------------------

91. An organization wants to collect Windows performance data from a set of clients, however, installing Splunk software on these clients is not allowed. What option is available to collect this data in Splunk Enterprise?
A. Use Local Windows host monitoring.
B. Use Windows Remote Inputs with WMI.
C. Use Local Windows network monitoring.
D. Use an index with an Index Data Type of Metrics.

ugo1 Highly Voted 3 years, 8 months ago
The Ans is B
https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/ConsiderationsfordecidinghowtomonitorWindowsdata
upvoted 13 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/8.1.0/Data/ConsiderationsfordecidinghowtomonitorWindowsdata

"The Splunk platform collects remote Windows data for indexing in one of two ways: From Splunk forwarders, Using Windows Management Instrumentation (WMI). For Splunk Cloud deployments, you must use the Splunk Universal Forwarder on a Windows machines to montior remote Windows data."
upvoted 1 times
Sarah1124 Highly Voted 3 years, 8 months ago
Answer is B.
upvoted 8 times
mngesha Most Recent 1 year, 5 months ago
Answer is B as per the following document.
https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/ConsiderationsfordecidinghowtomonitorWindowsdata
upvoted 1 times
denominator 2 years ago
Selected Answer: B
pg 190 data admin pdf
upvoted 3 times
trevero 3 years ago
Answer is B, per Data Admin pdf page 190
upvoted 3 times
------------------------

88. How is a remote monitor input distributed to forwarders?
A. As an app.
B. As a forward.conf file.
C. As a monitor.conf file.
D. As a forwarder monitor profile.

jakal12345 Highly Voted 2 years, 11 months ago
A.
As an App.
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Reference URL https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/Usingforwardingagents
Scroll down to the section Titled, How to configure forwarder inputs, and subsection Here are the main ways that you can configure data inputs on a forwarder

Install the app or add-on that contains the inputs you wants
upvoted 1 times
newrose Highly Voted 3 years, 7 months ago
I think the answer is A
upvoted 5 times
mngesha Most Recent 1 year, 5 months ago
Best answer would be A. I will be corrected if correct answer is presented
https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/Usingforwardingagents
upvoted 1 times
Hudda 3 years ago
Friends, could you please confirm this answer?
upvoted 3 times
------------------------

84. After how many warnings within a rolling 30-day period will a license violation occur with an enforced Enterprise license?
A. 1
B. 3
C. 4
D. 5

mybox1 Highly Voted 3 years, 6 months ago
D is correct for Splunk 8.0 and older. Since 8.1, 45 license warnings in a rolling 60 day period cause violation.
upvoted 6 times
AngusBlack 3 years ago
For licenses of 100GB or less
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the Splunk reference URL, https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Aboutlicenseviolations

"Enterprise Trial license. If you get five or more warnings in a rolling 30 days period, you are in violation of your license. Dev/Test license. If you generate five or more warnings in a rolling 30-day period, you are in violation of your license. Developer license. If you generate five or more warnings in a rolling 30-day period, you are in violation of your license. BUT for Free license. If you get three or more warnings in a rolling 30 days period, you are in violation of your license."
upvoted 1 times
mngesha Most Recent 1 year, 5 months ago
for the new version the violation may be different as per the following link yet for the trail license it is 5 days out of 30. For the stack license it is 45 times in 60 days but that is not included in the answer and that tells me this question is old for the old licenses.
https://docs.splunk.com/Documentation/Splunk/8.1.2/Admin/Aboutlicenseviolations
upvoted 1 times
------------------------

79. The Splunk administrator wants to ensure data is distributed evenly amongst the indexers. To do this, he runs the following search over the last 24 hours: index=*
What field can the administrator check to see the data distribution?
A. host
B. index
C. linecount
D. splunk_server

gsplunker Highly Voted 3 years, 5 months ago
Yes it is splunk_server that will list the indexers with event count
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Agreed it's D. Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Knowledge/Usedefaultfields

splunk_server
The splunk server field contains the name of the Splunk server containing the event. Useful in a distributed Splunk environment. 
Example: Restrict a search to the main index on a server named remote.
splunk_server=remote index=main 404
upvoted 3 times
mngesha Most Recent 1 year, 5 months ago
not sure if splunk_server would be the silver bullet to get the data distribution. splunk_server would help to filter events based on indexer server for latency purposes as described in this link and is best positioned for the answer in this case.
D would be the closest answer in my humble opinion.
https://docs.splunk.com/Documentation/Splunk/8.0.5/Search/Searchdistributedpeers
upvoted 1 times
denominator 2 years ago
Module 9 lab pdf pg37 ans D
upvoted 1 times
denominator 2 years ago
System Admin Lab
upvoted 1 times
Salman23 2 years, 9 months ago
I would say A is correct, When you perform a search and reporting app and get results, you will see on the left side selected fields if you click on hosts you will get all indexers link to the searchhead with the count and percentages according the search results.
upvoted 1 times
TeeCeeP 3 years, 6 months ago
splunk_server its in the lab
upvoted 3 times
leiot 3 years, 7 months ago
i think its D
upvoted 2 times
newrose 3 years, 7 months ago
Shouldnt it be B
upvoted 1 times
nunxyo 3 years, 7 months ago
it says indexers not indexes, right?
upvoted 1 times
------------------------

72. Which of the following configuration files are used with a universal forwarder? (Choose all that apply.)
A. inputs.conf
B. monitor.conf
C. outputs.conf
D. forwarder.conf

thomass Highly Voted 3 years, 3 months ago
answer: a,c
upvoted 9 times
ucsdmiami2020 2 years, 10 months ago
Per the provided URL referencehttps://docs.splunk.com/Documentation/Forwarder/8.0.5/Forwarder/Configuretheuniversalforwarder
--Key configuration files are:
inputs.conf controls how the forwarder collects data.
outputs.conf controls how the forwarder sends data to an indexer or other forwarder
server.conf for connection and performance tuning
deploymentclient.conf for connecting to a deployment server
upvoted 2 times
harrytbb Most Recent 1 year, 5 months ago
Selected Answer: AC
A & C...
upvoted 1 times
------------------------

129. Which of the following are reasons to create separate indexes? (Choose all that apply.)
A. Different retention times.
B. Increase number of users.
C. Restrict user permissions.
D. File organization.

KiyaK 1 year, 5 months ago
You seperate indexes based on 3 reasons: retention policy (A), visibility (C), log format. so the answer is AC
upvoted 1 times
Rodders2828 1 year, 7 months ago
Selected Answer: AC
Agree, A & C
upvoted 2 times
anonyuser 1 year, 7 months ago
Selected Answer: AC
Yeah, I feel like this is AC, but D could be possible if it elaborated a little more on why.
upvoted 2 times
cb42 1 year, 7 months ago
Selected Answer: AC
D: is not specific enough in my opinion. It is not a splunk reason.
upvoted 3 times
shergar 1 year, 7 months ago
I would say C also applies, as you can restrict roles to only have access to certain indexes.
upvoted 1 times
------------------------

128. In this example, if useACK is set to true and the maxQueueSize is set to 7MB, what is the size of the wait queue on this universal forwarder?
A. 21MB
B. 28MB
C. 14MB
D. 7MB

bapun17 1 year, 6 months ago
Right, A https://docs.splunk.com/Documentation/Splunk/latest/Forwarding/Protectagainstlossofin-flightdata#:~:text=The%20default%20for%20the%20maxQueueSize,wait%20queue%20size%20is%2021MB.
upvoted 1 times
cb42 1 year, 7 months ago
Selected Answer: A
It is always three times the value of the output queue
upvoted 4 times
------------------------

130. Which network input option provides durable file-system buffering of data to mitigate data loss due to network outages and splunkd restarts?
A. diskQueueSize
B. durableQueueSize
C. persistentQueueSize
D. queueSize

Hemnaath 1 year, 6 months ago
Splunk data admin slides page 145 CLEARLY shows that Persistent queue provides additional file-system buffering of data and useful for high volume data and in the case of network outage to indexers.
upvoted 3 times
Rodders2828 1 year, 7 months ago
Selected Answer: C
https://docs.splunk.com/Documentation/SplunkCloud/latest/Data/Usepersistentqueues
upvoted 2 times
------------------------

135. Which forwarder is recommended by Splunk to use in a production environment?
A. Heavy forwarder
B. SSL forwarder
C. Lightweight forwarder
D. Universal forwarder

Hemnaath 1 year, 6 months ago
UF has minimal footprint on production servers and generally requires less bandwidth and has faster processing than same data on HF.
UF is recommended by splunk to use in production Environments.
upvoted 1 times
------------------------

133. When using license pools, volume allocations apply to which Splunk components?
A. Indexers
B. Indexes
C. Heavy Forwarders
D. Search Heads

cb42 1 year, 7 months ago
Selected Answer: A
see https://docs.splunk.com/Documentation/Splunk/9.0.2/Admin/Createalicensepool
upvoted 2 times
------------------------

16. When configuring monitor inputs with whitelists or blacklists, what is the supported method of filtering the lists?
A. Slash notation
B. Regular expression
C. Irregular expression
D. Wildcard-only expression

Asami Highly Voted 4 years ago
B. Regular expression
upvoted 9 times
erick165 Most Recent 1 year, 7 months ago
Correction: D wildcards is incorrect as it says wildcards only so Regular Expressions is correct.
upvoted 2 times
erick165 1 year, 7 months ago
D. Wildcards:
clientName is a logical or tag name that can be assigned to a deployment client in deploymentclient.conf.
ipAddress is the IP address of the deployment client. Can use wildcards, such as 10.1.1.*
DNSname is the DNS name of the deployment client. Can use wildcards, such as *.ops.yourcompany.com
hostname is the host name of deployment client. Can use wildcards, such as *.splunk.com
instanceId is the instanceId of the client. This is a GUID string, for example: ffe9fe01-a4fb-425e-9f63-56cc274d7f8b.
upvoted 1 times
RedYeti 2 years, 3 months ago
Selected Answer: B
B. Regular expression
Data Admin course, page 123
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 2 times
DeltaPotato 2 years, 10 months ago
B. Regular Expression - Page 123 - Data Admin PDF.
upvoted 3 times
newrose 3 years, 7 months ago
https://docs.splunk.com/Documentation/Splunk/latest/Data/Whitelistorblacklistspecificincomingdata#Include_or_exclude_specific_incoming_data is a better reference
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the reference URL

"When you define filter entries, you must use exact regular expression syntax."
upvoted 1 times
------------------------

118. Which of the following applies only to Splunk index data integrity check?
A. Lookup table
B. Summary Index
C. Raw data in the index
D. Data model acceleration

linux_programmer46 1 year, 8 months ago
The answer is C. Please read info in link:
https://docs.splunk.com/Documentation/Splunk/9.0.1/Security/Dataintegritycontrol
upvoted 1 times
hesbee 2 years, 8 months ago
I think the answer is C since Data Integrity Check is done when data is being ingested which is the raw data.
upvoted 1 times
qwerasdf1234 2 years, 9 months ago
Any ideas what is the answer here?
upvoted 1 times
Powdered_Sugar 2 years, 8 months ago
I believe the answer is C. Splunk data integrity computes hashes on the raw data, and then a secondary hash on all of those hashes. C is the only answer that seems even remotely related, so that'd be my choice.

https://docs.splunk.com/Documentation/Splunk/8.0.0/Security/Dataintegritycontrol
upvoted 1 times
------------------------

114. Which of the following is an appropriate description of a deployment server in a non-cluster environment?
A. Allows management of local Splunk instances, requires Enterprise license, handles job of sending configurations packaged as apps, can automatically restart remote Splunk instances.
B. Allows management of remote Splunk instances, requires Enterprise license, handles job of sending configurations, can automatically restart remote Splunk instances.
C. Allows management of remote Splunk instances, requires no license, handles job of sending configurations, can automatically restart remote Splunk instances.
D. Allows management of remote Splunk instances, requires Enterprise license, handles job of sending configurations, can manually restart remote Splunk instances.

islamjy Highly Voted 2 years, 10 months ago
B is correct, data admin 97
upvoted 7 times
ucsdmiami2020 2 years, 10 months ago
Using Splunk docs, specifically the reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Updating/Deploymentserverarchitecture
"A deployment client is a Splunk instance remotely configured by a deployment server".
upvoted 1 times
ChinaBandit Highly Voted 2 years, 10 months ago
Ans is A. P91 Data admin. 'Built-in tool for centrally managing configuration packages as apps for clients'
upvoted 5 times
tjwe 2 years, 8 months ago
Not A, it says local instances.
upvoted 2 times
Mando22 Most Recent 1 year, 9 months ago
Correct Answer: B
upvoted 1 times
------------------------

96. Which of the following statements accurately describes using SSL to secure the feed from a forwarder?
A. It does not encrypt the certificate password.
B. SSL automatically compresses the feed by default.
C. It requires that the forwarder be set to compressed=true.
D. It requires that the receiver be set to compression=true.

Shaq007 Highly Voted 3 years, 6 months ago
B. SSL automatically compresses the feed by default.
Data Admin PDF Page 67
- Encrypts the feed
- Automatically compresses the feed
- Increase CPU Utilization
upvoted 13 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/AboutsecuringyourSplunkconfigurationwithSSL

"You can turn on SSL encryption using the default certificate to provide encryption and compression. However, communication using the default certificate does not provide secure authentication, as the certificate password is supplied with every installation of Splunk software. The default certificates are set to expire threee years after initial startup, and forwarder to indexer communications will fail at this point."
upvoted 1 times
mybox1 Highly Voted 3 years, 6 months ago
B is correct, turning on SSL automatically compresses the feed
upvoted 6 times
useheee Most Recent 1 year, 10 months ago
Selected Answer: B
ans is B
upvoted 2 times
denominator 2 years ago
Data Admin Pdf pg 74, ans B
upvoted 3 times
Jackall 3 years, 4 months ago
Only B is right. Once ssl is enabled, compress is enabled by default.
upvoted 5 times
Sarah1124 3 years, 8 months ago
Isn't the answer B?
upvoted 4 times
Toanbego 3 years, 8 months ago
Yup. It definelty encrypts the password. Right after a restart if I'm not mistaken
upvoted 3 times
------------------------

31. What is the default character encoding used by Splunk during the input phase?
A. UTF-8
B. UTF-16
C. EBCDIC
D. ISO 8859

emlch 1 year, 10 months ago
UTF-8 and you can change it using the CHARSET attribute
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 2 times
loky0 2 years, 10 months ago
A. P207 Data admin pdf
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/7.3.1/Data/Configurecharactersetencoding

"Configure character set encoding. Splunk software attempts to apply UTF-8 encoding to your scources by default. If a source foesn't use UTF-8 encoding or is a non-ASCII file, Splunk software tries to convert data from the source to UTF-8 encoding unless you specify a character set to use by setting the CHARSET key in the props.conf file."
upvoted 1 times
amsinha 3 years, 5 months ago
A is True !!
upvoted 1 times
ectomorph 3 years, 10 months ago
A:

https://docs.splunk.com/Splexicon:Charactersetencoding#:~:text=A%20method%20for%20displaying%20and,conf%20configuration%20file.
upvoted 1 times
amporiik 3 years, 11 months ago
A. UTF-8
upvoted 1 times
------------------------

21. To set up a network input in Splunk, what needs to be specified?
A. File path.
B. Username and password.
C. Network protocol and port number.
D. Network protocol and MAC address.

oksey Highly Voted 3 years, 10 months ago
C is the Ans
upvoted 18 times
AbuAli Highly Voted 4 years, 3 months ago
According to Splunk doc:
https://docs.splunk.com/Documentation/Splunk/8.0.3/Data/Monitornetworkports

Network port and protocol is required
upvoted 10 times
ames 3 years, 10 months ago
So.... C?
upvoted 7 times
emlch Most Recent 1 year, 10 months ago
Check question 18
upvoted 3 times
Helaros 2 years, 2 months ago
Selected Answer: C
C is correct
upvoted 4 times
Toffaletti 2 years, 3 months ago
Selected Answer: C
C is the answ
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 3 times
JJJefferson 2 years, 9 months ago
So this is a duplicate question, the previous answer was protocol and port.....now its file path? I don't think so.
upvoted 2 times
gsplunker 3 years, 5 months ago
C is the answer
upvoted 2 times
happy_and_lucky 3 years, 6 months ago
q said network input, so makes more sense if C = network protocol and port
upvoted 2 times
Sandy_1988 3 years, 7 months ago
C is the answer
upvoted 3 times
------------------------

20. Which optional configuration setting in inputs.conf allows you to selectively forward the data to specific indexer(s)?
A. _TCP_ROUTING
B. _INDEXER_LIST
C. _INDEXER_GROUP
D. _INDEXER_ROUTING

emlch 1 year, 10 months ago
[monitor:<file path>]
_TCP_ROUTING = <index name>

In inputs.conf (UF)
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 3 times
loky0 2 years, 10 months ago
A
P66 on Data Admin pdf
upvoted 3 times
ames 3 years, 10 months ago
A. 

Extra read: https://docs.splunk.com/Documentation/Splunk/7.0.3/Forwarding/Routeandfilterdatad#Perform_selective_indexing_and_forwarding
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Per the provided reference URL

_TCP_ROUTING = <tcpout_group_name>,<tcpout_group_name>,...

 Specifies a comma-separated list of tcpout group names. Use this setting to selectively forward your data to specific indexers by specifying the tcpout groups that the forwarder should use when forwarding the data.

Define the tcpout group names in the outputs.conf file in [tcpout:<tcpout_group_name>] stanzas. The groups present in defaultGroup in [tcpout] stanza in the outputs.conf file.
upvoted 2 times
Asami 4 years ago
A. _TCP_ROUTING
upvoted 3 times
------------------------

18. What are the minimum required settings when creating a network input in Splunk?
A. Protocol, port number
B. Protocol, port, location
C. Protocol, username, port
D. Protocol, IP, port number

BMO Highly Voted 3 years, 1 month ago
A is correct
Data Admin - Slide 137
upvoted 5 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the Reference URL https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Inputsconf

[tcp://<remote server>:<port>]
*Configures the input to listen on a specific TCP network port.
*If a <remote server> makes a connection to this instance, the input uses this stanza to configure itself.
*If you do not specify <remote server>, this stanza matches all connections on the specified port.
*Generates events with source set to "tcp:<port>", for example: tcp:514
*If you do not specify a sourcetype, generates events with sourcetype set to "tcp-raw"
upvoted 2 times
emlch Most Recent 1 year, 10 months ago
Selected Answer: A
When you configure a network input you have to specify 4 configurations (only 2 are optional):
- Protocol: TCP or UDP
- Port
- Source name override
- Only Accept Conection from
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 4 times
krishdee 3 years, 2 months ago
A. Protocal and Port Number
upvoted 1 times
ames 3 years, 10 months ago
A
[tcp:<port>]
* Configures the input listen on the specified TCP network port.
https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Inputsconf
upvoted 1 times
Asami 4 years ago
A. Protocol, port number
upvoted 2 times
------------------------

12. In which phase of the index time process does the license metering occur?
A. Input phase
B. Parsing phase
C. Indexing phase
D. Licensing phase

ames Highly Voted 3 years, 10 months ago
True, its C. Part of the indexing process is to measure the volume of data being ingested, and report that volume to the license master for license volume tracking.
upvoted 6 times
Marco63 Most Recent 2 years, 2 months ago
Actually license metering happens AFTER parsing and BEFORE indexing, it's an intermediate phase.
upvoted 2 times
emlch 1 year, 10 months ago
Nah, the indexing phase includes license meter and indexing. The license meter runs as data is initially written to disk
upvoted 2 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
BMO 3 years, 1 month ago
Data Admin - Slide 14
C. is the correct answer
upvoted 1 times
splunkuser03 2 years, 6 months ago
Can anyone help sharing the splunk admin pdf.
upvoted 1 times
krishdee 3 years, 2 months ago
C. Indexing Phase
upvoted 1 times
Asami 4 years ago
C. Indexing phase
upvoted 3 times
demarko 3 years, 9 months ago
https://docs.splunk.com/Documentation/Splunk/8.0.6/Admin/HowSplunklicensingworks
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Quoting the reference URL, 

"When ingesting event data, the measured data volume is based on the new raw data that is placed into the indexing pipeline. Because the data is measured at the indexing pipeline, data that is filetered and dropped prior to indexing does not count against the license volume qota."
upvoted 1 times
------------------------

117. Consider the following stanza in inputs.conf:

What will the value of the source filed be for events generated by this scripts input?
A. /opt/splunk/etc/apps/search/bin/lister.sh
B. unknown
C. lister
D. lister.sh

loky0 Highly Voted 2 years, 10 months ago
A. it defaults to script name, sourcetype also does the same
upvoted 7 times
ucsdmiami2020 2 years, 10 months ago
Per Splunk docs reference 
https://docs.splunk.com/Documentation/Splunk/8.2.2/Admin/Inputsconf
-Scroll down to source = <string>
*Default: the input file path
upvoted 3 times
Floyda Most Recent 1 year, 11 months ago
Selected Answer: A
script name
upvoted 1 times
imggnz 2 years, 3 months ago
A, /opt/splunk/etc/....
upvoted 1 times
------------------------

95. Which feature in Splunk allows Event Breaking, Timestamp extractions, and any advanced configurations found in props.conf to be validated all through the UI?
A. Apps
B. Search
C. Data preview
D. Forwarder inputs

ugo1 Highly Voted 3 years, 8 months ago
The Ans is C
Watch this video
http://www.splunk.com/view/SP-CAAAGPR
upvoted 14 times
ucsdmiami2020 2 years, 10 months ago
Video no longer available :(
upvoted 2 times
mybox1 Highly Voted 3 years, 6 months ago
C - Use Data Preview to validate event creation during the
parsing phase
upvoted 8 times
Steve2610 Most Recent 1 year, 11 months ago
Selected Answer: C
Data Admin - Slide 22
upvoted 1 times
denominator 2 years ago
System admin pdf pg 240. Ans Data Preview "C"
upvoted 1 times
Jackall 3 years, 4 months ago
C is answer.
upvoted 4 times
Sarah1124 3 years, 8 months ago
Answer is C. Data Preview.
upvoted 6 times
------------------------

30. Which of the following are supported options when configuring optional network inputs?
A. Metadata override, sender filtering options, network input queues (quantum queues)
B. Metadata override, sender filtering options, network input queues (memory/persistent queues)
C. Filename override, sender filtering options, network output queues (memory/persistent queues)
D. Metadata override, receiver filtering options, network input queues (memory/persistent queues)

Asami Highly Voted 4 years ago
B. Metadata override, sender filtering options, network input queues (memory/persistent queues)
upvoted 20 times
Steve2610 Most Recent 1 year, 11 months ago
Selected Answer: B
Data Admin Slide 143
upvoted 1 times
Marco63 2 years, 2 months ago
Selected Answer: B
See Data Admin page 141
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 3 times
loky0 2 years, 10 months ago
B. P141 Data admin pdf
upvoted 3 times
Sandy_1988 3 years, 7 months ago
B is the answer
upvoted 2 times
Mimi88 3 years, 9 months ago
Ans. B. See Optional Network Input Settings in the Data Administration PDF
upvoted 4 times
Sapero 3 years, 10 months ago
B is the correct Answer for sure.
upvoted 2 times
ectomorph 3 years, 10 months ago
B is correct:

https://docs.splunk.com/Documentation/Splunk/latest/Data/Monitornetworkports
upvoted 2 times
Sammy33 4 years, 1 month ago
Should be B, according to the Data Admin Course ppx from Splunk
upvoted 2 times
giubal 4 years, 2 months ago
It is trick question, because on forwarder when the queue is full due to latency toward the indexer, persistent queue (writing to a file) is used and is preserves across restarts. So even the answer C could be right.
upvoted 1 times
Amith 4 years, 2 months ago
It should be B, Any one can clarify ?
upvoted 3 times
------------------------

27. Where can scripts for scripted inputs reside on the host file system? (Choose all that apply.)
A. $SPLUNK_HOME/bin/scripts
B. $SPLUNK_HOME/etc/apps/bin
C. $SPLUNK_HOME/etc/system/bin
D. $SPLUNK_HOME/etc/apps/<your_app>/bin

Asami Highly Voted 4 years ago
A. $SPLUNK_HOME/bin/scripts
C. $SPLUNK_HOME/etc/system/bin
D. $SPLUNK_HOME/etc/apps/<your_app>/bin
upvoted 8 times
BMO Highly Voted 3 years, 1 month ago
ACD is correct
Data Admin - Slide 143
upvoted 5 times
ucsdmiami2020 2 years, 9 months ago
Agreed A, C, D. Quoting the provided Splunk reference URL https://docs.splunk.com/Documentation/Splunk/7.3.1/Data/Getdatafromscriptedinputs#Where_to_place_the_scripts_for_scripted_inputs

"Where to place the scripts for scripted inputs. The script that you refer to in $SCRIPT can reside in only one of the following places on the host file system:
 $SPLUNK_HOME/etc/system/bin
 $SPLUNK_HOME/etc/apps/<your_App>/bin
 $SPLUNK_HOME/bin/scripts
As a best practice, put your script in the bin/ directory that is nearest to the inputs.conf file that calls your script on the host file system."
upvoted 2 times
Steve2610 Most Recent 1 year, 11 months ago
Selected Answer: ACD
Data Admin Slide 154
upvoted 2 times
Nnatech 2 years ago
Selected Answer: ACD
ACD is correct
upvoted 1 times
denominator 2 years ago
Data Admin Pg 154
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: ACD
A, C & D are correct
upvoted 2 times
thomass 3 years, 3 months ago
answer : acd
upvoted 2 times
------------------------

26. What is the correct order of steps in Duo Multifactor Authentication?
A. 1. Request Login 2. Connect to SAML server 3. Duo MFA 4. Create User session 5. Authentication Granted 6. Log into Splunk
B. 1. Request Login 2. Duo MFA 3. Authentication Granted 4. Connect to SAML server 5. Log into Splunk 6. Create User session
C. 1. Request Login 2. Check authentication / group mapping 3. Authentication Granted 4. Duo MFA 5. Create User session 6. Log into Splunk
D. 1. Request Login 2. Duo MFA 3. Check authentication / group mapping 4. Create User session 5. Authentication Granted 6. Log into Splunk

Asami Highly Voted 4 years ago
C. 1. Request Login 2. Check authentication / group mapping 3. Authentication Granted 4. Duo MFA 5. Create User session 6. Log into Splunk
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Using the provided DUO/Splunk reference URL https://duo.com/docs/splunk
Scroll down to the Network Diagram section and note the following 6 similar steps
1 - SPlunk connection initiated
2 - Primary authentication
3 - Splunk connection established to Duo Security over TCP port 443
4 - Secondary authentication via Duo Security's service
5 - Splunk receives authentication response
6 - Splunk session logged in
upvoted 4 times
Steve2610 Most Recent 1 year, 11 months ago
Selected Answer: C
System Admin - P:230
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 1 times
BMO 3 years, 1 month ago
C is correct
System Admin - Slide 225
upvoted 1 times
Sandy_1988 3 years, 6 months ago
Yes C is the ans.
upvoted 1 times
------------------------

102. What action is required to enable forwarder management in Splunk Web?
A. Navigate to Settings > Server Settings > General Settings, and set an App server port.
B. Navigate to Settings > Forwarding and receiving, and click on Enable Forwarding.
C. Create a server class and map it to a client in SPLUNK_HOME/etc/system/local/serverclass.conf.
D. Place an app in the SPLUNK_HOME/etc/deployment-apps directory of the deployment server.

lilsem Highly Voted 2 years, 10 months ago
https://docs.splunk.com/Documentation/MSApp/2.0.3/MSInfra/Setupadeploymentserver 
"To activate deployment server, you must place at least one app into %SPLUNK_HOME%\etc\deployment-apps on the host you want to act as deployment server. In this case, the app is the "send to indexer" app you created earlier, and the host is the indexer you set up initially."
upvoted 5 times
Ada1234 2 years, 5 months ago
yes answer is D
upvoted 2 times
splunkkid Most Recent 2 years ago
Selected Answer: D
Ans: D
A & B is definitely incorrect it should be Settings > Forwarder Management. The "Forwarder Management" page in the GUI would not be available without placing a single in the deployment-apps path of the DS.
upvoted 2 times
BlueRoselia 2 years, 4 months ago
answer D
the wording is not wright for C
Overview of how to set up Forwarder Management in your implementation:
1.On DS, add one or more apps in SPLUNK_HOME/etc/deployment-apps
2. On forwarders, run splunk set deploy-poll <deployServer:port>, to make it a client
3.create one or more server classes------ add client to a server class
4.Verify on deployment server:
List of clients phoning home
Deployment status
5.Verify on forwarders:
etc/appsfolder for deployed apps
upvoted 3 times
kiranhar 2 years, 11 months ago
Answer is C
upvoted 1 times
------------------------

90. Which option on the Add Data menu is most useful for testing data ingestion without creating inputs.conf?
A. Upload option
B. Forward option
C. Monitor option
D. Download option

Sarah1124 Highly Voted 3 years, 8 months ago
Answer is A. Upload option doesn't create inputs.conf
upvoted 18 times
splunkkid Most Recent 2 years ago
Selected Answer: A
Ans: A
Allows you to preview your data and plan your configurations for props and transforms.
upvoted 2 times
Seba0297 2 years, 2 months ago
Selected Answer: A
Answers is A. Pag. 244 System admin pdf
"Entries in the inputs.conf file are not created when Upload or index Once is selected."
upvoted 1 times
SasnycoN 2 years, 7 months ago
Selected Answer: A
Answer is A. Upload option doesn't create inputs.conf
upvoted 2 times
daguero 2 years, 11 months ago
Answers is A. Pag. 244 System admin pdf
upvoted 3 times
Hudda 3 years ago
Friends, could you please confirm this answer?
upvoted 1 times
------------------------

119. Which of the following types of data count against the license daily quota?
A. Replicated data
B. splunkd logs
C. Summary index data
D. Windows internal logs

Hakeniz Highly Voted 2 years, 10 months ago
Answer is D
"Windows internal log" is not a Splunk internal log.
upvoted 9 times
ucsdmiami2020 2 years, 10 months ago
Agreed, even the provided reference in the answer clearly states "only data/log sent to Splunk affect license" and as such Windows, internal logs are external to Splunk and are sent to Splunk
upvoted 2 times
denominator Most Recent 2 years ago
Selected Answer: D
Sys Admin Pdf pg 46. Ans is D
upvoted 2 times
not_another_user_007 2 years, 10 months ago
splunkd (ie _internal _metrics, etc) do not count towards license.

The correct answer in this question is D.

Replicated/Summary does not count. People create a summary index to bypass licence usage at times.
upvoted 3 times
loky0 2 years, 10 months ago
Replicated data don't count towards license.
Summary data don't count towards license.
Internal logs don't count towards license.

so maybe D?
upvoted 3 times
kiranhar 2 years, 11 months ago
Sorry answer is B
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
The answer is D, please read through the provided Splunk URL reference https://docs.splunk.com/Documentation/Splunk/8.0.3/Admin/Distdeploylicenses#Clustered_deployments_and_licensing_issues
upvoted 1 times
kiranhar 2 years, 11 months ago
Answer is A
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
The answer is D, please read through the provided Splunk URL reference https://docs.splunk.com/Documentation/Splunk/8.0.3/Admin/Distdeploylicenses#Clustered_deployments_and_licensing_issues
upvoted 1 times
------------------------

100. Using SEDCMD in props.conf allows raw data to be modified. With the given event below, which option will mask the first three digits of the AcctID field resulting output: [22/Oct/2018:15:50:21] VendorID=1234 Code=B AcctID=xxx5309
Event:
[22/Oct/2018:15:50:21] VendorID=1234 Code=B AcctID=xxx5309
A. SEDCMD-1acct = s/VendorID=\d{3}(\d{4})/VendorID=xxx/g
B. SEDCMD-xxxAcct = s/AcctID=\d{3}(\d{4})/AcctID=xxx/g
C. SEDCMD-1acct = s/AcctID=\d{3}(\d{4})/AcctID=\1xxx/g
D. SEDCMD-1acct = s/AcctID=\d{3}(\d{4})/AcctID=xxx\1/g

Seba0297 2 years, 2 months ago
Selected Answer: D
"SEDCMD-1acct = s/AcctID=\d{3}(\d{4})/AcctID=xxx\1/g"
 follows the SEDCMD rule
"s/<regex>/<replacement>/<flags>"

In this case we are re-writing AcctID with three 'x', appending then the first (and only one) capture group, made of 4 digits
upvoted 3 times
Pacheco 2 years, 10 months ago
Right answer is D
upvoted 2 times
loky0 2 years, 10 months ago
should be D. the \1 indicates the capture group, should come after the xxx not before
upvoted 4 times
ucsdmiami2020 2 years, 10 months ago
Confirmed via Splunk docs https://docs.splunk.com/Documentation/Splunk/8.2.2/Data/Anonymizedata
Scrolling down to the section titled "Define the sed script in props.conf shows the correct syntax of an example which validates that the number/character /1 immediately preceded the /g
upvoted 1 times
------------------------

45. What type of data is counted against the Enterprise license at a fixed 150 bytes per event?
A. License data
B. Metrics data
C. Internal Splunk data
D. Internal Windows logs

amporiik Highly Voted 3 years, 11 months ago
B. Metrics data
upvoted 12 times
Ailen_Man Most Recent 2 years, 2 months ago
For metrics data, each metric event counts as a fixed 150 bytes. Metrics data does not use a separate license. Rather, it draws from the same license quota as event data. So B is correct
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 3 times
Bianchi 2 years, 11 months ago
B. Metrics. Pag: 46 from System Admin PDF
upvoted 4 times
------------------------

116. What happens when the same username exists in Splunk as well as through LDAP?
A. Splunk user is automatically deleted from authentication.conf.
B. LDAP settings take precedence.
C. Splunk settings take precedence.
D. LDAP user is automatically deleted from authentication.conf.

imggnz 2 years, 3 months ago
C, Splunk takes precendence
upvoted 2 times
inwigboji 2 years, 9 months ago
C is correct. Splunk platform attempts native authentication first. If authentication fails outside of a local account that doesn't exist, there is no attempt to use LDAP to log in. This is adapted from precedence of Splunk authentication schema.
upvoted 2 times
------------------------

115. Which Splunk forwarder has a built-in license?
A. Light forwarder
B. Heavy forwarder
C. Universal forwarder
D. Cloud forwarder

imggnz 2 years, 3 months ago
C, Universal Forwarder
upvoted 1 times
inwigboji 2 years, 9 months ago
universal forwarder
upvoted 1 times
kiranhar 2 years, 11 months ago
Answer is C
upvoted 1 times
------------------------

39. Which Splunk component does a search head primarily communicate with?
A. Indexer
B. Forwarder
C. Cluster master
D. Deployment server

amporiik Highly Voted 3 years, 11 months ago
A. Indexer
upvoted 9 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Splunk URL reference https://docs.splunk.com/Documentation/Splunk/7.3.1/InheritedDeployment/Deploymenttopology
"Search heads manage searches. They handle search requests from user and distribute the requests across the set of indexers, which search their local data. The search head then consolidates the results from all of the indexers and serves them to the users."
upvoted 1 times
rafiki31 Most Recent 2 years, 3 months ago
Ambiguous, It also could be the Cluster master, depending if we are adding a SH for the first time or we're just running a search...
upvoted 1 times
Apis 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 2 times
------------------------

42. Which of the following authentication types requires scripting in Splunk?
A. ADFS
B. LDAP
C. SAML
D. RADIUS

Josi12 Highly Voted 4 years, 1 month ago
RADIUS, PAM, Kerberos, TACACS+ support script authentication
upvoted 5 times
ucsdmiami2020 2 years, 9 months ago
Using Splunk Splexicon reference URL https://docs.splunk.com/Splexicon:Scriptedauthentication
Scripted Authentication: An option for Splunk Enterprise authentication. You can use an authentication system that you have in place (such as PAM or RADIUS) by configuring authentication.conf to use a script instead of using LDAP or Splunk Enterprise default authentication.
upvoted 1 times
RedYeti Most Recent 2 years, 3 months ago
D. RADIUS
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 1 times
haneeka 3 years, 10 months ago
D should be the answer
Reference: https://answers.splunk.com/answers/131127/scripted-authentication.html
upvoted 4 times
AbuAli 4 years, 3 months ago
Also, PAM using scripted
upvoted 3 times
------------------------

15. The priority of layered Splunk configuration files depends on the file's:
A. Owner
B. Weight
C. Context
D. Creation time

RedYeti 2 years, 3 months ago
Answer C
Data Admin course, page 43 and 257
upvoted 3 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
DeltaPotato 2 years, 10 months ago
C. Page 43, Data Admin pdf (page number as of August 2021 class). 

"In case of conflicts, priority is based on context: 
- Global context (index time) 
- App/User context (search-time)"
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Per the Splunk Reference URL 
https://docs.splunk.com/Documentation/Splunk/8.2.2/Admin/Wheretofindtheconfigurationfiles

"To determine the order of directories for evaludating configuration file precendence, Splunk software considers each file's context. Configuration files operate in either a global context or in the context of the current app and user"
upvoted 4 times
------------------------

22. Which Splunk forwarder type allows parsing of data before forwarding to an indexer?
A. Universal forwarder
B. Parsing forwarder
C. Heavy forwarder
D. Advanced forwarder

Fe01 2 years, 6 months ago
That has already been asked in question 7
upvoted 4 times
Apis 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 3 times
thomass 3 years, 3 months ago
Answer : C
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Forwarding/Typesofforwarders

"A heavy forwarder is a full Splunk Enterprise instance that can index, search, and change data as well as forward it. The heavy forwarder has some features disabled to reduce system resource usage."
upvoted 1 times
Asami 4 years ago
C. Heavy forwarde
upvoted 3 times
------------------------

63. After configuring a universal forwarder to communicate with an indexer, which index can be checked via the Splunk Web UI for a successful connection?
A. index=main
B. index=test
C. index=summary
D. index=_internal

NS2007 Highly Voted 3 years, 7 months ago
correct answer is D
upvoted 8 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 2 times
DeltaPotato 2 years, 10 months ago
D. index=_internal host=<forwarder_hostname>, page 183, System Admin PDF
upvoted 2 times
------------------------

49. What are the required stanza attributes when configuring the transforms.conf to manipulate or remove events?
A. REGEX, DEST, FORMAT
B. REGEX, SRC_KEY, FORMAT
C. REGEX, DEST_KEY, FORMAT
D. REGEX, DEST_KEY, FORMATTING

amporiik Highly Voted 3 years, 11 months ago
C. REGEX, DEST_KEY, FORMAT
upvoted 9 times
ucsdmiami2020 2 years, 9 months ago
Agreed C. Doing a Ctrl+F within the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/latest/Admin/Transformsconf

REGEX = <regular expression>
* Enter a regular expression to operate on your data.

FORMAT = <string>
* NOTE: This option is valid for both index-time and search-time field extraction. Index-time field extraction configuration require the FORMAT settings. The FORMAT settings is optional for search-time field extraction configurations.
* This setting specifies the format of the event, including any field names or values you want to add.

DEST_KEY = <key>
* NOTE: This setting is only valid for index-time field extractions.
* Specifies where SPLUNK software stores the expanded FORMAT results in accordance with the REGEX match.
upvoted 2 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 1 times
DeltaPotato 2 years, 10 months ago
Confirming C. - Data Admin pdf, page 240-241. When SOURCE_KEY is omitted, _raw is used as default.
upvoted 1 times
ames 3 years, 10 months ago
Latest version https://docs.splunk.com/Documentation/Splunk/latest/Admin/Transformsconf
upvoted 1 times
------------------------

47. How do you remove missing forwarders from the Monitoring Console?
A. By restarting Splunk.
B. By rescanning active forwarders.
C. By reloading the deployment server.
D. By rebuilding the forwarder asset table.

ames Highly Voted 3 years, 10 months ago
D is correct. More info here <https://docs.splunk.com/Documentation/Splunk/7.3.1/DMC/Configureforwardermonitoring#Rebuild_the_forwarder_asset_table>
upvoted 7 times
ucsdmiami2020 2 years, 9 months ago
Agreed D. Quoting the reference URL

"The data in the forwarder asset table are cumulative. If a forwarder connects to an indexer, its record exists in the table. Then if you later remove the forwarder from your deployment, the forwarder's record is not removed from the asset table. It is instead marked "missing" in the asset table, and it still appears in the DMC forwarder dashboards. To remove a forwarder entirely from the DMC dashboards, click rebuild forwarder assets ...
upvoted 2 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: D
D is correct
upvoted 2 times
------------------------

44. What is the difference between the two wildcards ... and * for the monitor stanza in inputs.conf?
A. ... is not supported in monitor stanzas.
B. There is no difference, they are interchangeable and match anything beyond directory boundaries.
C. * matches anything in that specific directory path segment, whereas ... recurses through subdirectories as well.
D. ... matches anything in that specific directory path segment, whereas * recurses through subdirectories as well.

Asami Highly Voted 4 years ago
C. * matches anything in that specific directory path segment, whereas ... recurses through subdirectories as well.
upvoted 12 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Reference URL https://docs.splunk.com/Documentation/Splunk/7.3.0/Data/Specifyinputpathswithwildcards

... The ellipsis wildcard searches recursively through directories and any number of levels of subdirectories to find matches.
If you specify a folder separator (for example, //var/log/.../file), it does not match the first folder level, only subfolders.

* The asterisk wildcard matches anything in that specific folder path segment.
Unlike ..., * does not recurse through subfolders.
upvoted 2 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: C
C is correct
upvoted 2 times
------------------------

43. Which option accurately describes the purpose of the HTTP Event Collector (HEC)?
A. A token-based HTTP input that is secure and scalable and that requires the use of forwarders.
B. A token-based HTTP input that is secure and scalable and that does not require the use of forwarders.
C. An agent-based HTTP input that is secure and scalable and that does not require the use of forwarders.
D. A token-based HTTP input that is insecure and non-scalable and that does not require the use of forwarders.

Asami Highly Voted 4 years ago
B. A token-based HTTP input that is secure and scalable and that does not require the use of forwarders.
upvoted 12 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk Reference URL https://docs.splunk.com/Documentation/Splunk/8.2.2/Data/UsetheHTTPEventCollector

"The HTTP Event Collector (HEC) lets you send data and application events to a Splunk deployment over the HTTP and Secure HTTP (HTTPS) protocols. HEC uses a token-based authentication model. You can generate a token and then configure a logging library or HTTP client with the token to send data to HEC in a specific format. This process eliminates the need for a Splunk forwarder when you send application events."
upvoted 2 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 2 times
gsplunker 3 years, 5 months ago
I would go with B
upvoted 3 times
------------------------

38. For single line event sourcetypes, it is most efficient to set SHOULD_LINEMERGE to what value?
A. True
B. False
C. <regex string>
D. Newline Character

amporiik Highly Voted 3 years, 11 months ago
B. False
upvoted 6 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk reference URL https://docs.splunk.com/Documentation/Splunk/latest/Data/Configureeventlinebreaking

Attribute : SHOULD_LINEMERGE = [true|false]

Description : When set to true, the Splunk platform combines several input lines into a single event, with configuration based on the settings described in the next section.

Default : true
upvoted 1 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 1 times
mikey_76 2 years, 10 months ago
If it's a single line event, then SHOULD_LINEMERGE is set to False
upvoted 2 times
------------------------

37. Local user accounts created in Splunk store passwords in which file?
A. $SPLUNK_HOME/etc/passwd
B. $SPLUNK_HOME/etc/authentication
C. $SPLUNK_HOME/etc/users/passwd.conf
D. $SPLUNK_HOME/etc/users/authentication.conf

Asami Highly Voted 4 years ago
A. $SPLUNK_HOME/etc/passwd
upvoted 10 times
ucsdmiami2020 2 years, 9 months ago
Per the provided reference URL https://docs.splunk.com/Documentation/Splunk/7.3.1/Admin/User-seedconf
"To set the default username and password, place user-seed.conf in $SPLUNK_HOME/etc/system/local. You must restart Splunk to enable configurations. If the $SPLUNK_HOME/etc/passwd file is present, the settings in this file (user-seed.conf) are not used."
upvoted 1 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 3 times
------------------------

32. Which of the following enables compression for universal forwarders in outputs.conf?
A. [udpout:mysplunk_indexer11] compression=true
B. [tcpout] defaultGroup=my_indexers compressed=true
C. /opt/splunkforwarder/bin/splunk enable compression
D. [tcpount:my_indexers] server=mysplunk_indexer1:9997, mysplunk_indexer2:9997 decompression=false

amporiik Highly Voted 3 years, 11 months ago
B. [tcpout] defaultGroup=my_indexers compressed=true 
to enable compression on UF add option compressed=true in stanza
upvoted 7 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: B
B is correct
upvoted 1 times
loky0 2 years, 10 months ago
B. P73 data admin pdf
upvoted 2 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the reference URL https://docs.splunk.com/Documentation/Splunk/latest/Admin/Outputsconf

# Compression
#
# This example sends compressed events to the remote indexer.
# NOTE: Compression can be enabled TCP or SSL outputs only.
# The receiver input port should also have compression enabled.
[tcpout]
server = splunkServer.example.com:4433
compressed = true
upvoted 3 times
gsplunker 3 years, 5 months ago
B is the ans
upvoted 1 times
ectomorph 3 years, 10 months ago
this answer = B - formatting is wonky... You could also see for SSL (shown below)
[tcpout]
defaultGroup=my_indexers 
compressed=true #HTTP Only
useClientSSLCompression=true #SSL
upvoted 4 times
------------------------

9. Which Splunk component distributes apps and certain other configuration updates to search head cluster members?
A. Deployer
B. Cluster master
C. Deployment server
D. Search head cluster master

loky0 Highly Voted 2 years, 10 months ago
A. Deployer handles search heads, Deployment Server handles forwarders.
upvoted 7 times
Apis Most Recent 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 2 times
DeltaPotato 2 years, 11 months ago
Unable to find any discussion of the deployer in either the sys or data admin class materials, but this page (https://docs.splunk.com/Documentation/Splunk/8.2.2/DistSearch/PropagateSHCconfigurationchanges) mirrors what Tony_123 said below.
upvoted 2 times
Tony_123 3 years, 5 months ago
The deployer is a Splunk Enterprise instance that you use to distribute apps and certain other configuration updates to search head cluster members. The set of updates that the deployer distributes is called the configuration bundle.
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Continuing with the quoting of the reference URL provided by @DeltaPotato 

"You must use the deployer, not the deployment server, to distribute apps to cluster members. Use of the deployer eliminates the possibility of conflict with the run-time updates that the cluster replicates automatically by means of the mechanism described in Configuration updates that the cluster replicates."
upvoted 1 times
sargeholik 3 years, 6 months ago
A. deployer
upvoted 1 times
Asami 4 years ago
A. Deployer
upvoted 4 times
------------------------

6. Which parent directory contains the configuration files in Splunk?
A. $SPLUNK_HOME/etc
B. $SPLUNK_HOME/var
C. $SPLUNK_HOME/conf
D. $SPLUNK_HOME/default

Apis Highly Voted 2 years, 6 months ago
Selected Answer: A
A is correct
upvoted 6 times
krishdee Most Recent 2 years, 9 months ago
could you please share the pdf? thanks
upvoted 1 times
loky0 2 years, 11 months ago
A, P77 in the new Sys admin pdf
upvoted 3 times
hesbee 2 years, 10 months ago
Please can you share the pdf file with me? Thank you!
upvoted 1 times
gabo1969 2 years, 7 months ago
Do you have the PDF document?
upvoted 1 times
ucsdmiami2020 2 years, 9 months ago
Per the provided Splunk URL reference https://docs.splunk.com/Documentation/Splunk/7.3.1/Admin/Configurationfiledirectories

Section titled, Configuration file directories, states "A detailed list of settings for each configuration file is provided in the .spec file names for that configuration file. You can find the latest version of the .spec and .example files in the $SPLUNK_HOME/etc system/README folder of your Splunk Enterprise installation..."
upvoted 3 times
ZeusP 3 years, 1 month ago
Ans is A
upvoted 4 times
sargeholik 3 years, 6 months ago
page 68 system admin
upvoted 2 times
Asami 4 years ago
A. $SPLUNK_HOME/etc
upvoted 4 times
------------------------

73. On the deployment server, administrators can map clients to server classes using client filters. Which of the following statements is accurate?
A. The blacklist takes precedence over the whitelist.
B. The whitelist takes precedence over the blacklist.
C. Wildcards are not supported in any client filters.
D. Machine type filters are applied before the whitelist and blacklist.

Orion42 Highly Voted 2 years, 11 months ago
Only A is correct
Ref: https://docs.splunk.com/Documentation/Splunk/8.2.1/Updating/Filterclients
upvoted 11 times
sam_1215 Most Recent 2 years, 8 months ago
Answer is A
A. The blacklist takes precedence over the whitelist. 

course "Data Admin" > Forwarder Management > Selecting Clients
- supports wildcards
- in addition ... you can further filter based on machine types

See also : 
https://docs.splunk.com/Documentation/Splunk/latest/Updating/Filterclients
upvoted 2 times
Hudda 3 years ago
friends, could you please confirm the answer for this Q ?
upvoted 1 times
thomass 3 years, 3 months ago
answer : a
upvoted 2 times
------------------------

105. What is the default value of LINE_BREAKER?
A. \r\n
B. ([\r\n]+)
C. \r+\n+
D. (\r\n+)

furiousjase 2 years, 10 months ago
Confirmed - B - "Default is any sequence of new lines and carriage returns: ([\r\n]+)"
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed B. Quoting the Splunk Reference URL, https://docs.splunk.com/Documentation/SplunkCloud/8.2.2105/Data/Configureeventlinebreaking

"How the Splunk platform determines event boundaries. The Splunk platform determines event boundaries in two phases: 1. Line breaking, which use the LINE_BREAKER setting to split the incoming stream of data into separate lines. By default, the LINE_BREAKER value is any sequence of newlines and carriage returns. In regular expression format, this is represented as the following string: ([\r\n]+)"
upvoted 1 times
kiranhar 2 years, 11 months ago
Answer is B
upvoted 1 times
------------------------

85. Who provides the Application Secret, Integration, and Secret keys, as well as the API Hostname when setting up Duo for Multi-Factor Authentication in Splunk
Enterprise?
A. Duo Administrator
B. LDAP Administrator
C. SAML Administrator
D. Trio Administrator

Bianchi 2 years, 11 months ago
The answer is A. Pag 232 from Sys Adm PDF
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed A. Quoting the reference URL, https://duo.com/docs/splunk

"Configure Duo for Splunk 6.5 and later. 1-Log into Splunk Enterprise as an admin and navigate to Settings -> Users and Authentication - Access Controls. ... 4-Fill out the form with your Duo Splunk application information: Integration Key ; Secret Key ; API Hostname ; Authentication behavior when DUO Security is unavailable ; Connection Timeout"
upvoted 1 times
gsplunker 3 years, 5 months ago
Answer is A
upvoted 3 times
mybox1 3 years, 6 months ago
A is correct (System Administration PDF, page 250).
upvoted 1 times
newrose 3 years, 7 months ago
Any thoughts?
upvoted 1 times
------------------------

104. When indexing a data source, which fields are considered metadata?
A. source, host, time
B. time, sourcetype, source
C. host, raw, sourcetype
D. sourcetype, source, host

furiousjase 2 years, 10 months ago
Confirmed D - "metadata (source, sourcetype, host, timestamp, punct, etc.)"
upvoted 3 times
ucsdmiami2020 2 years, 9 months ago
Agreed answer is D. Quoting the Reference URL

"The metadata command returns a list of sources, sourcetypes, or hosts from a specified index or distributed search peer."
upvoted 1 times
kiranhar 2 years, 11 months ago
answer is D
upvoted 1 times
------------------------

110. After automatic load balancing is enabled on a forwarder, the time interval for switching indexers can be updated by using which of the following attributes?
A. channelTTL
B. connectionTimeout
C. autoLBFrequency
D. secsInFailureInterval

loky0 2 years, 10 months ago
C. P76 Data admin pdf
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Per the provided URL reference, scroll down to the section titled, Choose a load balancing method
https://docs.splunk.com/Documentation/Forwarder/8.2.1/Forwarder/Configureloadbalancing

By time. The default method for load balancing is how frequently the forwarders change indexers in the load balanced list. The autoLBFrequency settings in outputs.conf controls how often forwarders switch between indexers. The default frequency is every 30 seconds, but you can set it higher or lower.
upvoted 1 times
------------------------

97. Which feature of Splunk's role configuration can be used to aggregate multiple roles intended for groups of users?
A. Linked roles
B. Grantable roles
C. Role federation
D. Role inheritance

jm130106 2 years, 11 months ago
D
Role inheritance: You can have a role inherit certain properties from one or more existing role
https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/Aboutusersandroles
upvoted 4 times
ucsdmiami2020 2 years, 9 months ago
Using the provided Splunk URL by @jim130106
scroll down to the section titled, Set permission granularity with custom roles, and see
"Role inheritance: You can have a role inherit certain properties from one or more existing roles."
upvoted 1 times
Hudda 3 years ago
Friends, could you please confirm this answer?
upvoted 2 times
mybox1 3 years, 6 months ago
D is correct
upvoted 2 times
newrose 3 years, 7 months ago
Anyone confirms?
upvoted 1 times
------------------------

112. After an Enterprise Trial license expires, it will automatically convert to a Free license. How many days is an Enterprise Trial license valid before this conversion occurs?
A. 90 days
B. 60 days
C. 7 days
D. 14 days

furiousjase 2 years, 10 months ago
Answer - B
Comes with product; Valid for 60 days, after which another license type must be activated
Page 42 Splunk Enterprise System Administration
upvoted 4 times
ucsdmiami2020 2 years, 10 months ago
Per the answer provided URL https://docs.splunk.com/Documentation/Splunk/8.2.1/Admin/MoreaboutSplunkFree
How do I get Splunk Enterprise with the Free license?
Create your user account on splunk.com.
Review the list of supported operating systems for the "Free" license in Supported Operating Systems.
Download the latest version of Splunk Enterprise for your operating system from Free Trials and Downloads on splunk.com. Login required.
Use the installation instructions for your operating system. See Installation instructions.
After installation, you'll have an Enterprise Trial license for 60 days. You can change to the Free license at any point before the Enterprise Trial is complete. See Switching to Free from an Enterprise Trial license.
upvoted 1 times
------------------------

109. The LINE_BREAKER attribute is configured in which configuration file?
A. props.conf
B. indexes.conf
C. inputs.conf
D. transforms.conf

Pacheco Highly Voted 2 years, 10 months ago
Correct answer is A per Data Admin docs
upvoted 6 times
ucsdmiami2020 2 years, 10 months ago
Using the provided Reference URL https://docs.splunk.com/Documentation/SplunkCloud/8.2.2105/Data/Configureeventlinebreaking
"How to configure event boundaries
Many event logs have a strict one-line-per-event format, but others don't. The Splunk platform can often recognize the event boundaries, but if event boundary recognition doesn't occur, or happens incorrectly, you can set custom rules in the props.conf configuration file to establish event boundaries."
upvoted 1 times
------------------------

98. Which of the following is the use case for the deployment server feature of Splunk?
A. Managing distributed workloads in a Splunk environment.
B. Automating upgrades of Splunk forwarder installations on endpoints.
C. Orchestrating the operations and scale of a containerized Splunk deployment.
D. Updating configuration and distributing apps to processing components, primarily forwarders.

merte Highly Voted 3 years, 5 months ago
Answer is D.
upvoted 9 times
not_another_user_007 2 years, 10 months ago
D is Correct
upvoted 1 times
ucsdmiami2020 2 years, 10 months ago
Confirmed per Splunk docs https://docs.splunk.com/Documentation/Splunk/8.2.2/Updating/Aboutdeploymentserver
"The deployment server is the tool for distributing configurations, apps, and content updates to groups of Splunk Enterprise instances."
upvoted 1 times
Hudda Most Recent 3 years ago
Friends, could you please confirm this answer?
upvoted 1 times
newrose 3 years, 7 months ago
Anyone confirms that?
upvoted 1 times
Lerd15 3 years, 6 months ago
A deployment server is used to deploy apps to forwarders (and technically could be used to deploy apps to other Splunk servers as well but with a number of caveats)
upvoted 4 times
------------------------

